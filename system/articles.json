[
{
"code":215,
"title":"Visual market sector analysis for financial time series data",
"abstract":"The massive amount of financial time series data that originates from the stock market generates large amounts of complex data of high interest. However, adequate solutions that can effectively handle the information in order to gain insight and to understand the market mechanisms are rare. In this paper, we present two techniques and applications that enable the user to interactively analyze large amounts of time series data in real-time in order to get insight into the development of assets, market sectors, countries, and the financial market as a whole. The first technique allows users to quickly analyze combinations of single assets, market sectors as well as countries, compare them to each other, and to visually discover the periods of time where market sectors and countries get into turbulence. The second application clusters a selection of large amounts of financial time series data according to their similarity, and analyzes the distribution of the assets among market sectors. This allows users to identify the characteristic graphs which are representative for the development of a particular market sector, and also to identify the assets which behave considerably differently compared to other assets in the same sector. Both applications allow the user to perform investigative exploration techniques and interactive visual analysis in real-time.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:Time Series, visual method:Glyphs, analysis method:Clustering, paper type:Application, visual method:Novel Visual Encoding"
},
{
"code":214,
"title":"Visual Clustering in Parallel Coordinates",
"abstract":"Parallel coordinates have been widely applied to visualize high-dimensional and multivariate data, discerning patterns within the data through visual clustering. However, the effectiveness of this technique on large data is reduced by edge clutter. In this paper, we present a novel framework to reduce edge clutter, consequently improving the effectiveness of visual clustering. We exploit curved edges and optimize the arrangement of these curved edges by minimizing their curvature and maximizing the parallelism of adjacent edges. The overall visual clustering is improved by adjusting the shape of the edges while keeping their relative order. The experiments on several representative datasets demonstrate the effectiveness of our approach.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric,visual method:Parallel Coordinates, analysis method:Clustering, Clutter Reduction, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":213,
"title":"Splatting the Lines in Parallel Coordinates",
"abstract":"In this paper, we propose a novel splatting framework for clutter reduction and pattern revealing in parallel coordinates. Our framework consists of two major components: a polyline splatter for cluster detection and a segment splatter for clutter reduction. The cluster detection is performed by splatting the lines one by one into the parallel coordinates plots, and for each splatted line we enhance its neighboring lines and suppress irrelevant ones. To reduce visual clutter caused by line crossings and overlappings in the clustered results, we provide a segment splatter which represents each polyline by one segment and splats these segments with different speeds, colors, and lengths from the leftmost axis to the rightmost axis. Users can interactively control both the polyline splatting and the segment splatting processes to emphasize the features they are interested in. The experimental results demonstrate that our framework can effectively reveal some hidden patterns in parallel coordinates.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Parallel Coordinates, visual method:Novel Visual Encoding"
},
{
"code":212,
"title":"Interactive Exploration of Implicit and Explicit Relations in Faceted Datasets",
"abstract":"Many datasets, such as scientific literature collections, contain multiple heterogeneous facets which derive implicit relations, as well as explicit relational references between data items. The exploration of this data is challenging not only because of large data scales but also the complexity of resource structures and semantics. In this paper, we present PivotSlice, an interactive visualization technique which provides efficient faceted browsing as well as flexible capabilities to discover data relationships. With the metaphor of direct manipulation, PivotSlice allows the user to visually and logically construct a series of dynamic queries over the data, based on a multi-focus and multi-scale tabular view that subdivides the entire dataset into several meaningful parts with customized semantics. PivotSlice further facilitates the visual exploration and sensemaking process through features including live search and integration of online data, graphical interaction histories and smoothly animated visual state transitions. We evaluated PivotSlice through a qualitative lab study with university researchers and report the findings from our observations and interviews. We also demonstrate the effectiveness of PivotSlice using a scenario of exploring a repository of information visualization literature.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Data Subset, paper type:Technical, visual method:Scatterplot"
},
{
"code":211,
"title":"A network-based interface for the exploration of high-dimensional data spaces",
"abstract":"The navigation of high-dimensional data spaces remains challenging, making multivariate data exploration difficult. To be effective and appealing for mainstream application, navigation should use paradigms and metaphors that users are already familiar with. One such intuitive navigation paradigm is interactive route planning on a connected network. We have employed such an interface and have paired it with a prominent high-dimensional visualization paradigm showing the N-D data in undistorted raw form: parallel coordinates. In our network interface, the dimensions form nodes that are connected by a network of edges representing the strength of association between dimensions. A user then interactively specifies nodes/edges to visit, and the system computes an optimal route, which can be further edited and manipulated. In our interface, this route is captured by a parallel coordinate data display in which the dimension ordering is configured by the specified route. Our framework serves both as a data exploration environment and as an interactive presentation platform to demonstrate, explain, and justify any identified relationships to others. We demonstrate our interface within a business scenario and other applications.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimension Relationship, paper type:Technical, visual method:Parallel Coordinates, User Study"
},
{
"code":210,
"title":"Dimension Projection Matrix/Tree: Interactive Subspace Visual Exploration and Analysis of High Dimensional Data",
"abstract":"For high-dimensional data, this work proposes two novel visual exploration methods to gain insights into the data aspect and the dimension aspect of the data. The first is a Dimension analysis method:Projection Matrix, as an extension of a scatterplot matrix. In the matrix, each row or column represents a group of dimensions, and each cell shows a dimension projection (such as MDS) of the data with the corresponding dimensions. The second is a Dimension analysis method:Projection Tree, where every node is either a dimension projection plot or a Dimension analysis method:Projection Matrix. Nodes are connected with links and each child node in the tree covers a subset of the parent node's dimensions or a subset of the parent node's data items. While the tree nodes visualize the subspaces of dimensions or subsets of the data items under exploration, the matrix nodes enable cross-comparison between different combinations of subspaces. Both Dimension analysis method:Projection Matrix and Dimension Project Tree can be constructed algorithmically through automation, or manually through user interaction. Our implementation enables interactions such as drilling down to explore different levels of the data, merging or splitting the subspaces to adjust the matrix, and applying brushing to select data clusters. Our method enables simultaneously exploring data correlation and dimension correlation for data with high dimensions.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, analysis method:Dimension Relationship, visual method:Hierarchy, visual method:Scatterplot, analysis method:Projection, analysis method:Subspace, View Optimization, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":209,
"title":"Scattering points in parallel coordinates",
"abstract":"In this paper, we present a novel parallel coordinates design integrated with points (scattering points in parallel coordinates, SPPC), by taking advantage of both parallel coordinates and scatterplots. Different from most multiple views visualization frameworks involving parallel coordinates where each visualization type occupies an individual window, we convert two selected neighboring coordinate axes into a scatterplot directly. Multidimensional scaling is adopted to allow converting multiple axes into a single subplot. The transition between two visual types is designed in a seamless way. In our work, a series of interaction tools has been developed. Uniform brushing functionality is implemented to allow the user to perform data selection on both points and parallel coordinate polylines without explicitly switching tools. A GPU accelerated dimensional incremental multidimensional scaling (DIMDS) has been developed to significantly improve the system performance. Our case study shows that our scheme is more efficient than traditional multi-view methods in performing visual analysis tasks.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Scatterplot,visual method:Parallel Coordinates, visual method:Novel Visual Encoding, analysis method:Dimensionality Reduction, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":208,
"title":"Analysis Guided Visual Exploration of Multivariate Data",
"abstract":"Visualization systems traditionally focus on graphical representation of information. They tend not to provide integrated analytical services that could aid users in tackling complex knowledge discovery tasks. Users' exploration in such environments is usually impeded due to several problems: 1) valuable information is hard to discover when too much data is visualized on the screen; 2) Users have to manage and organize their discoveries off line, because no systematic discovery management mechanism exists; 3) their discoveries based on visual exploration alone may lack accuracy; 4) and they have no convenient access to the important knowledge learned by other users. To tackle these problems, it has been recognized that analytical tools must be introduced into visualization systems. In this paper, we present a novel analysis-guided exploration system, called the nugget management system (NMS). It leverages the collaborative effort of human comprehensibility and machine computations to facilitate users' visual exploration processes. Specifically, NMS first extracts the valuable information (nuggets) hidden in datasets based on the interests of users. Given that similar nuggets may be re-discovered by different users, NMS consolidates the nugget candidate set by clustering based on their semantic similarity. To solve the problem of inaccurate discoveries, localized data mining techniques are applied to refine the nuggets to best represent the captured patterns in datasets. Lastly, the resulting well-organized nugget pool is used to guide users' exploration. To evaluate the effectiveness of NMS, we integrated NMS into Xmd- vTool, a freeware multivariate visualization system. User studies were performed to compare the users' efficiency and accuracy in finishing tasks on real datasets, with and without the help of NMS. Our user studies confirmed the effectiveness of NMS.",
"keywords":"pipeline stage:Data Transformation, user involvement:Model Manipulation, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Parallel Coordinates, visual method:Scatterplot, User Study"
},
{
"code":207,
"title":"Interactive hierarchical dimension ordering, spacing and filtering for exploration of high dimensional datasets",
"abstract":"Large number of dimensions not only cause clutter in multi-dimensional visualizations, but also make it difficult for users to navigate the data space. Effective dimension management, such as dimension ordering, spacing and filtering, is critical for visual exploration of such datasets. Dimension ordering and spacing explicitly reveal dimension relationships in arrangement-sensitive multidimensional visualization techniques, such as parallel coordinates, star glyphs, and pixel-oriented techniques. They facilitate the visual discovery of patterns within the data. Dimension filtering hides some of the dimensions to reduce clutter while preserving the major information of the dataset. In this paper, we propose an interactive hierarchical dimension ordering, spacing and filtering approach, called DOSFA. DOSFA is based on dimension hierarchies derived from similarities among dimensions. It is scalable multi-resolution approach making dimensional management a tractable task. On the one hand, it automatically generates default settings for dimension ordering, spacing and filtering. On the other hand, it allows users to efficiently control all aspects of this dimension management process via visual interaction tools for dimension hierarchy manipulation. A case study visualizing a dataset containing over 200 dimensions reveals high dimensional visualization techniques.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration,visual method:Parallel Coordinates, analysis method:Dimension Relationship, analysis method:Clustering, visual method:Hierarchy, Query, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":206,
"title":"Value and Relation Display for Interactive Exploration of High Dimensional Datasets",
"abstract":"Traditional multidimensional visualization techniques, such as glyphs, parallel coordinates and scatterplot matrices, suffer from clutter at the display level and difficult user navigation among dimensions when visualizing high dimensional datasets. In this paper, we propose a new multidimensional visualization technique named a value and relation (VaR) display, together with a rich set of navigation and selection tools, for interactive exploration of datasets with up to hundreds of dimensions. By explicitly conveying the relationships among the dimensions of a high dimensional dataset, the VaR display helps users grasp the associations among dimensions. By using pixel-oriented techniques to present values of the data items in a condensed manner, the VaR display reveals data patterns in the dataset using as little screen space as possible. The navigation and selection tools enable users to interactively reduce clutter, navigate within the dimension space, and examine data value details within context effectively and efficiently. The VaR display scales well to datasets with large numbers of data items by employing sampling and texture mapping. A case study on a real dataset, as well as the VaR displays of multiple real datasets throughout the paper, reveals how our proposed approach helps users interactively explore high dimensional datasets with large numbers of data items",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, visual method:Glyphs, analysis method:Dimension Relationship, Clutter Reduction, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":205,
"title":"Value and relation display: interactive visual exploration of large data sets with hundreds of dimensions",
"abstract":"Few existing visualization systems can handle large data sets with hundreds of dimensions, since high-dimensional data sets cause clutter on the display and large response time in interactive exploration. In this paper, we present a significantly improved multidimensional visualization approach named Value and Relation (VaR) display that allows users to effectively and efficiently explore large data sets with several hundred dimensions. In the VaR display, data values and dimension relationships are explicitly visualized in the same display by using dimension glyphs to explicitly represent values in dimensions and glyph layout to explicitly convey dimension relationships. In particular, pixel-oriented techniques and density-based scatterplots are used to create dimension glyphs to convey values. Multidimensional scaling, Jigsaw map hierarchy visualization techniques, and an animation metaphor named Rainfall are used to convey relationships among dimensions. A rich set of interaction tools has been provided to allow users to interactively detect patterns of interest in the VaR display. A prototype of the VaR display has been fully implemented. The case studies presented in this paper show how the prototype supports interactive exploration of data sets of several hundred dimensions. A user study evaluating the prototype is also reported in this paper.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, visual method:Glyphs, analysis method:Dimension Relationship, Clutter Reduction, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":204,
"title":"3D Grand Tour for Multidimensional Data and Clusters",
"abstract":"Grand tour is a method for viewing multidimensional data via linear projections onto a sequence of two dimensional subspaces and then moving continuously from one projection to the next. This paper extends the method to 3D grand tour where projections are made onto three dimensional subspaces. 3D cluster-guided tour is proposed where sequences of projections are determined by cluster centroids. Cluster-guided tour makes inter-cluster distance-preserving projections under which clusters are displayed as separate as possible. Various add-on features, such as projecting variable vectors together with data points, interactive picking and drill down, and cluster similarity graphs, help further the understanding of data. A CAVE virtual reality environment is at our disposal for 3D immersive display. This approach of multidimensional visualization provides a natural metaphor to visualize clustering results and data at hand by mapping the data onto a time-indexed family of 3D natural projections suitable for human eye's exploration.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot"
},
{
"code":203,
"title":"Exploratory Visualization of Multivariate Data with Variable Quality",
"abstract":"Real-world data is known to be imperfect, suffering from various forms of defects such as sensor variability, estimation errors, uncertainty, human errors in data entry, and gaps in data gathering. Analysis conducted on variable quality data can lead to inaccurate or incorrect results. An effective visualization system must make users aware of the quality of their data by explicitly conveying not only the actual data content, but also its quality attributes. While some research has been conducted on visualizing uncertainty in spatio-temporal data and univariate data, little work has been reported on extending this capability into multivariate data visualization. In this paper we describe our approach to the problem of visually exploring multivariate data with variable quality. As a foundation, we propose a general approach to defining quality measures for tabular data, in which data may experience quality problems at three granularities: individual data values, complete records, and specific dimensions. We then present two approaches to visual mapping of quality information into display space. In particular, one solution embeds the quality measures as explicit values into the original dataset by regarding value quality and record quality as new data dimensions. The other solution is to superimpose the quality information within the data visualizations using additional visual variables. We also report on user studies conducted to assess alternate mappings of quality attributes to visual variables for the second method. In addition, we describe case studies that expose some of the advantages and disadvantages of these two approaches",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Parallel Coordinates, User Study"
},
{
"code":202,
"title":"Navigating high-dimensional spaces to support design steering",
"abstract":"Throughout the design cycle, visualization, whether a sketch scribbled on the back of a spare piece of paper or a fully detailed drawing, has been the mainstay of design: we need to see the product. One of the most important stages of the design cycle is the initial, or concept, stage and it is here that design variants occur in large numbers to be vetted quickly. At this initial stage the human element, the designer is crucial to the success of the product. We describe an interactive environment for concept design which recognises the needs of the designer, not only to see the product and make rapid modifications, but also to monitor the progress of their design towards some preferred solution. This leads to the notion of a design parameter space, typically high-dimensional, which must also be visualized in addition to the product itself. Using a module developed for IRIS Explorer design steering is presented as a navigation of this space in order to search for optimal designs, either manually or by local optimisation.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, Parameter Exploration, paper type:Application, data type:High-dimensional Points, visual method:Novel Visual Encoding, analysis method:Optimization"
},
{
"code":201,
"title":"Dynamic visualization of transient data streams",
"abstract":"We introduce two dynamic visualization techniques using multidimensional scaling to analyze transient data streams such as newswires and remote sensing imagery. While the time-sensitive nature of these data streams requires immediate attention in many applications, the unpredictable and unbounded characteristics of this information can potentially overwhelm many scaling algorithms that require a full re-computation for every update. We present an adaptive visualization technique based on data stratification to ingest stream information adaptively when influx rate exceeds processing rate. We also describe an incremental visualization technique based on data fusion to project new information directly onto a visualization subspace spanned by the singular vectors of the previously processed neighboring data. The ultimate goal is to leverage the value of legacy and new information and minimize re-processing of the entire dataset in full resolution. We demonstrate these dynamic visualization results using a newswire corpus and a remote sensing imagery sequence.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Qualtiy Measure, paper type:Technical, visual method:Parallel Coordinates, visual method:Scatterplot,  User Study"
},
{
"code":200,
"title":"Steerable, Progressive Multidimensional Scaling",
"abstract":"Current implementations of multidimensional scaling (MDS), an approach that attempts to best represent data point similarity in a low-dimensional representation, are not suited for many of today's large-scale datasets. We propose an extension to the spring model approach that allows the user to interactively explore datasets that are far beyond the scale of previous implementations of MDS. We present MDSteer, a steerable MDS computation engine and visualization tool that progressively computes an MDS layout and handles datasets of over one million points. Our technique employs hierarchical data structures and progressive layouts to allow the user to steer the computation of the algorithm to the interesting areas of the dataset. The algorithm iteratively alternates between a layout stage in which a subselection of points are added to the set of active points affected by the MDS iteration, and a binning stage which increases the depth of the bin hierarchy and organizes the currently unplaced points into separate spatial regions. This binning strategy allows the user to select onscreen regions of the layout to focus the MDS computation into the areas of the dataset that are assigned to the selected bins. We show both real and common synthetic benchmark datasets with dimensionalities ranging from 3 to 300 and cardinalities of over one million points",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimensionality Reduction, paper type:Technical, visual method:Scatterplot"
},
{
"code":199,
"title":"High-dimensional visual analytics: Interactive exploration guided by pairwise views of point distributions",
"abstract":"We introduce a method for organizing multivariate displays and for guiding interactive exploration through high-dimensional data. The method is based on nine characterizations of the 2D distributions of orthogonal pairwise projections on a set of points in multidimensional Euclidean space. These characterizations include such measures as density, skewness, shape, outliers, and texture. Statistical analysis of these measures leads to ways for 1) organizing 2D scatterplots of points for coherent viewing, 2) locating unusual (outlying) marginal 2D distributions of points for anomaly detection, and 3) sorting multivariate displays based on high-dimensional data, such as trees, parallel coordinates, and glyphs.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Scatterplot, View Optimization, Ranking, paper type:Technical, data type:High-Dimensional Points, analysis method:Scagnostics"
},
{
"code":198,
"title":"Graph-Theoretic Scagnostics.",
"abstract":"We introduce Tukey and Tukey scagnostics and develop graph-theoretic methods for implementing their procedure on large datasets.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Scatterplot, View Optimization, Ranking, paper type:Technical, data type:High-Dimensional Points, analysis method:Scagnostics"
},
{
"code":197,
"title":"Topological Landscapes: A Terrain Metaphor for Scientific Data",
"abstract":"Scientific visualization and illustration tools are designed to help people understand the structure and complexity of scientific data with images that are as informative and intuitive as possible. In this context the use of metaphors plays an important role since they make complex information easily accessible by using commonly known concepts. In this paper we propose a new metaphor, called topological landscapes, which facilitates understanding the topological structure of scalar functions. The basic idea is to construct a terrain with the same topology as a given dataset and to display the terrain as an easily understood representation of the actual input data. In this projection from an n-dimensional scalar function to a two-dimensional (2D) model we preserve function values of critical points, the persistence (function span) of topological features, and one possible additional metric property (in our examples volume). By displaying this topologically equivalent landscape together with the original data we harness the natural human proficiency in understanding terrain topography and make complex topological information easily accessible.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, analysis method:Topological Analysis, visual method:Novel Visual Encoding, visual method:Glyphs, paper type:Technical, data type:High-Dimensional Function"
},
{
"code":196,
"title":"Multidimensional data dissection using attribute relationship graphs",
"abstract":"Visual exploration and analysis is a process of discovering and dissecting the abundant and complex attribute relationships that pervade multidimensional data. Recent research has identified and characterized patterns of multiple coordinated views, such as cross-filtered views, in which rapid sequences of simple interactions can be used to express queries on subsets of attribute values. In visualizations designed around these patterns, for the most part, distinct views serve to visually isolate each attribute from the others. Although the brush-and-click simplicity of visual isolation facilitates discovery of many-to-many relationships between attributes, dissecting these relationships into more fine-grained one-to-many relationships is interactively tedious and, worse, visually fragmented over prolonged sequences of queries. This paper describes: (1) a method for interactively dissecting multidimensional data by iteratively slicing and manipulating a multigraph representation of data values and value co-occurrences; and (2) design strategies for extending the construction of coordinated multiple view interfaces for dissection as well as discovery of attribute relationships in multidimensional data sets. Using examples from different domains, we describe how attribute relationship graphs can be combined with cross-filtered views, modularized for reuse across designs, and integrated into broader visual analysis tools. The exploratory and analytic utility of these examples suggests that an attribute relationship graph would be a useful addition to a wide variety of visual analysis tools.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:Nominal Data, analysis method:Clustering, paper type:Technical, visual method:graph"
},
{
"code":195,
"title":"Cross-Filtered Views for Multidimensional Visual Analysis",
"abstract":"Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: 1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views and 2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. We also identify several important limitations of the approach. The demonstrated analytic utility of these examples suggests that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Query, data type:Nominal Data, analysis method:Cross-filtering, visual method:Graph, paper type:Technical"
},
{
"code":194,
"title":"Conjunctive Visual Forms",
"abstract":"Visual exploration of multidimensional data is a process of isolating and extracting relationships within and between dimensions. Coordinated multiple view approaches are particularly effective for visual exploration because they support precise expression of heterogeneous multidimensional queries using simple interactions. Recent visual analytics research has made significant progress in identifying and understanding patterns of composed views and coordinations that support fast, flexible, and open-ended data exploration. What is missing is formalization of the space of expressible queries in terms of visual representation and interaction. This paper introduces the conjunctive visual form model in which visual exploration consists of interactively-driven sequences of transitions between visual states that correspond to conjunctive normal forms in boolean logic. The model predicts several new and useful ways to extend the space of rapidly expressible queries through addition of simple interactive capabilities to existing compositional patterns. Two recent related visual tools offer a subset of these capabilities, providing a basis for conjecturing about such extensions.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Query, paper type:Technical, data type:Nominal Data, visual method:Graph, analysis method:conjunctive relationship"
},
{
"code":193,
"title":"Multidimensional visual analysis using cross-filtered views",
"abstract":"Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: (1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views, and (2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. The demonstrated analytic utility of these examples suggest that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Query, paper type:Technical, data type:High-Dimensional Points, analysis method:Cross-filtering, visual method:Novel Visual Encoding"
},
{
"code":192,
"title":"A note on space-filling visualizations and space-filling curves",
"abstract":"A recent line of treemap research has focused on layout algorithms that optimize properties such as stability, preservation of ordering information, and aspect ratio of rectangles. No ideal treemap layout algorithm has been found, and so it is natural to explore layouts that produce nonrectangular regions. This note describes a connection between space-filling visualizations and the mathematics of space-filling curves, and uses that connection to characterize a family of layout algorithms which produce nonrectangular regions but enjoy geometric continuity under changes to the data and legibility even for highly unbalanced trees.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-Dimensional Points, paper type:Technical, analysis method:Space-filling Curves, visual method:Treemap"
},
{
"code":191,
"title":"Multivariate Data Glyphs: Principles and Practice",
"abstract":"In the context of data visualization, a glyph is a visual representation of a piece of data where the attributes of a graphical entity are dictated by one or more attributes of a data record. For example, the width and height of a box could be determined by a student鈥檚 score on the midterm and final exam for a course, while the box鈥檚 color might indicate the genderof the student.Thedefinitionabove is ratherbroad, as it can cover such visual elements as the markers in a scatterplot, the bars of a histogram, or even an entire line plot. However, a narrower definition would not be sufficient to capture the wide range of data visualization techniques that have been developed over the centuries that are termed glyphs.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, analysis method:Not Applicable, paper type:Survey, data type:High-dimensional Points, visual method:Glyphs"
},
{
"code":190,
"title":"Branching and Circular Features in High Dimensional Data",
"abstract":"Large observations and simulations in scientific research give rise to high-dimensional data sets that present many challenges and opportunities in data analysis and visualization. Researchers in application domains such as engineering, computational biology, climate study, imaging and motion capture are faced with the problem of how to discover compact representations of highdimensional data while preserving their intrinsic structure. In many applications, the original data is projected onto low-dimensional space via dimensionality reduction techniques prior to modeling. One problem with this approach is that the projection step in the process can fail to preserve structure in the data that is only apparent in high dimensions. Conversely, such techniques may create structural illusions in the projection, implying structure not present in the original high-dimensional data. Our solution is to utilize topological techniques to recover important structures in high-dimensional data that contains non-trivial topology. Specifically, we are interested in high-dimensional branching structures. We construct local circle-valued coordinate functions to represent such features. Subsequently, we perform dimensionality reduction on the data while ensuring such structures are visually preserved. Additionally, we study the effects of global circular structures on visualizations. Our results reveal never-before-seen structures on real-world data sets from a variety of applications.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Topological Analysis, paper type:Technical, data type:High-dimensional Points, visual method:Scatterplot"
},
{
"code":189,
"title":"SketchPadN-D: WYDIWYG Sculpting and Editing in High-Dimensional Space",
"abstract":"High-dimensional data visualization has been attracting much attention. To fully test related software and algorithms, researchers require a diverse pool of data with known and desired features. Test data do not always provide this, or only partially. Here we propose the paradigm WYDIWYGS (What You Draw Is What You Get). Its embodiment, SketchPadND, is a tool that allows users to generate high-dimensional data in the same interface they also use for visualization. This provides for an immersive and direct data generation activity, and furthermore it also enables users to interactively edit and clean existing high-dimensional data from possible artifacts. SketchPadND offers two visualization paradigms, one based on parallel coordinates and the other based on a relatively new framework using an N-D polygon to navigate in high-dimensional space. The first interface allows users to draw arbitrary profiles of probability density functions along each dimension axis and sketch shapes for data density and connections between adjacent dimensions. The second interface embraces the idea of sculpting. Users can carve data at arbitrary orientations and refine them wherever necessary. This guarantees that the data generated is truly high-dimensional. We demonstrate our tool's usefulness in real data visualization scenarios.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:not applicable, paper type:Technical, visual method:Scatterplot"
},
{
"code":188,
"title":"RnavGraph: A visualization tool for navigating through high-dimensional data",
"abstract":"none",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Animation, View Optimization, Ranking, paper type:Technical, data type:High-Dimensional Points, analysis method:Quality Measure"
},
{
"code":187,
"title":"HyperSlice",
"abstract":"HyperSlice is a new method for the visualization of scalar functions of many variables. With this method the multi-dimensional function is presented in a simple and easy to understand way in which all dimensions are treated identically. The central concept is the representation of a multi-dimensional function as a matrix of orthogonal two-dimensional slices. These two-dimensional slices lend themselves very well to interaction via direct manipulation, due to a one to one relation between screen space and variable space. Several interaction techniques, for navigation, the location of maxima, and the use of user-defined paths, are presented",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration,data type:High-dimensional Function, Parameter Exploration, visual method:Novel Visual Encoding, paper type:Technical, analysis method:not applicable"
},
{
"code":186,
"title":"Opening the black box - data driven visualization of neural networks",
"abstract":"Artificial neural networks are computer software or hardware models inspired by the structure and behavior of neurons in the human nervous system. As a powerful learning tool, increasingly neural networks have been adopted by many large-scale information processing applications but there is no a set of well defined criteria for choosing a neural network. The user mostly treats a neural network as a black box and cannot explain how learning from input data was done nor how performance can be consistently ensured. We have experimented with several information visualization designs aiming to open the black box to possibly uncover underlying dependencies between the input data and the output data of a neural network. In this paper, we present our designs and show that the visualizations not only help us design more efficient neural networks, but also assist us in the process of using neural networks for problem solving such as performing a classification task.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration,visual method:Parallel Coordinates, Parameter Exploration, visual method:Novel Visual Encoding, visual method:Glyphs, Machine Learning, paper type:Technical, data type:High-Dimensional Points, analysis method:not applicable"
},
{
"code":185,
"title":"Representative factor generation for the interactive visual analysis of high-dimensional data",
"abstract":"Datasets with a large number of dimensions per data item (hundreds or more) are challenging both for computational and visual analysis. Moreover, these dimensions have different characteristics and relations that result in sub-groups and/or hierarchies over the set of dimensions. Such structures lead to heterogeneity within the dimensions. Although the consideration of these structures is crucial for the analysis, most of the available analysis methods discard the heterogeneous relations among the dimensions. In this paper, we introduce the construction and utilization of representative factors for the interactive visual analysis of structures in high-dimensional datasets. First, we present a selection of methods to investigate the sub-groups in the dimension set and associate representative factors with those groups of dimensions. Second, we introduce how these factors are included in the interactive visual analysis cycle together with the original dimensions. We then provide the steps of an analytical procedure that iteratively analyzes the datasets through the use of representative factors. We discuss how our methods improve the reliability and interpretability of the analysis process by enabling more informed selections of computational tools. Finally, we demonstrate our techniques on the analysis of brain imaging study results that are performed over a large group of subjects.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, visual method:Scatterplot, analysis method:Subspace, analysis method:Dimension Relationship, View Optimization, analysis method:Projection, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":184,
"title":"Brushing dimensions-a dual visual analysis model for high-dimensional data",
"abstract":"In many application fields, data analysts have to deal with datasets that contain many expressions per item. The effective analysis of such multivariate datasets is dependent on the user's ability to understand both the intrinsic dimensionality of the dataset as well as the distribution of the dependent values with respect to the dimensions. In this paper, we propose a visualization model that enables the joint interactive visual analysis of multivariate datasets with respect to their dimensions as well as with respect to the actual data values. We describe a dual setting of visualization and interaction in items space and in dimensions space. The visualization of items is linked to the visualization of dimensions with brushing and focus+context visualization. With this approach, the user is able to jointly study the structure of the dimensions space as well as the distribution of data items with respect to the dimensions. Even though the proposed visualization model is general, we demonstrate its application in the context of a DNA microarray data analysis.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, visual method:Scatterplot, analysis method:Subspace, analysis method:Dimension Relationship, View Optimization, analysis method:Projection, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":183,
"title":"Tuner: Principled Parameter Finding for Image Segmentation Algorithms Using Visual Response Surface Exploration",
"abstract":"none",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, Parameter Exploration, Segmentation, data type:Spatial Data, paper type:Technical, analysis method:Regression, visual method:Surfaces"
},
{
"code":182,
"title":"Balancing Interactive Data Management of Massive Data with Situational Awareness through Smart Aggregation",
"abstract":"Designing a visualization system capable of processing, managing, and presenting massive data sets while maximizing the user's situational awareness (SA) is a challenging, but important, research question in visual analytics. Traditional data management and interactive retrieval approaches have often focused on solving the data overload problem at the expense of the user's SA. This paper discusses various data management strategies and the strengths and limitations of each approach in providing the user with SA. A new data management strategy, coined Smart Aggregation, is presented as a powerful approach to overcome the challenges of both massive data sets and maintaining SA. By combining automatic data aggregation with user-defined controls on what, how, and when data should be aggregated, we present a visualization system that can handle massive amounts of data while affording the user with the best possible SA. This approach ensures that a system is always usable in terms of both system resources and human perceptual resources. We have implemented our Smart Aggregation approach in a visual analytics system called VIAssist (Visual Assistant for Information Assurance Analysis) to facilitate exploration, discovery, and SA in the domain of Information Assurance.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Hierarchy"
},
{
"code":181,
"title":"PaintingClass: interactive construction, visualization and exploration of decision trees",
"abstract":"Decision trees are commonly used for classification. We propose to use decision trees not just for classification but also for the wider purpose of knowledge discovery, because visualizing the decision tree can reveal much valuable information in the data. We introduce PaintingClass, a system for interactive construction, visualization and exploration of decision trees. PaintingClass provides an intuitive layout and convenient navigation of the decision tree. PaintingClass also provides the user the means to interactively construct the decision tree. Each node in the decision tree is displayed as a visual projection of the data. Through actual examples and comparison with other classification methods, we show that the user can effectively use PaintingClass to construct a decision tree and explore the decision tree to gain additional knowledge.",
"keywords":"pipeline stage:View Transformation, user involvement:Interactive Exploration, Machine Learning, visual method:Novel Visual Encoding, visual method:Scatterplot, data type:Nominal Data, visual method:Parallel Coordinates, paper type:Technical, analysis method:Projection"
},
{
"code":180,
"title":"Subspace search and visualization to make sense of alternative clusterings in high-dimensional data",
"abstract":"In explorative data analysis, the data under consideration often resides in a high-dimensional (HD) data space. Currently many methods are available to analyze this type of data. So far, proposed automatic approaches include dimensionality reduction and cluster analysis, whereby visual-interactive methods aim to provide effective visual mappings to show, relate, and navigate HD data. Furthermore, almost all of these methods conduct the analysis from a singular perspective, meaning that they consider the data in either the original HD data space, or a reduced version thereof. Additionally, HD data spaces often consist of combined features that measure different properties, in which case the particular relationships between the various properties may not be clear to the analysts a priori since it can only be revealed if appropriate feature combinations (subspaces) of the data are taken into consideration. Considering just a single subspace is, however, often not sufficient since different subspaces may show complementary, conjointly, or contradicting relations between data items. Useful information may consequently remain embedded in sets of subspaces of a given HD input data space. Relying on the notion of subspaces, we propose a novel method for the visual analysis of HD data in which we employ an interestingness-guided subspace search algorithm to detect a candidate set of subspaces. Based on appropriately defined subspace similarity functions, we visualize the subspaces and provide navigation facilities to interactively explore large sets of subspaces. Our approach allows users to effectively compare and relate subspaces with respect to involved dimensions and clusters of objects. We apply our approach to synthetic and real data sets. We thereby demonstrate its support for understanding HD data from different perspectives, effectively yielding a more complete view on HD data.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration,visual method:Parallel Coordinates, visual method:Scatterplot, analysis method:Subspace, View Optimization, Ranking, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":179,
"title":"Automated analytical methods to support visual exploration of high-dimensional data",
"abstract":"Visual exploration of multivariate data typically requires projection onto lower dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non-class-based scatterplots and parallel coordinates visualizations. The proposed analysis methods are evaluated on different data sets.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, visual method:Scatterplot, Ranking, analysis method:Subspace, View Optimization, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":178,
"title":"Combining automated analysis and visualization techniques for effective exploration of high-dimensional data",
"abstract":"Visual exploration of multivariate data typically requires projection onto lower-dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non class-based visual method:Scatterplots and visual method:Parallel Coordinates visualizations. The proposed analysis methods are evaluated on different datasets.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, visual method:Scatterplot, Ranking, analysis method:Subspace, View Optimization, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":177,
"title":"Visualization of Time-Series Data in Parameter Space for Understanding Facial Dynamics",
"abstract":"Over the past decade, computer scientists and psychologists have made great efforts to collect and analyze facial dynamics data that exhibit different expressions and emotions. Such data is commonly captured as videos and are transformed into feature-based time-series prior to any analysis. However, the analytical tasks, such as expression classification, have been hindered by the lack of understanding of the complex data space and the associated algorithm space. Conventional graph-based time-series visualization is also found inadequate to support such tasks. In this work, we adopt a visual analytics approach by visualizing the correlation between the algorithm space and our goal 鈥�classifying facial dynamics. We transform multiple feature-based time-series for each expression in measurement space to a multi-dimensional representation in parameter space. This enables us to utilize parallel coordinates visualization to gain an understanding of the algorithm space, providing a fast and cost-effective means to support the design of analytical algorithms.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:Time Series, analysis method:Feature Extraction, paper type:Technical, visual method:Parallel Coordinates, Parameter Exploration"
},
{
"code":176,
"title":"An Extension of Wilkinson's Algorithm for Positioning Tick Labels on Axes",
"abstract":"The non-data components of a visualization, such as axes and legends, can often be just as important as the data itself. They provide contextual information essential to interpreting the data. In this paper, we describe an automated system for choosing positions and labels for axis tick marks. Our system extends Wilkinson's optimization-based labeling approach to create a more robust, full-featured axis labeler. We define an expanded space of axis labelings by automatically generating additional nice numbers as needed and by permitting the extreme labels to occur inside the data range. These changes provide flexibility in problematic cases, without degrading quality elsewhere. We also propose an additional optimization criterion, legibility, which allows us to simultaneously optimize over label formatting, font size, and orientation. To solve this revised optimization problem, we describe the optimization function and an efficient search algorithm. Finally, we compare our method to previous work using both quantitative and qualitative metrics. This paper is a good example of how ideas from automated graphic design can be applied to information visualization.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Not Applicable, paper type:Technical, visual method:Scatterplot, User Study"
},
{
"code":175,
"title":"Applying Manifold Learning to Plotting Approximate Contour Trees",
"abstract":"A contour tree is a powerful tool for delineating the topological evolution of isosurfaces of a single-valued function, and thus has been frequently used as a means of extracting features from volumes and their time-varying behaviors. Several sophisticated algorithms have been proposed for constructing contour trees while they often complicate the software implementation especially for higher-dimensional cases such as time-varying volumes. This paper presents a simple yet effective approach to plotting in 3D space, approximate contour trees from a set of scattered samples embedded in the high-dimensional space. Our main idea is to take advantage of manifold learning so that we can elongate the distribution of high-dimensional data samples to embed it into a low-dimensional space while respecting its local proximity of sample points. The contribution of this paper lies in the introduction of new distance metrics to manifold learning, which allows us to reformulate existing algorithms as a variant of currently available dimensionality reduction scheme. Efficient reduction of data sizes together with segmentation capability is also developed to equip our approach with a coarse-to-fine analysis even for large-scale datasets. Examples are provided to demonstrate that our proposed scheme can successfully traverse the features of volumes and their temporal behaviors through the constructed contour trees.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Isosurface, analysis method:Distance Metric, analysis method:Dimensionality Reduction, visual method:Progressive Update, analysis method:Topological Analysis, paper type:Technical, data type:High-Dimensional Function"
},
{
"code":174,
"title":"GGobi: evolving from XGobi into an extensible framework for interactive data visualization",
"abstract":"GGobi is a direct descendent of a data visualization system called XGobi that has been around since the early 1990s. GGobi's new features include multiple plotting windows, a color lookup table manager, and an Extensible Markup Language file format for data. Perhaps the biggest advance is that GGobi can be easily extended, either by being embedded in other software or by the addition of plugins; either way, it can be controlled using an Application Programming Interface. An illustration of its extensibility is that it can be embedded in R. The result is a full marriage between GGobi's direct manipulation graphical environment and R's familiar extensible environment for statistical data analysis.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot, visual method:Animation"
},
{
"code":173,
"title":"Multiscale visualization using data cubes",
"abstract":"Most analysts start with an overview of the data before gradually refining their view to be more focused and detailed. Multiscale pan-and-zoom systems are effective because they directly support this approach. However, generating abstract overviews of large data sets is difficult and most systems take advantage of only one type of abstraction: visual abstraction. Furthermore, these existing systems limit the analyst to a single zooming path on their data and thus to a single set of abstract views. This paper presents: 1) a formalism for describing multiscale visualizations of data cubes with both data and visual abstraction and 2) a method for independently zooming along one or more dimensions by traversing a zoom graph with nodes at different levels of detail. As an example of how to design multiscale visualizations using our system, we describe four design patterns using our formalism. These design patterns show the effectiveness of multiscale visualization of general relational databases.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:hierarchy"
},
{
"code":172,
"title":"Polaris: a system for query, analysis, and visualization of multidimensional relational databases",
"abstract":"In the last several years, large multidimensional databases have become common in a variety of applications, such as data warehousing and scientific computing. Analysis and exploration tasks place significant demands on the interfaces to these databases. Because of the size of the data sets, dense graphical representations are more effective for exploration than spreadsheets and charts. Furthermore, because of the exploratory nature of the analysis, it must be possible for the analysts to change visualizations rapidly as they pursue a cycle involving first hypothesis and then experimentation. In this paper, we present Polaris, an interface for exploring large multidimensional databases that extends the well-known pivot table interface. The novel features of Polaris include an interface for constructing visual specifications of table-based graphical displays and the ability to generate a precise set of relational queries from the visual specifications. The visual specifications can be rapidly and incrementally developed, giving the analyst visual feedback as he constructs complex queries and visualizations",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Query, paper type:System, Visual Data Mining, data type:High-Dimensional Points, analysis method:not applicable, visual method:Scatterplot"
},
{
"code":171,
"title":"Configuring Hierarchical Layouts to Address Research Questions",
"abstract":"We explore the effects of selecting alternative layouts in hierarchical displays that show multiple aspects of large multivariate datasets, including spatial and temporal characteristics. Hierarchical displays of this type condition a dataset by multiple discrete variable values, creating nested graphical summaries of the resulting subsets in which size, shape and colour can be used to show subset properties. These 'small multiples' are ordered by the conditioning variable values and are laid out hierarchically using dimensional stacking. Crucially, we consider the use of different layouts at different hierarchical levels, so that the coordinates of the plane can be used more effectively to draw attention to trends and anomalies in the data. We argue that these layouts should be informed by the type of conditioning variable and by the research question being explored. We focus on space-filling rectangular layouts that provide data-dense and rich overviews of data to address research questions posed in our exploratory analysis of spatial and temporal aspects of property sales in London. We develop a notation ('HiVE') that describes visualisation and layout states and provides reconfiguration operators, demonstrate its use for reconfiguring layouts to pursue research questions and provide guidelines for this process. We demonstrate how layouts can be related through animated transitions to reduce the cognitive load associated with their reconfiguration whilst supporting the exploratory process.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, visual method:Hierarchy, visual method:Novel Visual Encoding, paper type:Technical, data type:High-Dimensional Points, analysis method:Clustering"
},
{
"code":170,
"title":"Selecting good views of high-dimensional data using class consistency",
"abstract":"Many visualization techniques involve mapping high-dimensional data spaces to lower-dimensional views. Unfortunately, mapping a high-dimensional data space into a scatterplot involves a loss of information; or, even worse, it can give a misleading picture of valuable structure in higher dimensions. In this paper, we propose class consistency as a measure of the quality of the mapping. Class consistency enforces the constraint that classes of n-D data are shown clearly in 2-D scatterplots. We propose two quantitative measures of class consistency, one based on the distance to the class's center of gravity, and another based on the entropies of the spatial distributions of classes. We performed an experiment where users choose good views, and show that class consistency has good precision and recall. We also evaluate both consistency measures over a range of data sets and show that these measures are efficient and robust.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Clustering, View Optimization, Ranking, visual method:Hierarchy, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":169,
"title":"User Interfaces for the Exploration of Hierarchical Multi-dimensional Data",
"abstract":"A variety of user interfaces have been developed to support the querying of hierarchical multi-dimensional data in an OLAP setting such as pivot tables and Polaris. They are used to regularly check portions of a dataset and to explore a new dataset for the first time. In this paper, we establish criteria for OLAP user interface capabilities to facilitate comparison. Two criteria are the number of displayed dimensions along which comparisons can be made and the number of dimensions that are viewable at once - visual comparison depth and width. We argue that interfaces with greater visual comparison depth support regular checking of known data by users that know roughly where to look, while interfaces with greater comparison width support exploration of new data by users that have no a priori starting point and need to scan all dimensions. Pivot tables and Polaris are examples of the former. The main contribution of this paper is to introduce a new scalable interface that uses parallel dimension axis which supports the latter, greater visual comparison width. We compare our approach to both table based and parallel coordinate based interfaces. We present an implementation of our interface SGViewer, user scenarios and provide an evaluation that supports the usability of our interface",
"keywords":"pipeline stage:View Transformation, user involvement:Interactive Exploration, visual method:Hierarchy, visual method:Novel Visual Encoding, paper type:Technical, data type:High-Dimensional Points, analysis method:Clustering"
},
{
"code":168,
"title":"Supporting Exploratory Analysis with the Select Slice Table",
"abstract":"In interactive visualization, selection techniques such as dynamic queries and brushing are used to specify and extract items of interest. In other words, users define areas of interest in data space that often have a clear semantic meaning. We call such areas Semantic Zones, and argue that support for their manipulation and reasoning with them is highly useful during exploratory analysis. An important use case is the use of these zones across different subsets of the data, for instance to study the population of semantic zones over time. To support this, we present the Select & Slice Table. Semantic zones are arranged along one axis of the table, and data subsets are arranged along the other axis of the table. Each cell contains a set of items of interest from a data subset that matches the selection specifications of a zone. Items in cells can be visualized in various ways, as a count, as an aggregation of a measure, or as a separate visualization, such that the table gives an overview of the relationship between zones and data subsets. Furthermore, users can reuse zones, combine zones, and compare and trace items of interest across different semantic zones and data subsets. We present two case studies to illustrate the support offered by the Select & Slice table during exploratory analysis of multivariate data.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":167,
"title":"Vectorized Radviz and Its Application to Multiple Cluster Datasets",
"abstract":"Radviz is a radial visualization with dimensions assigned to points called dimensional anchors (DAs) placed on the circumference of a circle. Records are assigned locations within the circle as a function of its relative attraction to each of the DAs. The DAs can be moved either interactively or algorithmically to reveal different meaningful patterns in the dataset. In this paper we describe Vectorized Radviz (VRV) which extends the number of dimensions through data flattening. We show how VRV increases the power of Radviz through these extra dimensions by enhancing the flexibility in the layout of the DAs. We apply VRV to the problem of analyzing the results of multiple clusterings of the same data set, called multiple cluster sets or cluster ensembles. We show how features of VRV help discern patterns across the multiple cluster sets. We use the Iris data set to explain VRV and a newt gene microarray data set used in studying limb regeneration to show its utility. We then discuss further applications of VRV.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Clustering, analysis method:Dimension Relationship, paper type:Technical, visual method:RadViz, data type:High-Dimensional Points"
},
{
"code":166,
"title":"Visual Analysis of Higher-Order Conjunctive Relationships in Multidimensional Data Using a Hypergraph Query System",
"abstract":"Visual exploration and analysis of multidimensional data becomes increasingly difficult with increasing dimensionality. We want to understand the relationships between dimensions of data, but lack flexible techniques for exploration beyond low-order relationships. Current visual techniques for multidimensional data analysis focus on binary conjunctive relationships between dimensions. Recent techniques, such as cross-filtering on an attribute relationship graph, facilitate the exploration of some higher-order conjunctive relationships, but require a great deal of care and precision to do so effectively. This paper provides a detailed analysis of the expressive power of existing visual querying systems and describes a more flexible approach in which users can explore n-ary conjunctive inter- and intra- dimensional relationships by interactively constructing queries as visual hypergraphs. In a hypergraph query, nodes represent subsets of values and hyperedges represent conjunctive relationships. Analysts can dynamically build and modify the query using sequences of simple interactions. The hypergraph serves not only as a query specification, but also as a compact visual representation of the interactive state. Using examples from several domains, focusing on the digital humanities, we describe the design considerations for developing the querying system and incorporating it into visual analysis tools. We analyze query expressiveness with regard to the kinds of questions it can and cannot pose, and describe how it simultaneously expands the expressiveness of and is complemented by cross-filtering.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Query, paper type:Technical, data type:Nominal Data, visual method:Node-link, visual method:Scatterplot, analysis method:Dimension Similarity, Filtering"
},
{
"code":165,
"title":"Knowledge discovery in high-dimensional data: Case studies and a user survey for the rank-by-feature framework",
"abstract":"Knowledge discovery in high-dimensional data is a challenging enterprise, but new visual analytic tools appear to offer users remarkable powers if they are ready to learn new concepts and interfaces. Our three-year effort to develop versions of the hierarchical clustering explorer (HCE) began with building an interactive tool for exploring clustering results. It expanded, based on user needs, to include other potent analytic and visualization tools for multivariate data, especially the rank-by-feature framework. Our own successes using HCE provided some testimonial evidence of its utility, but we felt it necessary to get beyond our subjective impressions. This paper presents an evaluation of the hierarchical clustering explorer (HCE) using three case studies and an e-mail user survey (n",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, analysis method:Clustering, visual method:Hierarchy, paper type:Application, Ranking, data type:High-Dimensional Points"
},
{
"code":164,
"title":"A rank-by-feature framework for unsupervised multidimensional data exploration using low dimensional projections",
"abstract":"Exploratory analysis of multidimensional data sets is challenging because of the difficulty in comprehending more than three dimensions. Two fundamental statistical principles for the exploratory analysis are (1) to examine each dimension first and then find relationships among dimensions, and (2) to try graphical displays first and then find numerical summaries (D.S. Moore, (1999). We implement these principles in a novel conceptual framework called the rank-by-feature framework. In the framework, users can choose a ranking criterion interesting to them and sort 1D or 2D axis-parallel projections according to the criterion. We introduce the rank-by-feature prism that is a color-coded lower-triangular matrix that guides users to desired features. Statistical graphs (histogram, boxplot, and scatterplot) and information visualization techniques (overview, coordination, and dynamic query) are combined to help users effectively traverse 1D and 2D axis-parallel projections, and finally to help them interactively find interesting features",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Clustering, visual method:Hierarchy, paper type:Technical, Ranking, Data Type:High-Dimensional Points, data type:High-Dimensional Points"
},
{
"code":163,
"title":"DD-HDS: A method for visualization and exploration of high-dimensional data",
"abstract":"Mapping high-dimensional data in a low-dimensional space, for example, for visualization, is a problem of increasingly major concern in data analysis. This paper presents data-driven high-dimensional scaling (DD-HDS), a nonlinear mapping method that follows the line of multidimensional scaling (MDS) approach, based on the preservation of distances between pairs of data. It improves the performance of existing competitors with respect to the representation of high-dimensional data, in two ways. It introduces (1) a specific weighting of distances between data taking into account the concentration of measure phenomenon and (2) a symmetric handling of short distances in the original and output spaces, avoiding false neighbor representations while still allowing some necessary tears in the original distribution. More precisely, the weighting is set according to the effective distribution of distances in the data set, with the exception of a single user-defined parameter setting the tradeoff between local neighborhood preservation and global mapping. The optimization of the stress criterion designed for the mapping is realized by ldquoforce-directed placementrdquo (FDP). The mappings of low- and high-dimensional data sets are presented as illustrations of the features and advantages of the proposed algorithm. The weighting function specific to high-dimensional data and the symmetric handling of short distances can be easily incorporated in most distance preservation-based nonlinear dimensionality reduction methods.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Dimensionality Reduction, analysis method:Distance Metric, analysis method:Precision Measure, visual method:Surfaces,paper type:Technical, data type:Nominal Data"
},
{
"code":162,
"title":"Empirical guidance on scatterplot and dimension reduction technique choices",
"abstract":"To verify cluster separation in high-dimensional data, analysts often reduce the data with a dimension reduction (DR) technique, and then visualize it with 2D visual method:Scatterplots, interactive 3D visual method:Scatterplots, or visual method:Scatterplot Matrices (SPLOMs). With the goal of providing guidance between these visual encoding choices, we conducted an empirical data study in which two human coders manually inspected a broad set of 816 scatterplots derived from 75 datasets, 4 DR techniques, and the 3 previously mentioned scatterplot techniques. Each coder scored all color-coded classes in each scatterplot in terms of their separability from other classes. We analyze the resulting quantitative data with a heatmap approach, and qualitatively discuss interesting scatterplot examples. Our findings reveal that 2D scatterplots are often 'good enough', that is, neither SPLOM nor interactive 3D adds notably more cluster separability with the chosen DR technique. If 2D is not good enough, the most promising approach is to use an alternative DR technique in 2D. Beyond that, SPLOM occasionally adds additional value, and interactive 3D rarely helps but often hurts in terms of poorer class separation and usability. We summarize these results as a workflow model and implications for design. Our results offer guidance to analysts during the DR exploration process.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Dimensionality Reduction, visual method:Scatterplot, paper type:Survey, paper type:Theory, data type:High-Dimensional Points"
},
{
"code":161,
"title":"Techniques for precision-based visual analysis of projected data",
"abstract":"The analysis of high-dimensional data is an important, yet inherently difficult problem. analysis method:Projection techniques such as PCA, MDS, and SOM can be used to map high-dimensional data to 2D display space. However, projections typically incur a loss in information. Often uncertainty exists regarding the precision of the projection as compared with its original data characteristics. While the output quality of these projection techniques can be discussed in terms of algorithmic assessment, visualization is often helpful for better understanding the results. We address the visual assessment of projection precision by an approach integrating an appropriately designed projection precision measure directly into the projection visualization. To this end, a flexible projection precision measure is defined that allows the user to balance the degree of locality at which the measure is evaluated. Several visual mappings are designed for integrating the precision measure into the projection visualization at various levels of abstraction. The techniques are implemented in a fully interactive system which is practically applied on several data sets. We demonstrate the usefulness of the approach for visual analysis of classified and clustered high-dimensional data sets. We thereby show how our novel interactive precision quality visualization system helps to examine preservation of closeness of the data in original space into the low-dimensional space.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Dimensionality Reduction, analysis method:Precision Measure, paper type:Technical, data type:High-Dimensional Points, visual method:Scatterplot"
},
{
"code":160,
"title":"Pixnostics: Towards measuring the value of visualization",
"abstract":"During the last two decades a wide variety of advanced methods for the visual exploration of large data sets have been proposed. For most of these techniques user interaction has become a crucial element, since there are many situations in which a user or an analyst has to select the right parameter settings from among many or select a subset of the available attribute space for the visualization process, in order to construct valuable visualizations that provide insight, into the data and reveal interesting patterns. The right choice of input parameters is often essential, since suboptimal parameter settings or the investigation of irrelevant data dimensions make the exploration process more time consuming and may result in wrong conclusions. In this paper we propose a novel method for automatically determining meaningful parameter- and attribute settings based on the information content of the resulting visualizations. Our technique called Pixnostics, in analogy to Scagnostics (Wilkinson et al., 2005), automatically analyses pixel images resulting from diverse parameter mappings and ranks them according to the potential value for the user. This allows a more effective and more efficient visual data analysis process, since the attribute/parameter space is reduced to meaningful selections and thus the analyst obtains faster insight into the data. Real world applications are provided to show the benefit of the proposed approach.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, Parameter Exploration, Ranking, paper type:Technical, visual method:Pixel-based, data type:Nominal Data, data type:High-dimensional Points, analysis method:Projection"
},
{
"code":159,
"title":"Illuminated 3D Scatterplots",
"abstract":"In contrast to 2D scatterplots, the existing 3D variants have the advantage of showing one additional data dimension, but suffer from inadequate spatial and shape perception and therefore are not well suited to display structures of the underlying data. We improve shape perception by applying a new illumination technique to the pointcloud representation of 3D scatterplots. Points are classified as locally linear, planar, and volumetric structures鈥攁ccording to the eigenvalues of the inverse distance-weighted covariance matrix at each data element. Based on this classification, different lighting models are applied: codimension-2 illumination, surface illumination, and emissive volumetric illumination. Our technique lends itself to efficient GPU point rendering and can be combined with existing methods like semi-transparent rendering, halos, and depth or attribute based color coding. The user can interactively navigate in the dataset and manipulate the classification and other visualization parameters. We demonstrate our visualization technique by showing examples of multi-dimensional data and of generic pointcloud data.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":158,
"title":"3D Scatterplot Navigation",
"abstract":"For 3D scatterplots, we present an interpolation and projection technique that supports the smooth exchange of one or two data dimensions at a time. Even though this exchange can be considered as a rotation in 4D or 5D data domains, we guarantee that the projection to image space is perceived as a 3D rigid body rotation-with a consistent motion of the data points. We conducted a controlled user study showing that 3D rigid body rotations outperform direct transition between scatterplots. We further extend our technique to support navigation between 3D scatterplots by introducing 3D scatterplot matrices. The usefulness of our approach is demonstrated by application examples, including a case study with a natural language processing expert.",
"keywords":"pipeline stage:View Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":157,
"title":"A virtual workspace for hybrid multidimensional scaling algorithms",
"abstract":"In visualising multidimensional data, it is well known that different types of algorithms to process them. Data sets might be distinguished according to volume, variable types and distribution, and each of these characteristics imposes constraints upon the choice of applicable algorithms for their visualization. Previous work has shown that a hybrid algorithmic approach can be successful in addressing the impact of data volume on the feasibility of multidimensional scaling (MDS). This suggests that hybrid combinations of appropriate algorithms might also successfully address other characteristics of data. This paper presents a system and framework in which a user can easily explore hybrid algorithms and the data flowing through them. Visual programming and a novel algorithmic architecture let the user semi-automatically define data flows and the co-ordination of multiple views.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Dimensionality Reduction, paper type:System, visual method:Scatterplot, data type:High-dimensional Points"
},
{
"code":156,
"title":"Progressive parallel coordinates",
"abstract":"Progressive refinement is a methodology that makes it possible to elegantly integrate scalable data compression, access, and presentation into one approach. Specifically, this paper concerns the effective use of progressive parallel coordinates (PPCs), utilized routinely for high-dimensional data visualization. It discusses how the power of the typical stages of progressive data visualization can also be utilized fully for PPCs. Further, different implementations of the underlying methods and potential application domains are described. The paper also presents empirical results concerning the benefits of PPC with regard to efficient data management and improved presentation, indicating that the proposed approach is able to close the gap between data handling and visualization. ,",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration,visual method:Parallel Coordinates, visual method:Progressive Update, visual method:Hierarchy, paper type:Technical, data type:High-Dimensional Points, analysis method:Data Abstraction"
},
{
"code":155,
"title":"Mapping nominal values to numbers for effective visualization",
"abstract":"Data sets with a large numbers of nominal variables, including some with large number of distinct values, are becoming increasingly common and need to be explored. Unfortunately, most existing visual exploration tools are designed to handle numeric variables only. When importing data sets with nominal values into such visualization tools, most solutions to date are rather simplistic. Often, techniques that map nominal values to numbers do not assign order or spacing among the values in a manner that conveys semantic relationships. Moreover, displays designed for nominal variables usually cannot handle high cardinality variables well. This paper addresses the problem of how to display nominal variables in general-purpose visual exploration tools designed for numeric variables. Specifically, we investigate (1) how to assign order and spacing among the nominal values, and (2) how to reduce the number of distinct values to display. We propose a new technique, called the Distance-Quantification-Classing (DQC) approach, to preprocess nominal variables before being imported into a visual exploration tool. In the Distance Step, we identify a set of independent dimensions that can be used to calculate the distance between nominal values. In the Quantification Step, we use the independent dimensions and the distance information to assign order and spacing among the nominal values. In the Classing Step, we use results from the previous steps to determine which values within the domain of a variable are similar to each other and thus can be grouped together. Each step in the DQC approach can be accomplished by a variety of techniques. We extended the XmdvTool package to incorporate this approach. We evaluated our approach on several data sets using a variety of measures.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric,visual method:Parallel Coordinates, data type:Nominal Data, analysis method:Distance Metric, paper type:Technical"
},
{
"code":154,
"title":"Visualizing high-dimensional predictive model quality",
"abstract":"Using inductive learning techniques to construct classification models from large, high-dimensional data sets is a useful way to make predictions in complex domains. However, these models can be difficult for users to understand. We have developed a set of visualization methods that help users to understand and analyze the behavior of learned models, including techniques for high-dimensional data space projection, display of probabilistic predictions, variable/class correlation, and instance mapping. We show the results of applying these techniques to models constructed from a benchmark data set of census data, and draw conclusions about the utility of these methods for model understanding.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, analysis method:Clustering, analysis method:Projection, Machine Learning, paper type:Technical, data type:High-dimensional Points, visual method:Surfaces"
},
{
"code":153,
"title":"The perception of correlation in scatterplots",
"abstract":"We present a rigorous way to evaluate the visual perception of correlation in scatterplots, based on classical psychophysical methods originally developed for simple properties such as brightness. Although scatterplots are graphically complex, the quantity they convey is relatively simple. As such, it may be possible to assess the perception of correlation in a similar way.visual method:Scatterplots were each of 5.0掳 extent, containing 100 points with a bivariate normal distribution. Means were 0.5 of the range of the points, and standard deviations 0.2 of this range. Precision was determined via an adaptive algorithm to find the just noticeable differences (jnds) in correlation, i.e., the difference between two side-by-side scatterplots that could be discriminated 75 percent of the time. Accuracy was measured by direct estimation, using reference scatterplots with fixed upper and lower values, with a test scatterplot adjusted so that its correlation appeared to be halfway between these. This process was recursively applied to yield several further estimates.Results of the discrimination tests show jnd(r)",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, visual method:Scatterplot, Perception, paper type:Technical, data type:High-Dimensional Points, analysis method:Dimension Relationship"
},
{
"code":152,
"title":"Generating hypotheses of trends in high-dimensional data skeletons",
"abstract":"We seek an information-revealing representation for high-dimensional data distributions that may contain local trends in certain subspaces. Examples are data that have continuous support in simple shapes with identifiable branches. Such data can be represented by a graph that consists of segments of locally fit principal curves or surfaces summarizing each identifiable branch. We describe a new algorithm to find the optimal paths through such a principal graph. The paths are optimal in the sense that they represent the longest smooth trends through the data set, and jointly cover the data set entirely with minimum overlap. The algorithm is suitable for hypothesizing trends in high-dimensional data, and can assist exploratory data analysis and visualization.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Regression, analysis method:Subspace, analysis method:Dimensionality Reduction, paper type:Technical, visual method:Skeleton, data type:High-Dimension Function"
},
{
"code":151,
"title":"Visual Analysis of the Air Pollution Problem in Hong Kong",
"abstract":"We present a comprehensive system for weather data visualization. Weather data are multivariate and contain vector fields formed by wind speed and direction. Several well-established visualization techniques such as parallel coordinates and polar systems are integrated into our system. We also develop various novel methods, including circular pixel bar charts embedded into polar systems, enhanced parallel coordinates with S-shape axis, and weighted complete graphs. Our system was used to analyze the air pollution problem in Hong Kong and some interesting patterns have been found.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Not Applicable, paper type:Technical, visual method:pixel-based, visual method:Parallel Coordinates"
},
{
"code":150,
"title":"Visualization of Parameter Space for Image Analysis",
"abstract":"Image analysis algorithms are often highly parameterized and much human input is needed to optimize parameter settings. This incurs a time cost of up to several days. We analyze and characterize the conventional parameter optimization process for image analysis and formulate user requirements. With this as input, we propose a change in paradigm by optimizing parameters based on parameter sampling and interactive visual exploration. To save time and reduce memory load, users are only involved in the first step - initialization of sampling - and the last step - visual analysis of output. This helps users to more thoroughly explore the parameter space and produce higher quality results. We describe a custom sampling plug-in we developed for CellProfiler - a popular biomedical image analysis framework. Our main focus is the development of an interactive visualization technique that enables users to analyze the relationships between sampled input parameters and corresponding output. We implemented this in a prototype called Paramorama. It provides users with a visual overview of parameters and their sampled values. User-defined areas of interest are presented in a structured way that includes image-based output and a novel layout algorithm. To find optimal parameter settings, users can tag high- and low-quality results to refine their search. We include two case studies to illustrate the utility of this approach.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, Parameter Exploration, data type:Spatial Data, paper type:Technical, analysis method:Clustering,visual method:Hierarchy"
},
{
"code":149,
"title":"Visualizing Summary Statistics and Uncertainty",
"abstract":"Systems projecting a continuous n-dimensional parameter space to a continuous m-dimensional target space play an important role in science and engineering. If evaluating the system is expensive, however, an analysis is often limited to a small number of sample points. The main contribution of this paper is an interactive approach to enable a continuous analysis of a sampled parameter space with respect to multiple target values. We employ methods from statistical learning to predict results in real-time at any user-defined point and its neighborhood. In particular, we describe techniques to guide the user to potentially interesting parameter regions, and we visualize the inherent uncertainty of predictions in 2D scatterplots and parallel coordinates. An evaluation describes a real-world scenario in the application context of car engine design and reports feedback of domain experts. The results indicate that our approach is suitable to accelerate a local sensitivity analysis of multiple target dimensions, and to determine a sufficient local sampling density for interesting parameter regions.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Glyphs, visual method:Novel Visual Encoding, Uncertainty, Perception, paper type:Technical, data type:High-Dimensional Points, visual method:Skeleton, analysis method:not applicable"
},
{
"code":148,
"title":"iLAMP: Exploring high-dimensional spacing through backward multidimensional projection",
"abstract":"Ever improving computing power and technological advances are greatly augmenting data collection and scientific observation. This has directly contributed to increased data complexity and dimensionality, motivating research of exploration techniques for multidimensional data. Consequently, a recent influx of work dedicated to techniques and tools that aid in understanding multidimensional datasets can be observed in many research fields, including biology, engineering, physics and scientific computing. While the effectiveness of existing techniques to analyze the structure and relationships of multidimensional data varies greatly, few techniques provide flexible mechanisms to simultaneously visualize and actively explore high-dimensional spaces. In this paper, we present an inverse linear affine multidimensional projection, coined iLAMP, that enables a novel interactive exploration technique for multidimensional data. iLAMP operates in reverse to traditional projection methods by mapping low-dimensional information into a high-dimensional space. This allows users to extrapolate instances of a multidimensional dataset while exploring a projection of the data to the planar domain. We present experimental results that validate iLAMP, measuring the quality and coherence of the extrapolated data; as well as demonstrate the utility of iLAMP to hypothesize the unexplored regions of a high-dimensional space.",
"keywords":"pipeline stage:Data Transformation, user involvement:Model Manipulation, analysis method:Dimensionality Reduction, View Optimization, paper type:Technical,data type:High-dimensional Points, visual method:Scatterplot"
},
{
"code":147,
"title":"A Framework for Exploring Multidimensional Data with 3D Projections",
"abstract":"Visualization of high-dimensional data requires a mapping to a visual space. Whenever the goal is to preserve similarity relations a frequent strategy is to use 2D projections, which afford intuitive interactive exploration, e.g., by users locating and selecting groups and gradually drilling down to individual objects. In this paper, we propose a framework for projecting high-dimensional data to 3D visual spaces, based on a generalization of the Least-Square Projection (LSP). We compare projections to 2D and 3D visual spaces both quantitatively and through a user study considering certain exploration tasks. The quantitative analysis confirms that 3D projections outperform 2D projections in terms of precision. The user study indicates that certain tasks can be more reliably and confidently answered with 3D projections. Nonetheless, as 3D projections are displayed on 2D screens, interaction is more difficult. Therefore, we incorporate suitable interaction functionalities into a framework that supports 3D transformations, predefined optimal 2D views, coordinated 2D and 3D views, and hierarchical 3D cluster definition and exploration. For visually encoding data clusters in a 3D setup, we employ color coding of projected data points as well as four types of surface renderings. A second user study evaluates the suitability of these visual encodings. Several examples illustrate the framework's applicability for both visual exploration of multidimensional abstract (non-spatial) data as well as the feature space of multi-variate spatial data.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot, User Study"
},
{
"code":146,
"title":"Hypermoval: Interactive Visual Validation of Regression Models for Real-time Simulation",
"abstract":"During the development of car engines, regression models that are based on machine learning techniques are increasingly important for tasks which require a prediction of results in real-time. While the validation of a model is a key part of its identification process, existing computation- or visualization-based techniques do not adequately support all aspects of model validation. The main contribution of this paper is an interactive approach called HyperMoVal that is designed to support multiple tasks related to model validation: 1) comparing known and predicted results, 2) analyzing regions with a bad fit, 3) assessing the physical plausibility of models also outside regions covered by validation data, and 4) comparing multiple models. The key idea is to visually relate one or more n-dimensional scalar functions to known validation data within a combined visualization. HyperMoVal lays out multiple 2D and 3D sub-projections of the n-dimensional function space around a focal point. We describe how linking HyperMoVal to other views further extends the possibilities for model validation. Based on this integration, we discuss steps towards supporting the entire workflow of identifying regression models. An evaluation illustrates a typical workflow in the application context of car-engine design and reports general feedback of domain experts and users of our approach. These results indicate that our approach significantly accelerates the identification of regression models and increases the confidence in the overall engineering",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, analysis method:Regression, paper type:Technical, data type:High-dimensional Function, visual method:Scatterplot, visual method:Surfaces, visual method:Parallel Coordinates"
},
{
"code":145,
"title":"Visualization of Diversity in Large Multivariate Data Sets",
"abstract":"Understanding the diversity of a set of multivariate objects is an important problem in many domains, including ecology, college admissions, investing, machine learning, and others. However, to date, very little work has been done to help users achieve this kind of understanding. Visual representation is especially appealing for this task because it offers the potential to allow users to efficiently observe the objects of interest in a direct and holistic way. Thus, in this paper, we attempt to formalize the problem of visualizing the diversity of a large (more than 1000 objects), multivariate (more than 5 attributes) data set as one worth deeper investigation by the information visualization community. In doing so, we contribute a precise definition of diversity, a set of requirements for diversity visualizations based on this definition, and a formal user study design intended to evaluate the capacity of a visual representation for communicating diversity information. Our primary contribution, however, is a visual representation, called the Diversity Map, for visualizing diversity. An evaluation of the Diversity Map using our study design shows that users can judge elements of diversity consistently and as or more accurately than when using the only other representation specifically designed to visualize diversity.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, paper type:Technical, visual method:Heat Map, User Study, analysis method:Histogram"
},
{
"code":144,
"title":"Clutter reduction in multi-dimensional data visualization using dimension reordering",
"abstract":"Visual clutter denotes a disordered collection of graphical entities in information visualization. Clutter can obscure the structure present in the data. Even in a small dataset, clutter can make it hard for the viewer to find patterns, relationships and structure. In this paper, we define visual clutter as any aspect of the visualization that interferes with the viewer's understanding of the data, and present the concept of clutter-based dimension reordering. Dimension order is an attribute that can significantly affect a visualization's expressiveness. By varying the dimension order in a display, it is possible to reduce clutter without reducing information content or modifying the data in any way. Clutter reduction is a display-dependent task. In this paper, we follow a three-step procedure for four different visualization techniques. For each display technique, first, we determine what constitutes clutter in terms of display properties; then we design a metric to measure visual clutter in this display; finally we search for an order that minimizes the clutter in a display",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, Reordering, Clutter Reduction,visual method:Parallel Coordinates, visual method:Scatterplot, Visual method:Scatterplot, paper type:Technical, data type:High-Dimensional Points, analysis method:Dimension Relationship"
},
{
"code":143,
"title":"Two-Phase Mapping for Projecting Massive Data Sets",
"abstract":"Most multidimensional projection techniques rely on distance (dissimilarity) information between data instances to embed high-dimensional data into a visual space. When data are endowed with Cartesian coordinates, an extra computational effort is necessary to compute the needed distances, making multidimensional projection prohibitive in applications dealing with interactivity and massive data. The novel multidimensional projection technique proposed in this work, called Part-Linear Multidimensional Projection (PLMP), has been tailored to handle multivariate data represented in Cartesian high-dimensional spaces, requiring only distance information between pairs of representative samples. This characteristic renders PLMP faster than previous methods when processing large data sets while still being competitive in terms of precision. Moreover, knowing the range of variation for data instances in the high-dimensional space, we can make PLMP a truly streaming data projection technique, a trait absent in previous methods.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Projection, analysis method:Dimensionality Reduction, paper type:Technical, visual method:Scatterplot"
},
{
"code":142,
"title":"Least Square Projection: A Fast High-Precision Multidimensional Projection Technique and Its Application to Document Mapping",
"abstract":"The problem of projecting multidimensional data into lower dimensions has been pursued by many researchers due to its potential application to data analyses of various kinds. This paper presents a novel multidimensional projection technique based on least square approximations. The approximations compute the coordinates of a set of projected points based on the coordinates of a reduced number of control points with defined geometry. We name the technique least square projections (LSP). From an initial projection of the control points, LSP defines the positioning of their neighboring points through a numerical solution that aims at preserving a similarity relationship between the points given by a metric in mD. In order to perform the projection, a small number of distance calculations are necessary, and no repositioning of the points is required to obtain a final solution with satisfactory precision. The results show the capability of the technique to form groups of points by degree of similarity in 2D. We illustrate that capability through its application to mapping collections of textual documents from varied sources, a strategic yet difficult application. LSP is faster and more accurate than other existing high-quality methods, particularly where it was mostly tested, that is, for mapping text sets.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot"
},
{
"code":141,
"title":"Piece wise Laplacian-based Projection for Interactive Data Exploration and Organization",
"abstract":"Multidimensional projection has emerged as an important visualization tool in applications involving the visual analysis of high-dimensional data. However, high precision projection methods are either computationally expensive or not flexible enough to enable feedback from user interaction into the projection process. A built-in mechanism that dynamically adapts the projection based on direct user intervention would make the technique more useful for a larger range of applications and data sets. In this paper we propose the Piecewise Laplacian-based Projection (PLP), a novel multidimensional projection technique, that, due to the local nature of its formulation, enables a versatile mechanism to interact with projected data and to allow interactive changes to alter the projection map dynamically, a capability unique of this technique. We exploit the flexibility provided by PLP in two interactive projection-based applications, one designed to organize pictures visually and another to build music playlists. These applications illustrate the usefulness of PLP in handling high-dimensional data in a flexible and highly visual way. We also compare PLP with the currently most promising projections in terms of precision and speed, showing that it performs very well also according to these quality criteria.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimensionality Reduction, paper type:Technical, visual method:Scatterplot"
},
{
"code":140,
"title":"Robust On-line Computation of Reeb Graphs: Simplicity and Speed",
"abstract":"Reeb graphs are a fundamental data structure for understanding and representing the topology of shapes. They are used in computer graphics, solid modeling, and visualization for applications ranging from the computation of similarities and finding defects in complex models to the automatic selection of visualization parameters. We introduce an on-line algorithm that reads a stream of elements (vertices, triangles, tetrahedra, etc.) and continuously maintains the Reeb graph of all elements already reed. The algorithm is robust in handling non-manifold meshes and general in its applicability to input models of any dimension.Optionally, we construct a skeleton-like embedding of the Reeb graph, and/or remove topological noise to reduce the output size. For interactive multi-resolution navigation we also build a hierarchical data structure which allows real-time extraction of approximated Reeb graphs containing all topological features above a given error threshold. Our extensive experiments show both high performance and practical linear scalability for meshes ranging from thousands to hundreds of millions of triangles. We apply our algorithm to the largest, most general, triangulated surfaces available to us, including $3$D, $4$D and $5$D simplicial meshes. To demonstrate one important application we use Reeb graphs to find and highlight topological defects in meshes, including some widely believed to be ``clean.''",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Topological Analysis, paper type:Technical, visual method:Graph"
},
{
"code":139,
"title":"From visual data exploration to visual data mining: A survey",
"abstract":"We survey work on the different uses of graphical mapping and interaction techniques for visual data mining of large data sets represented as table data. Basic terminology related to data mining, data sets, and visualization is introduced. Previous work on information visualization is reviewed in light of different categorizations of techniques and systems. The role of interaction techniques is discussed, in addition to work addressing the question of selecting and evaluating visualization techniques. We review some representative work on the use of information visualization techniques in the context of mining data. This includes both visual data exploration and visually expressing the outcome of specific mining algorithms. We also review recent innovative approaches that attempt to integrate visualization into the DM/KDD process, using it to enhance user interaction and comprehension.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, Visual Data Mining, paper type:Survey, data type:High-Dimensional Points, analysis method:not applicable, visual method:not applicable"
},
{
"code":138,
"title":"Visualization of High-Dimensional Point Clouds Using Their Density Distribution's Topology",
"abstract":"We present a novel method to visualize multidimensional point clouds. While conventional visualization techniques, like scatterplot matrices or parallel coordinates, have issues with either overplotting of entities or handling many dimensions, we abstract the data using topological methods before presenting it. We assume the input points to be samples of a random variable with a high-dimensional probability distribution which we approximate using kernel density estimates on a suitably reconstructed mesh. From the resulting scalar field we extract the join tree and present it as a topological landscape, a visualization metaphor that utilizes the human capability of understanding natural terrains. In this landscape, dense clusters of points show up as hills. The nesting of hills indicates the nesting of clusters. We augment the landscape with the data points to allow selection and inspection of single points and point sets. We also present optimizations to make our algorithm applicable to large data sets and to allow interactive adaption of our visualization to the kernel window width used in the density estimation.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Topological Analysis, paper type:Technical, visual method:Hierarchy"
},
{
"code":137,
"title":"Visual analysis of high dimensional point clouds using topological landscapes",
"abstract":"In this paper, we present a novel three-stage process to visualize the structure of point clouds in arbitrary dimensions. To get insight into the structure and complexity of a data set, we would most preferably just look into it, e.g. by plotting its corresponding point cloud. Unfortunately, for orthogonal scatter plots, this only works up to three dimensions, and other visualizations, like parallel coordinates or scatterplot matrices, also have problems handling many dimensions and visual overlap of data entities. The presented solution tackles the problem of visualizing point clouds indirectly by visualizing the topology of their density distribution. The benefit of this approach is that this topology can be computed in arbitrary dimensions. Similar to examining scatter plots, this gives the important information like the number, size and nesting structure of accumulated regions. We view our approach as an alternative to cluster visualization. To create the visualization, we first estimate the density function using a novel high-dimensional interpolation scheme. Second, we compute that function's topology by means of the join tree, generate a corresponding 3-D terrain using the topological landscape metaphor introduced by Weber et al. (2007), and finally augment that landscape by placing the original data points at suitable locations.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Topological Analysis, visual method:Novel Visual Encoding, visual method:Glyphs, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":136,
"title":"Visualizing nD Point Clouds as Topological Landscape Profiles to Guide Local Data Analysis",
"abstract":"Analyzing high-dimensional point clouds is a classical challenge in visual analytics. Traditional techniques, such as projections or axis-based techniques, suffer from projection artifacts, occlusion, and visual complexity. We propose to split data analysis into two parts to address these shortcomings. First, a structural overview phase abstracts data by its density distribution. This phase performs topological analysis to support accurate and nonoverlapping presentation of the high-dimensional cluster structure as a topological landscape profile. Utilizing a landscape metaphor, it presents clusters and their nesting as hills whose height, width, and shape reflect cluster coherence, size, and stability, respectively. A second local analysis phase utilizes this global structural knowledge to select individual clusters or point sets for further, localized data analysis. Focusing on structural entities significantly reduces visual clutter in established geometric visualizations and permits a clearer, more thorough data analysis. This analysis complements the global topological perspective and enables the user to study subspaces or geometric properties, such as shape.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Topological Analysis, paper type:Technical, visual method:Hierarchy"
},
{
"code":135,
"title":"Interactive Visual Analysis of Perfusion Data",
"abstract":"Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:Spatial Data, analysis method:Dimensionality Reduction, paper type:Technical, visual method:Scatterplot"
},
{
"code":134,
"title":"Outlier-Preserving Focus+Context Visualization in Parallel Coordinates",
"abstract":"Focus+context visualization integrates a visually accentuated representation of selected data items in focus (more details, more opacity, etc.) with a visually deemphasized representation of the rest of the data, i.e., the context. The role of context visualization is to provide an overview of the data for improved user orientation and improved navigation. A good overview comprises the representation of both outliers and trends. Up to now, however, context visualization not really treated outliers sufficiently. In this paper we present a new approach to focus+context visualization in parallel coordinates which is truthful to outliers in the sense that small-scale features are detected before visualization and then treated specially during context visualization. Generally, we present a solution which enables context visualization at several levels of abstraction, both for the representation of outliers and trends. We introduce outlier detection and context generation to parallel coordinates on the basis of a binned data representation. This leads to an output-oriented visualization approach which means that only those parts of the visualization process are executed which actually affect the final rendering. Accordingly, the performance of this solution is much more dependent on the visualization size than on the data size which makes it especially interesting for large datasets. Previous approaches are outperformed, the new solution was successfully applied to datasets with up to 3 million data records and up to 50 dimensions",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, paper type:Technical, analysis method:Clustering, visual method:Focus+Context, visual method:Parallel Coordinates, visual method:Novel Visual Encoding, data type:High-Dimensional Points"
},
{
"code":133,
"title":"Using projection and 2D plots to visually reveal genetic mechanisms of complex human disorders",
"abstract":"Gene mapping is a statistical method used to localize human disease genes to particular regions of the human genome. When performing such analysis, a genetic likelihood space is generated and sampled, which results in a multidimensional scalar field. Researchers are interested in exploring this likelihood space through the use of visualization. Previous efforts at visualizing this space, though, were slow and cumbersome, only showing a small portion of the space at a time, thus requiring the user to keep a mental picture of several views. We have developed a new technique that displays much more data at once by projecting the multidimensional data into several 2D plots. One plot is created for each parameter that shows the change along that parameter. A radial projection is used to create another plot that provides an overview of the high dimensional surface from the perspective of a single point. Linking and brushing between all the plots are used to determine relationships between parameters. We demonstrate our techniques on real world autism data, showing how to visually examine features of the high dimensional space.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot"
},
{
"code":132,
"title":"TripAdvisor-ND: A Tourism-Inspired High-Dimensional Space Exploration Framework with Overview and Detail",
"abstract":"Gaining a true appreciation of high-dimensional space remains difficult since all of the existing high-dimensional space exploration techniques serialize the space travel in some way. This is not so foreign to us since we, when traveling, also experience the world in a serial fashion. But we typically have access to a map to help with positioning, orientation, navigation, and trip planning. Here, we propose a multivariate data exploration tool that compares high-dimensional space navigation with a sightseeing trip. It decomposes this activity into five major tasks: 1) Identify the sights: use a map to identify the sights of interest and their location; 2) Plan the trip: connect the sights of interest along a specifyable path; 3) Go on the trip: travel along the route; 4) Hop off the bus: experience the location, look around, zoom into detail; and 5) Orient and localize: regain bearings in the map. We describe intuitive and interactive tools for all of these tasks, both global navigation within the map and local exploration of the data distributions. For the latter, we describe a polygonal touchpad interface which enables users to smoothly tilt the projection plane in high-dimensional space to produce multivariate scatterplots that best convey the data relationships under investigation. Motion parallax and illustrative motion trails aid in the perception of these transient patterns. We describe the use of our system within two applications: 1) the exploratory discovery of data configurations that best fit a personal preference in the presence of tradeoffs and 2) interactive cluster analysis via cluster sculpting in N-D.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, analysis method:Subspace, visual method:Focus+Context, View Optimization, visual method:Animation, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":131,
"title":"ClusterSculptor: A Visual Analytics Tool for High-Dimensional Data",
"abstract":"Cluster analysis (CA) is a powerful strategy for the exploration of high-dimensional data in the absence of a-priori hypotheses or data classification models, and the results of CA can then be used to form such models. But even though formal models and classification rules may not exist in these data exploration scenarios, domain scientists and experts generally have a vast amount of non-compiled knowledge and intuition that they can bring to bear in this effort. In CA, there are various popular mechanisms to generate the clusters, however, the results from their non- supervised deployment rarely fully agree with this expert knowledge and intuition. To this end, our paper describes a comprehensive and intuitive framework to aid scientists in the derivation of classification hierarchies in CA, using k-means as the overall clustering engine, but allowing them to tune its parameters interactively based on a non-distorted compact visual presentation of the inherent characteristics of the data in high- dimensional space. These include cluster geometry, composition, spatial relations to neighbors, and others. In essence, we provide all the tools necessary for a high-dimensional activity we call cluster sculpting, and the evolving hierarchy can then be viewed in a space-efficient radial dendrogram. We demonstrate our system in the context of the mining and classification of a large collection of millions of data items of aerosol mass spectra, but our framework readily applies to any high-dimensional CA scenario.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Hierarchy"
},
{
"code":130,
"title":"A hybrid layout algorithm for sub-quadratic multidimensional scaling",
"abstract":"Many clustering and layout techniques have been used for structuring and visualising complex data. This paper is inspired by a number of such contemporary techniques and presents a novel hybrid approach based upon stochastic sampling, interpolation and spring models. We use Chalmers' 1996 O(N2) spring model as a benchmark when evaluating our technique, comparing layout quality and run times using data sets of synthetic and real data. Our algorithm runs in O(N鈭歂) and executes significantly faster than Chalmers' 1996 algorithm, whilst producing superior layouts. In reducing complexity and run time, we allow the visualisation of data sets of previously infeasible size. Our results indicate that our method is a solid foundation for interactive and visual exploration of data.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Dimensionality Reduction, paper type:Technical,data type:High-dimensional Points,visual method:Scatterplot"
},
{
"code":129,
"title":"Improving hybrid MDS with pivot-based searching",
"abstract":"An algorithm is presented for the visualisation of multidimensional abstract data, building on a hybrid model introduced at InfoVis 2002. The most computationally complex stage of the original model involved performing a nearest-neighbour search for every data item. The complexity of this phase has been reduced by treating all high-dimensional relationships as a set of discretised distances to a constant number of randomly selected pivot items. In improving this computational bottleneck, the complexity is reduced from $O(Nsqrt N)$ to $O(N^{\frac{5} {4}} )$. As well as documenting this improvement, the paper describes evaluation with a data set of 108000 14-dimensional items; a considerable increases on the size of data previously tested. Results illustrate that the reduction in complexity is reflected in significantly improved run times and that no negative impact is made upon the quality of layout produced.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Dimensionality Reduction, paper type:Technical, visual method:Scatterplot, data type:High-dimensional Points"
},
{
"code":128,
"title":"Visualizing the quality of dimensionality reduction",
"abstract":"The growing number of dimensionality reduction methods available for data visualization has recently inspired the development of formal measures to evaluate the resulting low-dimensional representation independently from the methods' inherent criteria. Many evaluation measures can be summarized based on the co-ranking matrix. In this work, we analyze the characteristics of the co-ranking framework, focusing on interpretability and controllability in evaluation scenarios where a fine-grained assessment of a given visualization is desired. We extend the framework in two ways: (i) we propose how to link the evaluation to point-wise quality measures which can be used directly to augment the evaluated visualization and highlight erroneous regions; (ii) we improve the parameterization of the quality measure to offer more direct control over the evaluation's focus, and thus help the user to investigate more specific characteristics of the visualization.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimensionality Reduction, analysis method:Precision Measure, paper type:Technical, visual method:Scatterplot"
},
{
"code":127,
"title":"Visual exploration of classification models for risk assessment",
"abstract":"In risk assessment applications well informed decisions are made based on huge amounts of multi-dimensional data. In many domains not only the risk of a wrong decision, but in particular the trade-off between the costs of possible decisions are of utmost importance. In this paper we describe a framework tightly integrating interactive visual exploration with machine learning to support the decision making process. The proposed approach uses a series of interactive 2D visualizations of numeric and ordinal data combined with visualization of classification models. These series of visual elements are further linked to the classifier's performance visualized using an interactive performance curve. An interactive decision point on the performance curve allows the decision maker to steer the classification model and instantly identify the critical, cost changing data elements, in the various linked visualizations. The critical data elements are represented as images in order to trigger associations related to the knowledge of the expert. In this context the data visualization and classification results are not only linked together, but are also linked back to the classification model. Such a visual analytics framework allows the user to interactively explore the costs of his decisions for different settings of the model and accordingly use the most suitable classification model and make more informed and reliable decisions. A case study on data from the Forensic Psychiatry domain reveals the usefulness of the suggested approach.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, analysis method:Clustering, Machine Learning, paper type:Application, data type:High-dimensional Points, visual method:Treemap, visual method:Scatterplot"
},
{
"code":126,
"title":"Interactive decision making using dissimilarity to visually represented prototypes",
"abstract":"To make informed decisions, an expert has to reason with multi-dimensional, heterogeneous data and analysis results of these. Items in such datasets are typically represented by features. However, as argued in cognitive science, features do not yield an optimal space for human reasoning. In fact, humans tend to organize complex information in terms of prototypes or known cases rather than in absolute terms. When confronted with unknown data items, humans assess them in terms of similarity to these prototypical elements. Interestingly, an analogues similarity-to-prototype approach, where prototypes are taken from the data, has been successfully applied in machine learning. Combining such a machine learning approach with human prototypical reasoning in a Visual Analytics context requires to integrate similarity-based classification with interactive visualizations. To that end, the data prototypes should be visually represented to trigger direct associations to cases familiar to the domain experts. In this paper, we propose a set of highly interactive visualizations to explore data and classification results in terms of dissimilarities to visually represented prototypes. We argue that this approach not only supports human reasoning processes, but is also suitable to enhance understanding of heterogeneous data. The proposed framework is applied to a risk assessment case study in Forensic Psychiatry.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":125,
"title":"Illustrative Parallel Coordinates",
"abstract":"Illustrative parallel coordinates (IPC) is a suite of artistic rendering techniques for augmenting and improving parallel coordinate (PC) visualizations. IPC techniques can be used to convey a large amount of information about a multidimensional dataset in a small area of the screen through the following approaches: (a) edge-bundling through splines; (b) visualization of 鈥渂ranched 鈥�clusters to reveal the distribution of the data; (c) opacity-based hints to show cluster density; (d) opacity and shading effects to illustrate local line density on the parallel axes; and (e) silhouettes, shadows and halos to help the eye distinguish between overlapping clusters. Thus, the primary goal of this work is to convey as much information as possible in a manner that is aesthetically pleasing and easy to understand for non-experts.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Data Abstraction, paper type:Technical, visual method:Parallel Coordinates, User Study"
},
{
"code":124,
"title":"Splatterplots: Overcoming Overdraw in Scatter Plots",
"abstract":"We introduce Splatterplots, a novel presentation of scattered data that enables visualizations that scale beyond standard scatter plots. Traditional scatter plots suffer from overdraw (overlapping glyphs) as the number of points per unit area increases. Overdraw obscures outliers, hides data distributions, and makes the relationship among subgroups of the data difficult to discern. To address these issues, Splatterplots abstract away information such that the density of data shown in any unit of screen space is bounded, while allowing continuous zoom to reveal abstracted details. Abstraction automatically groups dense data points into contours and samples remaining points. We combine techniques for abstraction with perceptually based color blending to reveal the relationship between data subgroups. The resulting visualizations represent the dense regions of each subgroup of the data set as smooth closed shapes and show representative outliers explicitly. We present techniques that leverage the GPU for Splatterplot computation and rendering, enabling interaction with massive data sets. We show how Splatterplots can be an effective alternative to traditional methods of displaying scatter data communicating data trends, outliers, and data set relationships much like traditional scatter plots, but scaling to data sets of higher density and up to millions of points on the screen.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, Clutter Reduction, visual method:Scatterplot, visual method:Novel Visual Encoding, visual method:Rendering Enhancement, Perception, data type:High-Dimensional Points, analysis method:Data Abstraction, paper type:Technical"
},
{
"code":123,
"title":"Guiding feature subset selection with an interactive visualization",
"abstract":"We propose a method for the semi-automated refinement of the results of feature subset selection algorithms. Feature subset selection is a preliminary step in data analysis which identifies the most useful subset of features (columns) in a data table. So-called filter techniques use statistical ranking measures for the correlation of features. Usually a measure is applied to all entities (rows) of a data table. However, the differing contributions of subsets of data entities are masked by statistical aggregation. Feature and entity subset selection are, thus, highly interdependent. Due to the difficulty in visualizing a high-dimensional data table, most feature subset selection algorithms are applied as a black box at the outset of an analysis. Our visualization technique, SmartStripes, allows users to step into the feature subset selection process. It enables the investigation of dependencies and interdependencies between different feature and entity subsets. A user may even choose to control the iterations manually, taking into account the ranking measures, the contributions of different entity subsets, as well as the semantics of the features.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Regression, ranking, analysis method:Subspace, analysis method:Data Subset, paper type:System, visual method:Bar Charts, visual method:Scatterplot, visual method:Treemap"
},
{
"code":122,
"title":"GRACE: A Visual Comparison Framework for Integrated Spatial and Non-Spatial Geriatric Data",
"abstract":"We present the design of a novel framework for the visual integration, comparison, and exploration of correlations in spatial and non-spatial geriatric research data. These data are in general high-dimensional and span both the spatial, volumetric domain - through magnetic resonance imaging volumes - and the non-spatial domain, through variables such as age, gender, or walking speed. The visual analysis framework blends medical imaging, mathematical analysis and interactive visualization techniques, and includes the adaptation of Sparse Partial Least Squares and iterated Tikhonov Regularization algorithms to quantify potential neurologymobility connections. A linked-view design geared specifically at interactive visual comparison integrates spatial and abstract visual representations to enable the users to effectively generate and refine hypotheses in a large, multidimensional, and fragmented space. In addition to the domain analysis and design description, we demonstrate the usefulness of this approach on two case studies. Last, we report the lessons learned through the iterative design and evaluation of our approach, in particular those relevant to the design of comparative visualization of spatial and non-spatial data.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, data type:Spatial Data, analysis method:Statistic, paper type:Technical, visual method:Starplot"
},
{
"code":121,
"title":"Exploring high-D spaces with multiform matrices and small multiples",
"abstract":"We introduce an approach to visual analysis of multivariate data that integrates several methods from information visualization, exploratory data analysis (EDA), and geovisualization. The approach leverages the component-based architecture implemented in GeoVISTA Studio to construct a flexible, multiview, tightly (but generically) coordinated, EDA toolkit. This toolkit builds upon traditional ideas behind both small multiples and scatterplot matrices in three fundamental ways. First, we develop a general, multiform, bivariate matrix and a complementary multiform, bivariate small multiple plot in which different bivariate representation forms can be used in combination. We demonstrate the flexibility of this approach with matrices and small multiples that depict multivariate data through combinations of: scatterplots, bivariate maps, and space-filling displays. Second, we apply a measure of conditional entropy to (a) identify variables from a high-dimensional data set that are likely to display interesting relationships and (b) generate a default order of these variables in the matrix or small multiple display. Third, we add conditioning, a kind of dynamic query/filtering in which supplementary (undisplayed) variables are used to constrain the view onto variables that are displayed. Conditioning allows the effects of one or more well understood variables to be removed form the analysis, making relationships among remaining variables easier to explore. We illustrate the individual and combined functionality enabled by this approach through application to analysis of cancer diagnosis and mortality data and their associated covariates and risk factors.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, visual method:Scatterplot, Query, Ranking, analysis method:Dimension Relationship, data type:High-dimensional Points, paper type:Technical"
},
{
"code":120,
"title":"Multivariate Volume Visualization through Dynamic Projections",
"abstract":"We propose a multivariate volume visualization framework that tightly couples dynamic projections with a high-dimensional transfer function design for interactive volume visualization. We assume that the complex, high-dimensional data in the attribute space can be well-represented through a collection of low-dimensional linear subspaces, and embed the data points in a variety of 2D views created as projections onto these subspaces. Through dynamic projections, we present animated transitions between different views to help the user navigate and explore the attribute space for effective transfer function design. Our framework not only provides a more intuitive understanding of the attribute space but also allows the design of the transfer function under multiple dynamic views, which is more flexible than being restricted to a single static view of the data. For large volumetric datasets, we maintain interactivity during the transfer function design via intelligent sampling and scalable clustering. Using examples in combustion and climate simulations, we demonstrate how our framework can be used to visualize interesting structures in the volumetric space.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":119,
"title":"Distortion-Guided Structure-Driven Interactive Exploration of High-Dimensional Data",
"abstract":"Dimension reduction techniques are essential for feature selection and feature extraction of complex high-dimensional data. These techniques, which construct low-dimensional representations of data, are typically geometrically motivated, computationally efficient and approximately preserve certain structural properties of the data. However, they are often used as black box solutions in data exploration and their results can be difficult to interpret. To assess the quality of these results, quality measures, such as co-ranking [LV09], have been proposed to quantify structural distortions that occur between high-dimensional and low-dimensional data representations. Such measures could be evaluated and visualized point-wise to further highlight erroneous regions [MLGH13]. In this work, we provide an interactive visualization framework for exploring high-dimensional data via its two-dimensional embeddings obtained from dimension reduction, using a rich set of user interactions. We ask the following question: what new insights do we obtain regarding the structure of the data, with interactive manipulations of its embeddings in the visual space? We augment the two-dimensional embeddings with structural abstractions obtained from hierarchical clusterings, to help users navigate and manipulate subsets of the data. We use point-wise distortion measures to highlight interesting regions in the domain, and further to guide our selection of the appropriate level of clusterings that are aligned with the regions of interest. Under the static setting, point-wise distortions indicate the level of structural uncertainty within the embeddings. Under the dynamic setting, on-the-fly updates of point-wise distortions due to data movement and data deletion reflect structural relations among different parts of the data, which may lead to new and valuable insights.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimensionality Reduction, analysis method:Precision Measure, paper type:Technical, visual method:Scatterplot pipeline stage:View Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":118,
"title":"A model of symbol lightness discrimination in sparse scatterplots",
"abstract":"Symbols are used in scatterplots to encode data in a way that is appropriate for perception through human visual channels. Color is believed to be the most dominant channel with lightness regarded as the most important one of three color dimensions. We study lightness perception in scatterplots in the context of analytic tasks requiring symbol discrimination. More specifically, we performed an experiment to measure human performance in three visual analytic tasks. Outlined circles and unframed spots, equally sized, with a uniform luminance that was varied at ten or eleven equispaced levels between black and white were used as symbols and displayed on a uniform white background. Sixteen subjects divided in two groups, participated in the experiment and their task performance times were recorded. We propose a model to describe the process. The perception of lightness is assumed to be an early step in the complex cognitive process to mediate discrimination, and psychophysical laws are used to describe this perceptual mapping. Different mapping schemes are compared by regression on the experimental data. The results show that approximate homogeneity of lightness perception exists in our complex tasks and can be closely described by a blended combination of two opposite power functions assuming either the light end or the dark end of the lightness scale as the starting point. The model further yields discriminability scales of lightness for sparse scatterplots with a white background.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot, Perception"
},
{
"code":117,
"title":"Evaluation of symbol contrast in scatterplots",
"abstract":"Symbols are frequently used to represent data objects in visualization. An appropriate contrast between symbols is a precondition that determines the efficiency of a visual analysis process. We study the contrast between different types of symbols in the context of scatterplots, based on user testing and a quantitative model for symbol contrast. In total, 32 different symbols were generated by using four sizes, two classes (polygon-and asterisk shaped), and four categories of rotational symmetry; and used three different tasks. From the user test results an internal separation space is established for the symbol types under study. In this space, every symbol is represented by a point, and the visual contrasts defined by task performance between the symbols are represented by the distances between the points. The positions of the points in the space, obtained by multidimensional scaling (MDS), reveal the effects of different visual feature scales. Also, larger distances imply better symbol separation for visual tasks, and therefore indicate appropriate choices for symbols. The resulting configurations are discussed, and a number of patterns in the relation between properties of the symbols and the resulting contrast are identified. In short we found that the size effect in the space is not linear and more dominant than shape effect.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Not Applicable, paper type:Technical, visual method:Scatterplot, User Study"
},
{
"code":116,
"title":"Comparative Analysis of Multidimensional, Quantitative Data",
"abstract":"When analyzing multidimensional, quantitative data, the comparison of two or more groups of dimensions is a common task. Typical sources of such data are experiments in biology, physics or engineering, which are conducted in different configurations and use replicates to ensure statistically significant results. One common way to analyze this data is to filter it using statistical methods and then run clustering algorithms to group similar values. The clustering results can be visualized using heat maps, which show differences between groups as changes in color. However, in cases where groups of dimensions have an a priori meaning, it is not desirable to cluster all dimensions combined, since a clustering algorithm can fragment continuous blocks of records. Furthermore, identifying relevant elements in heat maps becomes more difficult as the number of dimensions increases. To aid in such situations, we have developed Matchmaker, a visualization technique that allows researchers to arbitrarily arrange and compare multiple groups of dimensions at the same time. We create separate groups of dimensions which can be clustered individually, and place them in an arrangement of heat maps reminiscent of parallel coordinates. To identify relations, we render bundled curves and ribbons between related records in different groups. We then allow interactive drill-downs using enlarged detail views of the data, which enable in-depth comparisons of clusters between groups. To reduce visual clutter, we minimize crossings between the views. This paper concludes with two case studies. The first demonstrates the value of our technique for the comparison of clustering algorithms. In the second, biologists use our system to investigate why certain strains of mice develop liver disease while others remain healthy, informally showing the efficacy of our system when analyzing multidimensional data containing distinct groups of dimensions.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, Matching, visual method:Novel Visual Encoding, visual method:Hierarchy, visual method:Focus+Context, data type:High-Dimensional Points, analysis method:Dimension Relationship, paper type:Technical"
},
{
"code":115,
"title":"A review and taxonomy of distortion-oriented presentation techniques",
"abstract":"One of the common problems associated with large computer-based information systems is the relatively small window through which an information space can be viewed. Increasing interest in recent years has been focused on the development of distortion-oriented presentation techniques to address this problem. However, the growing number of new terminologies and techniques developed have caused considerable confusion to the graphical user interface designer, consequently making the comparison of these presentation techniques and generalization of empirical results of experiments with them very difficult, if not impossible. This article provides a taxonomy of distortion-oriented techniques which demonstrates clearly their underlying relationships. A unified theory is presented to reveal their roots and origins. Issues relating to the implementation and performance of these techniques are also discussed.",
"keywords":"pipeline stage:View Transformation, user involvement:Interactive Exploration, visual method:Magic Lens, visual method:Focus+Context, paper type:Survey, data type:High-Dimensional Points, analysis method:not applicable"
},
{
"code":114,
"title":"Orthographic Star Coordinates",
"abstract":"Star coordinates is a popular projection technique from an nD data space to a 2D/3D visualization domain. It is defined by setting n coordinate axes in the visualization domain. Since it generally defines an affine projection, strong distortions can occur: an nD sphere can be mapped to an ellipse of arbitrary size and aspect ratio. We propose to restrict star coordinates to orthographic projections which map an nD sphere of radius r to a 2D circle of radius r. We achieve this by formulating conditions for the coordinate axes to define orthographic projections, and by running a repeated non-linear optimization in the background of every modification of the coordinate axes. This way, we define a number of orthographic interaction concepts as well as orthographic data tour sequences: a scatterplot tour, a principle component tour, and a grand tour. All concepts are illustrated and evaluated with synthetic and real data.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, analysis method:Projection, visual method:Star Coordinates, visual method:Scatterplot, View Optimization, data type:High-Dimensional Points, paper type:Technical"
},
{
"code":113,
"title":"Features in Continuous Parallel Coordinates",
"abstract":"Continuous Parallel Coordinates (CPC) are a contemporary visualization technique in order to combine several scalar fields, given over a common domain. They facilitate a continuous view for parallel coordinates by considering a smooth scalar field instead of a finite number of straight lines. We show that there are feature curves in CPC which appear to be the dominant structures of a CPC. We present methods to extract and classify them and demonstrate their usefulness to enhance the visualization of CPCs. In particular, we show that these feature curves are related to discontinuities in Continuous Scatterplots (CSP). We show this by exploiting a curve-curve duality between parallel and Cartesian coordinates, which is a generalization of the well-known point-line duality. Furthermore, we illustrate the theoretical considerations. Concluding, we discuss relations and aspects of the CPC's/CSP's features concerning the data analysis.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Feature Extraction, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":112,
"title":"Selecting coherent and relevant plots in large scatterplot matrices",
"abstract":"The scatterplot matrix (SPLOM) is a well-established technique to visually explore high-dimensional data sets. It is characterized by the number of scatterplots (plots) of which it consists of. Unfortunately, this number quadratically grows with the number of the data set's dimensions. Thus, an SPLOM scales very poorly. Consequently, the usefulness of SPLOMs is restricted to a small number of dimensions. For this, several approaches already exist to explore such 'small' SPLOMs. Those approaches address the scalability problem just indirectly and without solving it. Therefore, we introduce a new greedy approach to manage 'large' SPLOMs with more than 100 dimensions. We establish a combined visualization and interaction scheme that produces intuitively interpretable SPLOMs by combining known quality measures, a pre-process reordering and a perception-based abstraction. With this scheme, the user can interactively find large amounts of relevant plots in large SPLOMs.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Ranking, Reordering, visual method:Scatterplot, paper type:Technical, data type:High-Dimensional Points, analysis method:Quality Measure"
},
{
"code":111,
"title":"Quality assessment of dimensionality reduction: Rank-based criteria",
"abstract":"Dimensionality reduction aims at providing low-dimensional representations of high-dimensional data sets. Many new nonlinear methods have been proposed for the last years, yet the question of their assessment and comparison remains open. This paper first reviews some of the existing quality measures that are based on distance ranking and KK-ary neighborhoods. Next, the definition of the co-ranking matrix provides a tool for comparing the ranks in the initial data set and some low-dimensional embedding. Rank errors and concepts such as neighborhood intrusions and extrusions can then be associated with different blocks of the co-ranking matrix. Several quality criteria can be cast within this unifying framework; they are shown to involve one or several of these characteristic blocks. Following this line, simple criteria are proposed, which quantify two aspects of the embedding quality, namely its overall quality and its tendency to favor intrusions or extrusions. They are applied to several recent dimensionality reduction methods in two experiments, with both artificial and real data.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimensionality Reduction, analysis method:Precision Measure, paper type:Technical, visual method:Scatterplot"
},
{
"code":110,
"title":"A Structure-Based Distance Metric for High-Dimensional Space Exploration with Multidimensional Scaling",
"abstract":"Although the euclidean distance does well in measuring data distances within high-dimensional clusters, it does poorly when it comes to gauging intercluster distances. This significantly impacts the quality of global, low-dimensional space embedding procedures such as the popular multidimensional scaling (MDS) where one can often observe nonintuitive layouts. We were inspired by the perceptual processes evoked in the method of parallel coordinates which enables users to visually aggregate the data by the patterns the polylines exhibit across the dimension axes. We call the path of such a polyline its structure and suggest a metric that captures this structure directly in high-dimensional space. This allows us to better gauge the distances of spatially distant data constellations and so achieve data aggregations in MDS plots that are more cognizant of existing high-dimensional structure similarities. Our biscale framework distinguishes far-distances from near-distances. The coarser scale uses the structural similarity metric to separate data aggregates obtained by prior classification or clustering, while the finer scale employs the appropriate euclidean distance.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Dimensionality Reduction, analysis method:Distance Metric, visual method:Scatterplot, visual method:Parallel Coordinates, paper type:Technical, data type:High-dimensional Points"
},
{
"code":109,
"title":"iVisClustering: An Interactive Visual Document Clustering via Topic Modeling",
"abstract":"analysis method:Clustering plays an important role in many large-scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisanalysis method:Clustering, based on a widely-used topic modeling method, latent Dirichlet allocation (LDA). iVisanalysis method:Clustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph-based representation. iVisanalysis method:Clustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re-cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster-level interactions such as sub-clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document-level interactions such as moving mis-clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisanalysis method:Clustering by using real-world document data sets.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":108,
"title":"Exploring N-dimensional Databases",
"abstract":"The ability of researchers in the scientific and engineering community to generate or acquire data far outstrips their ability to analyze it. This problem is even more pronounced when the data is of high dimensionality. Visualization has been identified as a critical technique for exploring data sets, but the visualization tools developed to date have mostly concentrated on the display of low (one to four) dimensional data. Ideally a tool for examining N-dimensional data should allow the presentation of the data in a way that can be intuitively interpreted and allow the display of arbitrary views and subsets of the data. The work presented in this paper describes the creation of such a tool using a technique which we term dimensional stacking.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, data type:Nominal Data, analysis method:Data Abstraction, paper type:Technical, visual method:Novel Visual Encoding"
},
{
"code":107,
"title":"A User-Assisted Approach to Visualizing Multidimensional Images",
"abstract":" We present a new technique for fusing together an arbitrary number of aligned images into a single color or intensity image. We approach this fusion problem from the context of Multidimensional Scaling (MDS) and describe an algorithm that preserves the relative distances between pairs of pixel values in the input (vectors of measurements) as perceived differences in a color image. The two main advantages of our approach over existing techniques are that it can incorporate user constraints into the mapping process and allows adaptively compressing or exaggerating features in the input in order to make better use of the output's limited dynamic range. We demonstrate these benefits by showing applications in various scientific domains and comparing our algorithm to previously proposed techniques.",
"keywords":"pipeline stage:View Transformation, user involvement:Interactive Exploration, analysis method:Dimensionality Reduction, data type:Spatial Data, paper type:Technical, visual method:Surfaces"
},
{
"code":106,
"title":"Empirical studies in information visualization: Seven scenarios",
"abstract":"We take a new, scenario-based look at evaluation in information visualization. Our seven scenarios, evaluating visual data analysis and reasoning, evaluating user performance, evaluating user experience, evaluating environments and work practices, evaluating communication through visualization, evaluating visualization algorithms, and evaluating collaborative data analysis were derived through an extensive literature review of over 800 visualization publications. These scenarios distinguish different study goals and types of research questions and are illustrated through example studies. Through this broad survey and the distillation of these scenarios, we make two contributions. One, we encapsulate the current practices in the information visualization research community and, two, we provide a different approach to reaching decisions about what might be the most effective evaluation of a given information visualization. Scenarios can be used to choose appropriate research questions and goals and the provided examples can be consulted for guidance on how to design one's own study. ,",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, paper type:Survey, paper type:Theory, data type:High-Dimensional Points, visual method:Scatterplot, analysis method:not applicable"
},
{
"code":105,
"title":"Hierarchical Reorganization of Dimensions in OLAP Visualizations",
"abstract":"In this paper, we propose a new method for the visual reorganization of online analytical processing (OLAP) cubes that aims at improving their visualization. Our method addresses dimensions with hierarchically organized members. It uses a genetic algorithm that reorganizes k-ary trees. Genetic operators perform permutations of subtrees to optimize a visual homogeneity function. We propose several ways to reorganize an OLAP cube depending on which set of members is selected for the reorganization: all of the members, only the displayed members, or the members at a given level (level by level approach). The results that are evaluated by using optimization criteria show that our algorithm has a reliable performance even when it is limited to 1 minute runs. Our algorithm was integrated in an interactive 3D interface for OLAP. A user study was conducted to evaluate our approach with users. The results highlight the usefulness of reorganization in two OLAP tasks.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, User Study, data type:Nominal Data, visual method:Glyphs, analysis method:Dimension Similarity, paper type:Technical"
},
{
"code":104,
"title":"A Data-Driven Approach to Hue-Preserving Color-Blending",
"abstract":"Color mapping and semitransparent layering play an important role in many visualization scenarios, such as information visualization and volume rendering. The combination of color and transparency is still dominated by standard alpha-compositing using the Porter-Duff over operator which can result in false colors with deceiving impact on the visualization. Other more advanced methods have also been proposed, but the problem is still far from being solved. Here we present an alternative to these existing methods specifically devised to avoid false colors and preserve visual depth ordering. Our approach is data driven and follows the recently formulated knowledge-assisted visualization (KAV) paradigm. Preference data, that have been gathered in web-based user surveys, are used to train a support-vector machine model for automatically predicting an optimized hue-preserving blending. We have applied the resulting model to both volume rendering and a specific information visualization technique, illustrative parallel coordinate plots. Comparative renderings show a significant improvement over previous approaches in the sense that false colors are completely removed and important properties such as depth ordering and blending vividness are better preserved. Due to the generality of the defined data-driven blending operator, it can be easily integrated also into other visualization frameworks.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Color Blending, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":103,
"title":"A flexible approach for visual data mining",
"abstract":"The exploration of heterogenous information spaces requires suitable mining methods as well as effective visual interfaces. Most of the existing systems concentrate either on mining algorithms or on visualization techniques. This paper describes a flexible framework for visual data mining which combines analytical and visual methods to achieve a better understanding of the information space. We provide several pre-processing methods for unstructured information spaces, such as a flexible hierarchy generation with user-controlled refinement. Moreover, we develop new visualization techniques, including an intuitive focus+context technique to visualize complex hierarchical graphs. A special feature of our system is a new paradigm for visualizing information structures within their frame of reference",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Hierarchy"
},
{
"code":102,
"title":"Robust linear dimensionality reduction",
"abstract":"We present a novel family of data-driven linear transformations, aimed at finding low-dimensional embeddings of multivariate data, in a way that optimally preserves the structure of the data. The well-studied PCA and Fisher's LDA are shown to be special members in this family of transformations, and we demonstrate how to generalize these two methods such as to enhance their performance. Furthermore, our technique is the only one, to the best of our knowledge, that reflects in the resulting embedding both the data coordinates and pairwise relationships between the data elements. Even more so, when information on the clustering (labeling) decomposition of the data is known, this information can also be integrated in the linear transformation, resulting in embeddings that clearly show the separation between the clusters, as well as their internal structure. All of this makes our technique very flexible and powerful, and lets us cope with kinds of data that other techniques fail to describe properly.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Dimensionality Reduction, paper type:Technical, visual method:Scatterplot"
},
{
"code":101,
"title":"Visualization of labeled data using linear transformations",
"abstract":"We present a novel family of data-driven linear transformations, aimed at visualizing multivariate data in a low-dimensional space in a way that optimally preserves the structure of the data. The well-studied PCA and Fisher's LDA are shown to be special members in this family of transformations, and we demonstrate how to generalize these two methods such as to enhance their performance. Furthermore, our technique is the only one, to the best of our knowledge, that reflects in the resulting embedding both the data coordi-nates and pairwise similarities and/or dissimilarities between the data elements. Even more so, when information on the clustering (labeling) decomposition of the data is known, this information can be integrated in the linear transformation, resulting in embeddings that clearly show the separation between the clusters, as well as their intra-structure. All this makes our technique very flexible and powerful, and lets us cope with kinds of data that other techniques fail to describe properly.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Dimensionality Reduction, visual method:Scatterplot, analysis method:Projection, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":100,
"title":"Just-in-time annotation of clusters, outliers, and trends in point-based data visualizations",
"abstract":"We introduce the concept of just-in-time descriptive analytics as a novel application of computational and statistical techniques performed at interaction-time to help users easily understand the structure of data as seen in visualizations. Fundamental to just-in-time descriptive analytics is (a) identifying visual features, such as clusters, outliers, and trends, user might observe in visualizations automatically, (b) determining the semantics of such features by performing statistical analysis as the user is interacting, and (c) enriching visualizations with annotations that not only describe semantics of visual features but also facilitate interaction to support high-level understanding of data. In this paper, we demonstrate just-in-time descriptive analytics applied to a point-based multi-dimensional visualization technique to identify and describe clusters, outliers, and trends. We argue that it provides a novel user experience of computational techniques working alongside of users allowing them to build faster qualitative mental models of data by demonstrating its application on a few use-cases. Techniques used to facilitate just-in-time descriptive analytics are described in detail along with their runtime performance characteristics. We believe this is just a starting point and much remains to be researched, as we discuss open issues and opportunities in improving accessibility and collaboration.",
"keywords":"pipeline stage:View Transformation, user involvement:Interactive Exploration, analysis method:Clustering, visual method:Scatterplot, data type:High-Dimensional Points, paper type:Technical"
},
{
"code":99,
"title":"Recursive pattern: a technique for visualizing very large amounts of data",
"abstract":"An important goal of visualization technology is to support the exploration and analysis of very large amounts of data. In this paper, we propose a new visualization technique called a `recursive pattern', which has been developed for visualizing large amounts of multidimensional data. The technique is based on a generic recursive scheme which generalizes a wide range of pixel-oriented arrangements for displaying large data sets. By instantiating the technique with adequate data- and application-dependent parameters, the user may greatly influence the structure of the resulting visualizations. Since the technique uses one pixel for presenting each data value, the amount of data which can be displayed is only limited by the resolution of current display technology and by the limitations of human perceptibility. Beside describing the basic idea of the `recursive pattern' technique, we provide several examples of useful parameter settings for the various recursion levels. We further show that our `recursive pattern' technique is particularly advantageous for the large class of data sets which have a natural order according to one dimension (e.g. time series data). We demonstrate the usefulness of our technique by using a stock market application",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Novel Visual Encoding, visual method:Pixel-based, paper type:Technical, analysis method:Feature Extraction, data type:High-Dimensional Points"
},
{
"code":98,
"title":"Visualization techniques for mining large databases: A comparison",
"abstract":"Visual data mining techniques have proven to be of high value in exploratory data analysis, and they also have a high potential for mining large databases. In this article, we describe and evaluate a new visualization-based approach to mining large databases. The basic idea of our visual data mining techniques is to represent as many data items as possible on the screen at the same time by mapping each data value to a pixel of the screen and arranging the pixels adequately. The major goal of this article is to evaluate our visual data mining techniques and to compare them to other well-known visualization techniques for multidimensional data: the parallel coordinate and stick-figure visualization techniques. For the evaluation of visual data mining techniques, the perception of data properties counts most, while the CPU time and the number of secondary storage accesses are only of secondary importance. In addition to testing the visualization techniques using real data, we developed a testing environment for database visualizations similar to the benchmark approach used for comparing the performance of database systems. The testing environment allows the generation of test data sets with predefined data characteristics which are important for comparing the perceptual abilities of visual data mining techniques",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, Visual Data Mining, paper type:Technical, data type:High-Dimensional Points, analysis method:not applicable , visual method:not applicable"
},
{
"code":97,
"title":"VisDB: database exploration using multidimensional visualization",
"abstract":"Discusses how the VisDB system supports the query specification process by representing the result visually. The main idea behind the system stems from the view of relational database tables as sets of multidimensional data where the number of attributes corresponds to the number of dimensions. In such a view, it is often unclear. In this system, each display pixel represents one database item. Pixels are arranged and colored to indicate the item's relevance to a user query and to give a visual impression of the resulting data set.<>",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Novel Visual Encoding, visual method:Pixel-based, paper type:Technical, analysis method:Feature Extraction, data type:High-Dimensional Points, Visual Data Mining"
},
{
"code":96,
"title":"Pixel bar charts: a new technique for visualizing large multi-attribute data sets without aggregation",
"abstract":"Simple presentation graphics are intuitive and easy-to-use, but show only highly aggregated data and present only a very limited number of data values (as in the case of bar charts). In addition, these graphics may have a high degree of overlap which may occlude a significant portion of the data values (as in the case of the x-y plots). In this paper, we therefore propose a generalization of traditional bar charts and x-y-plots which allows the visualization of large amounts of data. The basic idea is to use the pixels within the bars to present the detailed information of the data records. Our so-called pixel bar charts retain the intuitiveness of traditional bar charts while allowing very large data sets to be visualized in an effective way. We show that, for an effective pixel placement, we have to solve complex optimization problems, and present an algorithm which efficiently solves the problem. Our application using real-world e-commerce data shows the wide applicability and usefulness of our new idea.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, Clutter Reduction, visual method:Novel Visual Encoding, visual method:Bar Charts, visual method:Pixel-based, paper type:Technical, analysis method:Clustering, data type:Nominal Data"
},
{
"code":95,
"title":"Information visualization and visual data mining",
"abstract":"Never before in history has data been generated at such high volumes as it is today. Exploring and analyzing the vast volumes of data is becoming increasingly difficult. Information visualization and visual data mining can help to deal with the flood of information. The advantage of visual data exploration is that the user is directly involved in the data mining process. There are a large number of information visualization techniques which have been developed over the last decade to support the exploration of large data sets. In this paper, we propose a classification of information visualization and visual data mining techniques which is based on the data type to be visualized, the visualization technique, and the interaction and distortion technique. We exemplify the classification using a few examples, most of them referring to techniques and systems presented in this special section.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, paper type:Survey, Visual Data Mining, data type:high-dimensional points, paper type:Technical, analysis method:not applicable, visual method:not applicable"
},
{
"code":94,
"title":"Designing pixel-oriented visualization techniques: theory and applications",
"abstract":"Visualization techniques are of increasing importance in exploring and analyzing large amounts of multidimensional information. One important class of visualization techniques which is particularly interesting for visualizing very large multidimensional data sets is the class of pixel-oriented techniques. The basic idea of pixel-oriented visualization techniques is to represent as many data objects as possible on the screen at the same time by mapping each data value to a pixel of the screen and arranging the pixels adequately. A number of different pixel-oriented visualization techniques have been proposed in recent years and it has been shown that the techniques are useful for visual data exploration in a number of different application contexts. In this paper, we discuss a number of issues which are important in developing pixel-oriented visualization techniques. The major goal of this article is to provide a formal basis of pixel-oriented visualization techniques and show that the design decisions in developing them can be seen as solutions of well-defined optimization problems. This is true for the mapping of the data values to colors, the arrangement of pixels inside the subwindows, the shape of the subwindows, and the ordering of the dimension subwindows. The paper also discusses the design issues of special variants of pixel-oriented techniques for visualizing large spatial data sets",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, paper type:Theory, visual method:pixel-based, data type:High-dimensional Points, analysis method:not applicable"
},
{
"code":93,
"title":"Visualization and visual analysis of multifaceted scientific data: A survey",
"abstract":"Visualization and visual analysis play important roles in exploring, analyzing, and presenting scientific data. In many disciplines, data and model scenarios are becoming multifaceted: data are often spatiotemporal and multivariate; they stem from different data sources (multimodal data), from multiple simulation runs (multirun/ensemble data), or from multiphysics simulations of interacting phenomena (multimodel data resulting from coupled simulation models). Also, data can be of different dimensionality or structured on various types of grids that need to be related or fused in the visualization. This heterogeneity of data characteristics presents new opportunities as well as technical challenges for visualization research. Visualization and interaction techniques are thus often combined with computational analysis. In this survey, we study existing methods for visualization and interactive visual analysis of multifaceted scientific data. Based on a thorough literature review, a categorization of approaches is proposed. We cover a wide range of fields and discuss to which degree the different challenges are matched with existing solutions for visualization and visual analysis. This leads to conclusions with respect to promising research directions, for instance, to pursue new solutions for multirun and multimodel data as well as techniques that support a multitude of facets.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, paper type:Survey, visual method:Volume Visualization, data type:Spatial Data, analysis method:not applicable"
},
{
"code":92,
"title":"Brushing Moments in Interactive Visual Analysis",
"abstract":"We present a systematic study of opportunities for the interactive visual analysis of multi-dimensional scientific data that is based on the integration of statistical aggregations along selected independent data dimensions in a framework of coordinated multiple views (with linking and brushing). Traditional and robust estimates of the four statistical moments (mean, variance, skewness, and kurtosis) as well as measures of outlyingness are integrated in an iterative visual analysis process. Brushing particular statistics, the analyst can investigate data characteristics such as trends and outliers. We present a categorization of beneficial combinations of attributes in 2D scatterplots: (a) kth vs. (k + 1)th statistical moment of a traditional or robust estimate, (b) traditional vs. robust version of the same moment, (c) two different robust estimates of the same moment. We propose selected view transformations to iteratively construct this multitude of informative views as well as to enhance the depiction of the statistical properties in scatterplots and quantile plots. In the framework, we interrelate the original distributional data and the aggregated statistics, which allows the analyst to work with both data representations simultaneously. We demonstrate our approach in the context of two visual analysis scenarios of multi-run climate simulations.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Statistic, paper type:Technical, visual method:Scatterplot"
},
{
"code":91,
"title":"Star Coordinates: A Multi-dimensional Visualization Technique with Uniform Treatment of Dimensions",
"abstract":"Visualizing multi-dimensional data has tremendous effects on science, engineering, and business decisionmaking. A new visualization technique called visual method:Star Coordinates is presented to support users in early stages of their visual thinking activities. visual method:Star Coordinates arranges coordinates on a circle sharing the same origin at the center. It uses simply points to represent data, treating each dimension uniformly at the cost of coarse representation. Current implementation of visual method:Star Coordinates provided valuable insight on several real data sets for cluster discovery and multifactor analysis tasks. The work on visual method:Star Coordinates will continue on developing advanced transformations that will improve data understanding in multi-dimensions.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Star Coordinates, visual method:Novel Visual Encoding, paper type:Technical, data type:High-Dimensional Points, analysis method:not applicable"
},
{
"code":90,
"title":"Local Affine Multidimensional Projection",
"abstract":"Multidimensional projection techniques have experienced many improvements lately, mainly regarding computational times and accuracy. However, existing methods do not yet provide flexible enough mechanisms for visualization-oriented fully interactive applications. This work presents a new multidimensional projection technique designed to be more flexible and versatile than other methods. This novel approach, called Local Affine Multidimensional Projection (LAMP), relies on orthogonal mapping theory to build accurate local transformations that can be dynamically modified according to user knowledge. The accuracy, flexibility and computational efficiency of LAMP is confirmed by a comprehensive set of comparisons. LAMP's versatility is exploited in an application which seeks to correlate data that, in principle, has no connection as well as in visual exploration of textual documents.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot"
},
{
"code":89,
"title":"Visualization of multi-dimensional data with vector-fusion",
"abstract":"Multi-dimensional entities are modeled, displayed and understood with a new algorithm vectorizing data of any dimensionality. This algorithm is called SBP; it is a vectorized generalization of parallel coordinates. Classic geometries of any dimensionality can be demonstrated to facilitate perception and understanding of the shapes generated by this algorithm. SBP images of a 4D line, a circle and 3D and 4D spherical helices are shown. A strategy for synthesizing multi-dimensional models matching multi-dimensional data is presented. Current applications include data mining; modeling data-defined structures of scientific interest such as protein structure and Calabi-Yau figures as multi-dimensional geometric entities; generating vector-fused data signature fingerprints of classic frequency spectra that identify substances; and treating complex targets as multi-dimensional entities for automatic target recognition. SBP vector data signatures apply to all pattern recognition problems.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":88,
"title":"Revealing structure within clustered parallel coordinates displays",
"abstract":"In order to gain insight into multivariate data, complex structures must be analysed and understood. Parallel coordinates is an excellent tool for visualizing this type of data but has its limitations. This paper deals with one of its main limitations - how to visualize a large number of data items without hiding the inherent structure they constitute. We solve this problem by constructing clusters and using high precision textures to represent them. We also use transfer functions that operate on the high precision textures in order to highlight different aspects of the cluster characteristics. Providing predefined transfer functions as well as the support to draw customized transfer functions makes it possible to extract different aspects of the data. We also show how feature animation can be used as guidance when simultaneously analysing several clusters. This technique makes it possible to visually represent statistical information about clusters and thus guides the user, making the analysis process more efficient.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, Clutter Reduction,visual method:Parallel Coordinates, visual method:Novel Visual Encoding, visual method:Animation, data type:High-Dimensional Points, analysis method:clustering, paper type:Technical"
},
{
"code":87,
"title":"Interactive dimensionality reduction through user-defined combinations of quality metrics",
"abstract":"Multivariate data sets including hundreds of variables are increasingly common in many application areas. Most multivariate visualization techniques are unable to display such data effectively, and a common approach is to employ dimensionality reduction prior to visualization. Most existing dimensionality reduction systems focus on preserving one or a few significant structures in data. For many analysis tasks, however, several types of structures can be of high significance and the importance of a certain structure compared to the importance of another is often task-dependent. This paper introduces a system for dimensionality reduction by combining user-defined quality metrics using weight functions to preserve as many important structures as possible. The system aims at effective visualization and exploration of structures within large multivariate data sets and provides enhancement of diverse structures by supplying a range of automatic variable orderings. Furthermore it enables a quality-guided reduction of variables through an interactive display facilitating investigation of trade-offs between loss of structure and the number of variables to keep. The generality and interactivity of the system is demonstrated through a case scenario.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Clutter Reduction, View Optimization,visual method:Parallel Coordinates, analysis method:Dimensionality Reduction, data type:High-Dimensional Points, paper type:Technical"
},
{
"code":86,
"title":"A screen space quality method for data abstraction",
"abstract":"The rendering of large data sets can result in cluttered displays and non-interactive update rates, leading to time consuming analyses. A straightforward solution is to reduce the number of items, thereby producing an abstraction of the data set. For the visual analysis to remain accurate, the graphical representation of the abstraction must preserve the significant features present in the original data. This paper presents a screen space quality method, based on distance transforms, that measures the visual quality of a data abstraction. This screen space measure is shown to better capture significant visual structures in data, compared with data space measures. The presented method is implemented on the GPU, allowing interactive creation of high quality graphical representations of multivariate data sets containing tens of thousands of items. ,",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, Clutter Reduction, analysis method:Data Abstraction, visual method:Parallel Coordinates, data type:High-Dimensional Points, paper type:Technical"
},
{
"code":85,
"title":"iPCA: An Interactive System for PCA-based Visual Analytics",
"abstract":" is a widely used mathematical technique in many fields for factor and trend analysis, dimension reduction, etc. However, it is often considered to be a 鈥渂lack box鈥�operation whose results are difficult to interpret and sometimes counter-intuitive to the user. In order to assist the user in better understanding and utilizing PCA, we have developed a system that visualizes the results of principal component analysis using multiple coordinated views and a rich set of user interactions. Our design philosophy is to support analysis of multivariate datasets through extensive interaction with the PCA output. To demonstrate the usefulness of our system, we performed a comparative user study with a known commercial system, SAS/INSIGHT鈥檚 Interactive Data Exploration. Participants in our study solved a number of high-level analysis tasks with each interface and rated the systems on ease of learning and usefulness. Based on the participants鈥�accuracy, speed, and qualitative feedback, we observe that our system helps users to better understand relationships between the data and the calculated eigenspace, which allows the participants to more accurately analyze the data. User feedback suggests that the interactivity and transparency of our system are the key strengths of our approach.",
"keywords":"pipeline stage:Data Transformation, user involvement:Model Manipulation, analysis method:Dimensionality Reduction, visual method:Scatterplot, visual method:Parallel Coordinates, data type:High-dimensional Points, paper type:System"
},
{
"code":84,
"title":"A radial focus+context visualization for multi-dimensional functions",
"abstract":"The analysis of multidimensional functions is important in many engineering disciplines, and poses a major problem as the number of dimensions increases. Previous visualization approaches focus on representing three or fewer dimensions at a time. This paper presents a new focus+context visualization that provides an integrated overview of an entire multidimensional function space, with uniform treatment of all dimensions. The overview is displayed with respect to a user-controlled polar focal point in the function's parameter space. Function value patterns are viewed along rays that emanate from the focal point in all directions in the parameter space, and represented radially around the focal point in the visualization. Data near the focal point receives proportionally more screen space than distant data. This approach scales smoothly from two dimensions to 10-20, with a 1000 pixel range on each dimension.",
"keywords":"pipeline stage:Visual Mapping, user involvement`:Interactive Exploration, visual method:Focus+Context, data type:High-dimensional Function, analysis method:not applicable, paper type:Technical, user involvement:Interactive Exploration"
},
{
"code":83,
"title":"Glimmer: Multilevel MDS on the GPU",
"abstract":"We present Glimmer, a new multilevel algorithm for multidimensional scaling designed to exploit modern graphics processing unit (GPU) hardware. We also present GPU-SF, a parallel, force-based subsystem used by Glimmer. Glimmer organizes input into a hierarchy of levels and recursively applies GPU-SF to combine and refine the levels. The multilevel nature of the algorithm makes local minima less likely while the GPU parallelism improves speed of computation. We propose a robust termination condition for GPU-SF based on a filtered approximation of the normalized stress function. We demonstrate the benefits of Glimmer in terms of speed, normalized stress, and visual quality against several previous algorithms for a range of synthetic and real benchmark datasets. We also show that the performance of Glimmer on GPUs is substantially faster than a CPU implementation of the same algorithm.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimensionality Reduction, paper type:Technical, visual method:Scatterplot"
},
{
"code":82,
"title":"DimStiller: Workflows for dimensional analysis and reduction",
"abstract":"DimStiller is a system for dimensionality reduction and analysis. It frames the task of understanding and transforming input dimensions as a series of analysis steps where users transform data tables by chaining together different techniques, called operators, into pipelines of expressions. The individual operators have controls and views that are linked together based on the structure of the expression. Users interact with the operator controls to tune parameter choices, with immediate visual feedback guiding the exploration of local neighborhoods of the space of possible data tables. DimStiller also provides global guidance for navigating data-table space through expression templates called workflows, which permit re-use of common patterns of analysis.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, paper type:System, data type:Nominal Data, data type:High-dimensional Points, analysis method:Dimensionality Reduction, visual method:Bar Charts, visual method:Scatterplot"
},
{
"code":81,
"title":"GPLOM: The Generalized Plot Matrix for Visualizing Multidimensional Multivariate Data",
"abstract":"Scatterplot matrices (SPLOMs), parallel coordinates, and glyphs can all be used to visualize the multiple continuous variables (i.e., dependent variables or measures) in multidimensional multivariate data. However, these techniques are not well suited to visualizing many categorical variables (i.e., independent variables or dimensions). To visualize multiple categorical variables, 'hierarchical axes' that 'stack dimensions' have been used in systems like Polaris and Tableau. However, this approach does not scale well beyond a small number of categorical variables. Emerson et al. [8] extend the matrix paradigm of the SPLOM to simultaneously visualize several categorical and continuous variables, displaying many kinds of charts in the matrix depending on the kinds of variables involved. We propose a variant of their technique, called the Generalized Plot Matrix (GPLOM). The GPLOM restricts Emerson et al.'s technique to only three kinds of charts (scatterplots for pairs of continuous variables, heatmaps for pairs of categorical variables, and barcharts for pairings of categorical and continuous variable), in an effort to make it easier to understand. At the same time, the GPLOM extends Emerson et al.'s work by demonstrating interactive techniques suited to the matrix of charts. We discuss the visual design and interactive features of our GPLOM prototype, including a textual search feature allowing users to quickly locate values or variables by name. We also present a user study that compared performance with Tableau and our GPLOM prototype, that found that GPLOM is significantly faster in certain cases, and not significantly slower in other cases.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:Nominal Data, analysis method:not applicable, paper type:Technical, visual method:Scatterplot, User Study"
},
{
"code":80,
"title":"MoleView: An Attribute and Structure-Based Semantic Lens for Large Element-Based Plots",
"abstract":"We present MoleView, a novel technique for interactive exploration of multivariate relational data. Given a spatial embedding of the data, in terms of a scatter plot or graph layout, we propose a semantic lens which selects a specific spatial and attribute-related data range. The lens keeps the selected data in focus unchanged and continuously deforms the data out of the selection range in order to maintain the context around the focus. Specific deformations include distance-based repulsion of scatter plot points, deforming straight-line node-link graph drawings, and as varying the simplification degree of bundled edge graph layouts. Using a brushing-based technique, we further show the applicability of our semantic lens for scenarios requiring a complex selection of the zones of interest. Our technique is simple to implement and provides real-time performance on large datasets. We demonstrate our technique with actual data from air and road traffic control, medical imaging, and software comprehension applications.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Novel Visual Encoding, visual method:Magic Lens, paper type:Technical, data type:High-Dimensional Points, analysis method:not applicable"
},
{
"code":79,
"title":"Clustering visualizations of multidimensional data",
"abstract":"Many graphical methods for displaying multivariate data consist of arrangements of multiple displays of one or two variables; scatterplot matrices and parallel coordinates plots are two such methods. In principle these methods generalize to arbitrary numbers of variables but become difficult to interpret for even moderate numbers of variables. This article demonstrates that the impact of high dimensions is much less severe when the component displays are clustered together according to some index of merit. Effectively, this clustering reduces the dimensionality and makes interpretation easier. For scatterplot matrices and parallel coordinates plots clustering of component displays is achieved by finding suitable permutations of the variables. I discuss algorithms based on cluster analysis for finding permutations, and present examples using various indices of merit.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Scatterplot,visual method:Parallel Coordinates, analysis method:Clustering, data type:High-dimensional Points, paper type:Technical"
},
{
"code":78,
"title":"Semantics of Directly Manipulating Spatializations",
"abstract":"When high-dimensional data is visualized in a 2D plane by using parametric projection algorithms, users may wish to manipulate the layout of the data points to better reflect their domain knowledge or to explore alternative structures. However, few users are well-versed in the algorithms behind the visualizations, making parameter tweaking more of a guessing game than a series of decisive interactions. Translating user interactions into algorithmic input is a key component of Visual to Parametric Interaction (V2PI) [13]. Instead of adjusting parameters, users directly move data points on the screen, which then updates the underlying statistical model. However, we have found that some data points that are not moved by the user are just as important in the interactions as the data points that are moved. Users frequently move some data points with respect to some other 'unmoved' data points that they consider as spatially contextual. However, in current V2PI interactions, these points are not explicitly identified when directly manipulating the moved points. We design a richer set of interactions that makes this context more explicit, and a new algorithm and sophisticated weighting scheme that incorporates the importance of these unmoved data points into V2PI.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Model Manipulation, visual method:Scatterplot, paper type:Technical, data type:High-Dimensional Points, analysis method:Distance Metric"
},
{
"code":77,
"title":"Evaluation of Cluster Identification Performance for Different PCP Variants",
"abstract":"Parallel coordinate plots (PCPs) are a well-known visualization technique for viewing multivariate data. In the past, various visual modifications to PCPs have been proposed to facilitate tasks such as correlation and cluster identification, to reduce visual clutter, and to increase their information throughput. Most modifications pertain to the use of color and opacity, smooth curves, or the use of animation. Although many of these seem valid improvements, only few user studies have been performed to investigate this, especially with respect to cluster identification. We performed a user study to evaluate cluster identification performance – with respect to response time and correctness – of nine PCP variations, including standard PCPs. To generate the variations, we focused on covering existing techniques as well as possible while keeping testing feasible. This was done by adapting and merging techniques, which led to the following novel variations. The first is an effective way of embedding scatter plots into PCPs. The second is a technique for highlighting fuzzy clusters based on neighborhood density. The third is a spline-based drawing technique to reduce ambiguity. The last is a pair of animation schemes for PCP rotation. We present an overview of the tested PCP variations and the results of our study. The most important result is that a fair number of the seemingly valid improvements, with the exception of scatter plots embedded into PCPs, do not result in significant performance gains.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Not Applicable, paper type:Technical, visual method:Parallel Coordinates, User Study"
},
{
"code":76,
"title":"Dimensional anchors: a graphic primitive for multidimensional multivariate information visualizations",
"abstract":"none",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimensionality Reduction, paper type:Technical, visual method:RadViz"
},
{
"code":75,
"title":"DNA visual and analytic data mining",
"abstract":"Describes data exploration techniques designed to classify DNA sequences. Several visualization and data mining techniques were used to validate and attempt to discover new methods for distinguishing coding DNA sequences (exons) from non-coding DNA sequences (introns). The goal of the data mining was to see whether some other, possibly non-linear combination of the fundamental position-dependent DNA nucleotide frequency values could be a better predictor than the AMI (average mutual information). We tried many different classification techniques including rule-based classifiers and neural networks. We also used visualization of both the original data and the results of the data mining to help verify patterns and to understand the distinction between the different types of data and classifications. In particular, the visualization helped us develop refinements to neural network classifiers, which have accuracies as high as any known method. Finally, we discuss the interactions between visualization and data mining and suggest an integrated approach.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimensionality Reduction, paper type:Technical, visual method:RadViz"
},
{
"code":74,
"title":"Continuous Parallel Coordinates",
"abstract":"Typical scientific data is represented on a grid with appropriate interpolation or approximation schemes,defined on a continuous domain. The visualization of such data in parallel coordinates may reveal patterns latently contained in the data and thus can improve the understanding of multidimensional relations. In this paper, we adopt the concept of continuous scatterplots for the visualization of spatially continuous input data to derive a density model for parallel coordinates. Based on the point-line duality between scatterplots and parallel coordinates, we propose a mathematical model that maps density from a continuous scatterplot to parallel coordinates and present different algorithms for both numerical and analytical computation of the resulting density field. In addition, we show how the 2-D model can be used to successively construct continuous parallel coordinates with an arbitrary number of dimensions. Since continuous parallel coordinates interpolate data values within grid cells, a scalable and dense visualization is achieved, which will be demonstrated for typical multi-variate scientific data.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric,visual method:Parallel Coordinates, Clutter Reduction, visual method:Rendering Enhancement, paper type:Technical, data type:High-Dimensional Points, analysis method:Data Abstraction"
},
{
"code":73,
"title":"Animated transitions in statistical data graphics",
"abstract":"In this paper we investigate the effectiveness of animated transitions between common statistical data graphics such as bar charts, pie charts, and scatter plots. We extend theoretical models of data graphics to include such transitions, introducing a taxonomy of transition types. We then propose design principles for creating effective transitions and illustrate the application of these principles in DynaVis, a visualization system featuring animated data graphics. Two controlled experiments were conducted to assess the efficacy of various transition types, finding that animated transitions can significantly improve graphical perception.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Scatterplot, visual method:Novel Visual Encoding, visual method:Animation, Perception, visual method:Bar Charts, paper type:Technical, data type:High-dimensional Points, analysis method:not applicable"
},
{
"code":72,
"title":"Topological Landscape Ensembles for Visualization of Scalar-Valued Functions",
"abstract":"Visual representation techniques enable perception and exploration of scientific data. Following the topological landscapes metaphor of Weber et al., we provide a new algorithm for visualizing scalar functions defined on simply connected domains of arbitrary dimension. For a potentially high dimensional scalar field, our algorithm produces a collection of, in some sense complete, two-dimensional terrain models whose contour trees and corresponding topological persistences are identical to those of the input scalar field. The algorithm exactly preserves the volume of each region corresponding to an arc in the contour tree. We also introduce an efficiently computable metric on terrain models we generate. Based on this metric, we develop a tool that can help the users to explore the space of possible terrain models.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, visual method:Novel Visual Encoding, analysis method:Topological Analysis, data type:High-Dimensional Function, paper type:Technical"
},
{
"code":71,
"title":"Intelligent Visual Analytics Queries",
"abstract":"Visualizations of large multi-dimensional data sets, occurring in scientific and commercial applications, often reveal interesting local patterns. Analysts want to identify the causes and impacts of these interesting areas, and they also want to search for similar patterns occurring elsewhere in the data set. In this paper we introduce the Intelligent Visual Analytics Query (IVQuery) concept that combines visual interaction with automated analytical methods to support analysts in discovering the special properties and relations of identified patterns. The idea of IVQuery is to interactively select focus areas in the visualization. Then, according to the characteristics of the selected areas, such as the data dimensions and records, IVQuery employs analytical methods to identify the relationships to other portions of the data set. Finally, IVQuery generates visual representations for analysts to view and refine the results. IVQuery has been applied successfully to different real-world data sets, such as data warehouse performance, product sales, and sever performance analysis, and demonstrates the benefits of this technique over traditional filtering and zooming techniques. The visual analytics query technique can be used with many different types of visual representation. In this paper we show how to use IVQuery with parallel coordinates, visual maps, and scatter plots.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimension Relationship, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":70,
"title":"Weaving Versus Blending: a quantitative assessment of the information carrying capacities of two alternative methods for conveying multivariate data with color.",
"abstract":"In many applications, it is important to understand the individual values of, and relationships between, multiple related scalar variables defined across a common domain. Several approaches have been proposed for representing data in these situations. In this paper we focus on strategies for the visualization of multivariate data that rely on color mixing. In particular, through a series of controlled observer experiments, we seek to establish a fundamental understanding of the information-carrying capacities of two alternative methods for encoding multivariate information using color: color blending and color weaving. We begin with a baseline experiment in which we assess participants' abilities to accurately read numerical data encoded in six different basic color scales defined in the L*a*b* color space. We then assess participants' abilities to read combinations of 2, 3, 4 and 6 different data values represented in a common region of the domain, encoded using either color blending or color weaving. In color blending a single mixed color is formed via linear combination of the individual values in L*a*b* space, and in color weaving the original individual colors are displayed side-by-side in a high frequency texture that fills the region. A third experiment was conducted to clarify some of the trends regarding the color contrast and its effect on the magnitude of the error that was observed in the second experiment. The results indicate that when the component colors are represented side-by-side in a high frequency texture, most participants' abilities to infer the values of individual components are significantly improved, relative to when the colors are blended. Participants' performance was significantly better with color weaving particularly when more than 2 colors were used, and even when the individual colors subtended only 3 minutes of visual angle in the texture. However, the information-carrying capacity of the color weaving approach has its limits. - - We found that participants' abilities to accurately interpret each of the individual components in a high frequency color texture typically falls off as the number of components increases from 4 to 6. We found no significant advantages, in either color blending or color weaving, to using color scales based on component hues thatare more widely separated in the L*a*b* color space. Furthermore, we found some indications that extra difficulties may arise when opponent hues are employed.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Not Applicable, paper type:Technical, visual method:Color Blending"
},
{
"code":69,
"title":"Interactive local clustering operations for high dimensional data in parallel coordinates",
"abstract":"In this paper, we propose an approach of clustering data in parallel coordinates through interactive local operations. Different from many other methods in which clustering is globally applied to the whole dataset, our interactive scheme allows users to directly apply attractive and repulsive operators at regions of interests, taking advantages of an electricity interaction metaphor, for clutter reduction and cluster detection. Our design enables users to interact directly with the parallel coordinate plots and provides great flexibility in exploring and revealing underlying patterns. With instant feedback, our work allows users to dynamically adjust the clustering parameters to reach an optimum. We also supply the user with a graph indicating the logical relationship between clusters. Our experiments show that our scheme is more efficient than traditional methods in performing visual analysis tasks.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":68,
"title":"Pointwise local pattern exploration for sensitivity analysis",
"abstract":"Sensitivity analysis is a powerful method for discovering the significant factors that contribute to targets and understanding the interaction between variables in multivariate datasets. A number of sensitivity analysis methods fall into the class of local analysis, in which the sensitivity is defined as the partial derivatives of a target variable with respect to a group of independent variables. Incorporating sensitivity analysis in visual analytic tools is essential for multivariate phenomena analysis. However, most current multivariate visualization techniques do not allow users to explore local patterns individually for understanding the sensitivity from a pointwise view. In this paper, we present a novel pointwise local pattern exploration system for visual sensitivity analysis. Using this system, analysts are able to explore local patterns and the sensitivity at individual data points, which reveals the relationships between a focal point and its neighbors. During exploration, users are able to interactively change the derivative coefficients to perform sensitivity analysis based on different requirements as well as their domain knowledge. Each local pattern is assigned an outlier factor, so that users can quickly identify anomalous local patterns that do not conform with the global pattern. Users can also compare the local pattern with the global pattern both visually and statistically. Finally, the local pattern is integrated into the original attribute space using color mapping and jittering, which reveals the distribution of the partial derivatives. Case studies with real datasets are used to investigate the effectiveness of the visualizations and interactions.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Novel Visual Encoding, Sensitivity Analysis, paper type:Technical, data type:High-Dimensional Points, analysis method:Feature Extraction"
},
{
"code":67,
"title":"Model space visualization for multivariate linear trend discovery",
"abstract":"Discovering and extracting linear trends and correlations in datasets is very important for analysts to understand multivariate phenomena. However, current widely used multivariate visualization techniques, such as parallel coordinates and scatterplot matrices, fail to reveal and illustrate such linear relationships intuitively, especially when more than 3 variables are involved or multiple trends coexist in the dataset. We present a novel multivariate model parameter space visualization system that helps analysts discover single and multiple linear patterns and extract subsets of data that fit a model well. Using this system, analysts are able to explore and navigate in model parameter space, interactively select and tune patterns, and refine the model for accuracy using computational techniques. We build connections between model space and data space visually, allowing analysts to employ their domain knowledge during exploration to better interpret the patterns they discover and their validity. Case studies with real datasets are used to investigate the effectiveness of the visualizations.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimension Relationship, paper type:Technical, visual method:Scatterplot"
},
{
"code":66,
"title":"Coordinating computational and visual approaches for interactive feature selection and multivariate clustering",
"abstract":"Unknown (and unexpected) multivariate patterns lurking in high-dimensional datasets are often very hard to find. This paper describes a human-centered exploration environment, which incorporates a coordinated suite of computational and visualization methods to explore high-dimensional data for uncovering patterns in multivariate spaces. Specifically, it includes: (1) an interactive feature selection method for identifying potentially interesting, multidimensional subspaces from a high-dimensional data space, (2) an interactive, hierarchical clustering method for searching multivariate clusters of arbitrary shape, and (3) a suite of coordinated visualization and computational components centered around the above two methods to facilitate a human-led exploration. The implemented system is used to analyze a cancer dataset and shows that it is efficient and effective for discovering unknown and unexpected multivariate patterns from high-dimensional data",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, analysis method:Clustering, analysis method:Subspace, paper type:Technical, data type:High-dimensional Points, visual method:Parallel Coordinates, visual method:Bar Charts, visual method:Scatterplot"
},
{
"code":65,
"title":"Explainers: Expert Explorations with Crafted Projections",
"abstract":"This paper introduces an approach to exploration and discovery in high-dimensional data that incorporates a user's knowledge and questions to craft sets of projection functions meaningful to them. Unlike most prior work that defines projections based on their statistical properties, our approach creates projection functions that align with user-specified annotations. Therefore, the resulting derived dimensions represent concepts defined by the user's examples. These especially crafted projection functions, or explainers, can help find and explain relationships between the data variables and user-designated concepts. They can organize the data according to these concepts. Sets of explainers can provide multiple perspectives on the data. Our approach considers tradeoffs in choosing these projection functions, including their simplicity, expressive power, alignment with prior knowledge, and diversity. We provide techniques for creating collections of explainers. The methods, based on machine learning optimization frameworks, allow exploring the tradeoffs. We demonstrate our approach on model problems and applications in text analysis.",
"keywords":"pipeline stage:Data Transformation, user involvement:Model Manipulation, visual method:Scatterplot, analysis method:Distance Metric, paper type:Technical, data type:High-Dimensional Points"
},
{
"code":64,
"title":"Visual Exploration of High Dimensional Scalar Functions",
"abstract":"An important goal of scientific data analysis is to understand the behavior of a system or process based on a sample of the system. In many instances it is possible to observe both input parameters and system outputs, and characterize the system as a high-dimensional function. Such data sets arise, for instance, in large numerical simulations, as energy landscapes in optimization problems, or in the analysis of image data relating to biological or medical parameters. This paper proposes an approach to analyze and visualizing such data sets. The proposed method combines topological and geometric techniques to provide interactive visualizations of discretely sampled high-dimensional scalar fields. The method relies on a segmentation of the parameter space using an approximate Morse-Smale complex on the cloud of point samples. For each crystal of the Morse-Smale complex, a regression of the system parameters with respect to the output yields a curve in the parameter space. The result is a simplified geometric representation of the Morse-Smale complex in the high dimensional input domain. Finally, the geometric representation is embedded in 2D, using dimension reduction, to provide a visualization platform. The geometric properties of the regression curves enable the visualization of additional information about each crystal such as local and global shape, width, length, and sampling densities. The method is illustrated on several synthetic examples of two dimensional functions. Two use cases, using data sets from the UCI machine learning repository, demonstrate the utility of the proposed approach on real data. Finally, in collaboration with domain experts the proposed method is applied to two scientific challenges. The analysis of parameters of climate simulations and their relationship to predicted global energy flux and the concentrations of chemical species in a combustion simulation and their integration with temperature.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Topological Analysis, analysis method:Regression, data type:High-dimensional Function, visual method:Novel Visual Encoding, visual method:Hierarchy, paper type:Technical"
},
{
"code":63,
"title":"Angular Histograms: Frequency-Based Visualizations for Large, High Dimensional Data",
"abstract":"Parallel coordinates is a popular and well-known multivariate data visualization technique. However, one of their inherent limitations has to do with the rendering of very large data sets. This often causes an overplotting problem and the goal of the visual information seeking mantra is hampered because of a cluttered overview and non-interactive update rates. In this paper, we propose two novel solutions, namely, angular histograms and attribute curves. These techniques are frequency-based approaches to large, high-dimensional data visualization. They are able to convey both the density of underlying polylines and their slopes. Angular histogram and attribute curves offer an intuitive way for the user to explore the clustering, linear correlations and outliers in large data sets without the over-plotting and clutter problems associated with traditional parallel coordinates. We demonstrate the results on a wide variety of data sets including real-world, high-dimensional biological data. Finally, we compare our methods with the other popular frequency-based algorithms.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Not Applicable, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":62,
"title":"A visual analytics approach to model learning",
"abstract":"The process of learning models from raw data typically requires a substantial amount of user input during the model initialization phase. We present an assistive visualization system which greatly reduces the load on the users and makes the process of model initialization and refinement more efficient, problem-driven, and engaging. Utilizing a sequence segmentation task with a Hidden Markov Model as an example, we assign each token in the sequence a feature vector based on its various properties within the sequence. These vectors are then clustered according to similarity, generating a layout of the individual tokens in form of a node link diagram where the length of the links is determined by the feature vector similarity. Users may then tune the weights of the feature vector components to improve the segmentation, which is visualized as a better separation of the clusters. Also, as individual clusters represent different classes, the user can now work at the cluster level to define token classes, instead of labeling one entry at time. Inconsistent entries visually identify themselves by locating at the periphery of clusters, and the user then helps refine the model by resolving these inconsistencies. Our system therefore makes efficient use of the knowledge of its users, only requesting user assistance for non-trivial data items. It so allows users to visually analyse data at a higher, more abstract level, improving scalability.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, analysis method:Clustering, Machine Learning, paper type:Technical, data type:Nominal Data, visual method:Node-link"
},
{
"code":61,
"title":"Structure-based brushes: a mechanism for navigating hierarchically organized data and information spaces",
"abstract":"Interactive selection is a critical component in exploratory visualization, allowing users to isolate subsets of the displayed information for highlighting, deleting, analysis, or focused investigation. Brushing, a popular method for implementing the selection process, has traditionally been performed in either screen space or data space. In this paper, we introduce an alternate, and potentially powerful, mode of selection that we term structure-based brushing, for selection in data sets with natural or imposed structure. Our initial implementation has focused on hierarchically structured data, specifically very large multivariate data sets structured via hierarchical clustering and partitioning algorithms. The structure-based brush allows users to navigate hierarchies by specifying focal extents and level-of-detail on a visual representation of the structure. Proximity-based coloring, which maps similar colors to data that are closely related within the structure, helps convey both structural relationships and anomalies. We describe the design and implementation of our structure-based brushing tool. We also validate its usefulness using two distinct hierarchical visualization techniques, namely hierarchical parallel coordinates and tree-maps. Finally, we discuss relationships between different classes of brushes and identify methods by which structure-based brushing could be extended to alternate data structures",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Clutter Reduction, analysis method:Data Abstraction, visual method:Treemap, visual method:Parallel Coordinates, data type:High-dimensional Points, paper type:Technical"
},
{
"code":60,
"title":"A Projection Pursuit Algorithm for Exploratory Data Analysis",
"abstract":"An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples. The algorithm seeks to find one-and two-dimensional linear projections of multivariate data that are relatively highly revealing.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Projection, analysis method:Quality Measure, paper type:Technical, visual method:Scatterplot"
},
{
"code":59,
"title":"PRIM-9, an interactive multidimensional data display and analysis system",
"abstract":"none",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot, visual method:Animation"
},
{
"code":58,
"title":"Visualizing High-Dimensional Structures by Dimension Ordering and Filtering using Subspace Analysis",
"abstract":"High-dimensional data visualization is receiving increasing interest because of the growing abundance of high-dimensional datasets. To understand such datasets, visualization of the structures present in the data, such as clusters, can be an invaluable tool. Structures may be present in the full high-dimensional space, as well as in its subspaces. Two widely used methods to visualize high-dimensional data are the scatter plot matrix (SPM) and the parallel coordinate plot (PCP). SPM allows a quick overview of the structures present in pairwise combinations of dimensions. On the other hand, PCP has the potential to visualize not only bi-dimensional structures but also higher dimensional ones. A problem with SPM is that it suffers from crowding and clutter which makes interpretation hard. Approaches to reduce clutter are available in the literature, based on changing the order of the dimensions. However, usually this reordering has a high computational complexity. For effective visualization of high-dimensional structures, also PCP requires a proper ordering of the dimensions. In this paper, we propose methods for reordering dimensions in PCP in such a way that high-dimensional structures (if present) become easier to perceive. We also present a method for dimension reordering in SPM which yields results that are comparable to those of existing approaches, but at a much lower computational cost. Our approach is based on finding relevant subspaces for clustering using a quality criterion and cluster information. The quality computation and cluster detection are done in image space, using connected morphological operators. We demonstrate the potential of our approach for synthetic and astronomical datasets, and show that our method compares favorably with a number of existing approaches.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, Ranking, visual method:Scatterplot,visual method:Parallel Coordinates, View Optimization, Clutter Reduction, Query, Reordering, paper type:Technical, data type:High-Dimensional Points, analysis method:Subspace"
},
{
"code":57,
"title":"Finding and visualizing relevant subspaces for clustering high-dimensional astronomical data using connected morphological operators",
"abstract":"Data sets in astronomy are growing to enormous sizes. Modern astronomical surveys provide not only image data but also catalogues of millions of objects (stars, galaxies), each object with hundreds of associated parameters. Exploration of this very high-dimensional data space poses a huge challenge. Subspace clustering is one among several approaches which have been proposed for this purpose in recent years. However, many clustering algorithms require the user to set a large number of parameters without any guidelines. Some methods also do not provide a concise summary of the datasets, or, if they do, they lack additional important information such as the number of clusters present or the significance of the clusters. In this paper, we propose a method for ranking subspaces for clustering which overcomes many of the above limitations. First we carry out a transformation from parametric space to discrete image space where the data are represented by a grid-based density field. Then we apply so-called connected morphological operators on this density field of astronomical objects that provides visual support for the analysis of the important subspaces. Clusters in subspaces correspond to high-intensity regions in the density image. The importance of a cluster is measured by a new quality criterion based on the dynamics of local maxima of the density. Connected operators are able to extract such regions with an indication of the number of clusters present. The subspaces are visualized during computation of the quality measure, so that the user can interact with the system to improve the results. In the result stage, we use three visualization toolkits linked within a graphical user interface so that the user can perform an in-depth exploration of the ranked subspaces. Evaluation based on synthetic as well as real astronomical datasets demonstrates the power of the new method. We recover various known astronomical relations directly from the data with little or no a priori assumptions. Hence, our method holds good prospects for discovering new relations as well.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Subspace, Ranking, paper type:Application, data type:High-dimensional points, visual method:Scatterplot"
},
{
"code":56,
"title":"An interactive 3D integration of parallel coordinates and star glyphs",
"abstract":"Parallel coordinates are a powerful method for visualizing multidimensional data but, when applied to large data sets, they become cluttered and difficult to read. Star glyphs, on the other hand, can be used to display either the attributes of a data item or the values across all items for a single attribute. Star glyphs may readily provide a quick impression; however, since the full data set require multiple glyphs, overall readings are more difficult. We present parallel glyphs, an interactive integration of the visual representations of parallel coordinates and star glyphs that utilizes the advantages of both representations to offset the disadvantages they have separately. We discuss the role of uniform and stepped colour scales in the visual comparison of non-adjacent items and star glyphs. Parallel glyphs provide capabilities for focus-in-context exploration using two types of lenses and interactions specific to the 3D space.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Not Applicable, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":55,
"title":"Structural Decomposition Trees",
"abstract":"Researchers and analysts in modern industrial and academic environments are faced with a daunting amount of multi-dimensional data. While there has been significant development in the areas of data mining and knowledge discovery, there is still the need for improved visualizations and generic solutions. The state-of-the-art in visual analytics and exploratory data visualization is to incorporate more profound analysis methods while focusing on fast interactive abilities. The common trend in these scenarios is to either visualize an abstraction of the data set or to better utilize screen-space.This paper presents a novel technique that combines clustering, dimension reduction and multi-dimensional data representation to form a multivariate data visualization that incorporates both detail and overview. This amalgamation counters the individual drawbacks of common projection and multi-dimensional data visualization techniques, namely ambiguity and clutter. A specific clustering criterion is used to decompose a multi-dimensional data set into a hierarchical tree structure. This decomposition is embedded in a novel Dimensional Anchor visualization through the use of a weighted linear dimension reduction technique. The resulting Structural Decomposition Tree (SDT) provides not only an insight of the data set's inherent structure, but also conveys detailed coordinate value information. Further, fast and intuitive interaction techniques are explored in order to guide the user in highlighting, brushing, and filtering of the data.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Hierarchy"
},
{
"code":54,
"title":"Towards High-dimensional Data Analysis in Air Quality Research",
"abstract":"Analysis of chemical constituents from mass spectrometry of aerosols involves non-negative matrix factorization, an approximation of high-dimensional data in lower-dimensional space. The associated optimization problem is non-convex, resulting in crude approximation errors that are not accessible to scientists. To address this shortcoming, we introduce a new methodology for user-guided error-aware data factorization that entails an assessment of the amount of information contributed by each dimension of the approximation, an effective combination of visualization techniques to highlight, filter, and analyze error features, as well as a novel means to interactively refine factorizations. A case study and the domain-expert feedback provided by the collaborating atmospheric scientists illustrate that our method effectively communicates errors of such numerical optimization results and facilitates the computation of high-quality data factorizations in a simple and intuitive manner.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Dimension Relationship, paper type:Technical, visual method:Scatterplot"
},
{
"code":53,
"title":"Visual Steering and Verification of Mass Spectrometry Data Factorization in Air Quality Research",
"abstract":"The study of aerosol composition for air quality research involves the analysis of high-dimensional single particle mass spectrometry data. We describe, apply, and evaluate a novel interactive visual framework for dimensionality reduction of such data. Our framework is based on non-negative matrix factorization with specifically defined regularization terms that aid in resolving mass spectrum ambiguity. Thereby, visualization assumes a key role in providing insight into and allowing to actively control a heretofore elusive data processing step, and thus enabling rapid analysis meaningful to domain scientists. In extending existing black box schemes, we explore design choices for visualizing, interacting with, and steering the factorization process to produce physically meaningful results. A domain-expert evaluation of our system performed by the air quality research experts involved in this effort has shown that our method and prototype admits the finding of unambiguous and physically correct lower-dimensional basis transformations of mass spectrometry data at significantly increased speed and a higher degree of ease.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, analysis method:Subspace, analysis method:Projection, paper type:Application, data type:High-dimensional Points, visual method:Parallel Coordinates, visual method:Scatterplot"
},
{
"code":52,
"title":"Observation-level interaction with statistical models for visual analytics",
"abstract":"In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus 鈥渙bservation鈥� in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Statistic, paper type:Technical, visual method:Scatterplot"
},
{
"code":51,
"title":"BaobabView: Interactive construction and analysis of decision trees",
"abstract":"We present a system for the interactive construction and analysis of decision trees that enables domain experts to bring in domain specific knowledge. We identify different user tasks and corresponding requirements, and develop a system incorporating a tight integration of visualization, interaction and algorithmic support. Domain experts are supported in growing, pruning, optimizing and analysing decision trees. Furthermore, we present a scalable decision tree visualization optimized for exploration. We show the effectiveness of our approach by applying the methods to two use cases. The first case illustrates the advantages of interactive construction, the second case demonstrates the effectiveness of analysis of decision trees and exploration of the structure of the data.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Parallel Coordinates, Machine Learning"
},
{
"code":50,
"title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data",
"abstract":"Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:not applicable, paper type:Technical, visual method:Starplot"
},
{
"code":49,
"title":"Rolling the dice: Multidimensional visual exploration using scatterplot matrix navigation",
"abstract":"visual method:Scatterplots remain one of the most popular and widely-used visual representations for multidimensional data due to their simplicity, familiarity and visual clarity, even if they lack some of the flexibility and visual expressiveness of newer multidimensional visualization techniques. This paper presents new interactive methods to explore multidimensional data using scatterplots. This exploration is performed using a matrix of scatterplots that gives an overview of the possible configurations, thumbnails of the scatterplots, and support for interactive navigation in the multidimensional space. Transitions between scatterplots are performed as animated rotations in 3D space, somewhat akin to rolling dice. Users can iteratively build queries using bounding volumes in the dataset, sculpting the query from different viewpoints to become more and more refined. Furthermore, the dimensions in the navigation space can be reordered, manually or automatically, to highlight salient correlations and differences among them. An example scenario presents the interaction techniques supporting smooth and effortless visual exploration of multidimensional datasets.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Animation, visual method:Scatterplot, Reordering, paper type:System, data type:High-dimensional Points, analysis method:Dimension Similarity"
},
{
"code":48,
"title":"A Taxonomy of Clutter Reduction for Information Visualisation",
"abstract":"Information visualisation is about gaining insight into data through a visual representation. This data is often multivariate and increasingly, the datasets are very large. To help us explore all this data, numerous visualisation applications, both commercial and research prototypes, have been designed using a variety of techniques and algorithms. Whether they are dedicated to geo-spatial data or skewed hierarchical data, most of the visualisations need to adopt strategies for dealing with overcrowded displays, brought about by too much data to fit in too small a display space. This paper analyses a large number of these clutter reduction methods, classifying them both in terms of how they deal with clutter reduction and more importantly, in terms of the benefits and losses. The aim of the resulting taxonomy is to act as a guide to match techniques to problems where different criteria may have different importance, and more importantly as a means to critique and hence develop existing and new techniques.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, paper type:Survey, Clutter Reduction, data type:High-Dimensional Points, analysis method:not applicable, visual method:not applicable"
},
{
"code":47,
"title":"Enabling automatic clutter reduction in parallel coordinate plots",
"abstract":"We have previously shown that random sampling is an effective clutter reduction technique and that a sampling lens can facilitate focus+context viewing of particular regions. This demands an efficient method of estimating the overlap or occlusion of large numbers of intersecting lines in order to automatically adjust the sampling rate within the lens. This paper proposes several ways for measuring occlusion in parallel coordinate plots. An empirical study into the accuracy and efficiency of the occlusion measures show that a probabilistic approach combined with a 'binning' technique is very fast and yet approaches the accuracy of the more expensive 'true' complete measurement",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Parallel Coordinates, Clutter Reduction, visual method:Magic Lens, visual method:Focus+Context, paper type:Technical, data type:High-Dimensional Points, analysis method:Data Abstraction"
},
{
"code":46,
"title":"Visualizing Nuclear Scission through a Multifield Extension of Topological Analysis",
"abstract":"In nuclear science, density functional theory (DFT) is a powerful tool to model the complex interactions within the atomic nucleus, and is the primary theoretical approach used by physicists seeking a better understanding of fission. However DFT simulations result in complex multivariate datasets in which it is difficult to locate the crucial `scission' point at which one nucleus fragments into two, and to identify the precursors to scission. The Joint Contour Net (JCN) has recently been proposed as a new data structure for the topological analysis of multivariate scalar fields, analogous to the contour tree for univariate fields. This paper reports the analysis of DFT simulations using the JCN, the first application of the JCN technique to real data. It makes three contributions to visualization: (i) a set of practical methods for visualizing the JCN, (ii) new insight into the detection of nuclear scission, and (iii) an analysis of aesthetic criteria to drive further work on representing the JCN.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Topological Analysis, paper type:Application, data type:High-dimensional Function, visual method:Novel Visual Encoding"
},
{
"code":45,
"title":"Integrating Isosurface Statistics and Histograms",
"abstract":"Many data sets are sampled on regular lattices in two, three or more dimensions, and recent work has shown that statistical properties of these data sets must take into account the continuity of the underlying physical phenomena. However, the effects of quantization on the statistics have not yet been accounted for. This paper therefore reconciles the previous papers to the underlying mathematical theory, develops a mathematical model of quantized statistics of continuous functions, and proves convergence of geometric approximations to continuous statistics for regular sampling lattices. In addition, the computational cost of various approaches is considered, and recommendations made about when to use each type of statistic.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Isosurface, paper type:Technical, data type:High-Dimensional Function, analysis method:Feature Extraction"
},
{
"code":44,
"title":"Assisted navigation for large information spaces",
"abstract":"This paper presents a new technique for visualizing large, complex collections of data. The size and dimensionality of these datasets make them challenging to display in an effective manner. The images must show the global structure of spatial relationships within the dataset, yet at the same time accurately represent the local detail of each data element being visualized. We propose combining ideas from information and scientific visualization together with a navigation assistant, a software system designed to help users identify and explore areas of interest within their data. The assistant locates data elements of potential importance to the user, clusters them into spatial regions, and builds underlying graph structures to connect the regions and the elements they contain. Graph traversal algorithms, constraint-based viewpoint construction, and intelligent camera planning techniques can then be used to design animated tours of these regions. In this way, the navigation assistant can help users to explore any of the areas of interest within their data. We conclude by demonstrating how our assistant is being used to visualize a multidimensional weather dataset.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:not applicable, paper type:Technical, visual method:Glyphs"
},
{
"code":43,
"title":"Progressive High-Quality Response Surfaces for Visually Guided Sensitivity Analysis",
"abstract":"In this paper we present a technique which allows us to perform high quality and progressive response surface prediction from multidimensional input samples in an efficient manner. We utilize kriging interpolation to estimate a response surface which minimizes the expectation value and variance of the prediction error. High computational efficiency is achieved by employing parallel matrix and vector operations on the GPU. Our approach differs from previous kriging approaches in that it uses a novel progressive updating scheme for new samples based on blockwise matrix inversion. In this way we can handle very large sample sets to which new samples are continually added. Furthermore, we can monitor the incremental evolution of the surface, providing a means to early terminate the computation when no significant changes have occurred. When the generation of input samples is fast enough, our technique enables steering this generation process interactively to find relevant dependency relations.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, Sensitivity Analysis, visual method:Progressive Update, visual method:Surfaces, paper type:Technical, data type:High-dimensional Function, analysis method:Regression"
},
{
"code":42,
"title":"Pargnostics: Screen-space metrics for parallel coordinates",
"abstract":"Interactive visualization requires the translation of data into a screen space of limited resolution. While currently ignored by most visualization models, this translation entails a loss of information and the introduction of a number of artifacts that can be useful, (e.g., aggregation, structures) or distracting (e.g., over-plotting, clutter) for the analysis. This phenomenon is observed in parallel coordinates, where overlapping lines between adjacent axes form distinct patterns, representing the relation between variables they connect. However, even for a small number of dimensions, the challenge is to effectively convey the relationships for all combinations of dimensions. The size of the dataset and a large number of dimensions only add to the complexity of this problem. To address these issues, we propose Pargnostics, parallel coordinates diagnostics, a model based on screen-space metrics that quantify the different visual structures. Pargnostics metrics are calculated for pairs of axes and take into account the resolution of the display as well as potential axis inversions. Metrics include the number of line crossings, crossing angles, convergence, overplotting, etc. To construct a visualization view, the user can pick from a ranked display showing pairs of coordinate axes and the structures between them, or examine all possible combinations of axes at once in a matrix display. Picking the best axes layout is an NP-complete problem in general, but we provide a way of automatically optimizing the display according to the user's preferences based on our metrics and model.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, visual method:Parallel Coordinates, Ranking, View Optimization, paper type:Technical, data type:High-Dimensional Points, analysis method:Quality Measure"
},
{
"code":41,
"title":"Stacking Graphic Elements to Avoid Over-Plotting",
"abstract":"An ongoing challenge for information visualization is how to deal with over-plotting forced by ties or the relatively limited visual field of display devices. A popular solution is to represent local data density with area (bubble plots, treemaps), color(heatmaps), or aggregation (histograms, kernel densities, pixel displays). All of these methods have at least one of three deficiencies:1) magnitude judgments are biased because area and color have convex downward perceptual functions, 2) area, hue, and brightnesshave relatively restricted ranges of perceptual intensity compared to length representations, and/or 3) it is difficult to brush or link toindividual cases when viewing aggregations. In this paper, we introduce a new technique for visualizing and interacting with datasets that preserves density information by stacking overlapping cases. The overlapping data can be points or lines or other geometric elements, depending on the type of plot. We show real-dataset applications of this stacking paradigm and compare them to other techniques that deal with over-plotting in high-dimensional displays.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Not Applicable, paper type:Technical, visual method:Parallel Coordinates, visual method:Novel Visual Encoding"
},
{
"code":40,
"title":"TimeSeer: Scagnostics for High-Dimensional",
"abstract":"We introduce a method (Scagnostic time series) and an application (TimeSeer) for organizing multivariate time series and for guiding interactive exploration through high-dimensional data. The method is based on nine characterizations of the 2D distributions of orthogonal pairwise projections on a set of points in multidimensional euclidean space. These characterizations include measures, such as, density, skewness, shape, outliers, and texture. Working directly with these Scagnostic measures, we can locate anomalous or interesting subseries for further analysis. Our application is designed to handle the types of doubly multivariate data series that are often found in security, financial, social, and other sectors.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, Ranking, View Optimization, data type:Time Series, paper type:System, visual method:Magic Lens, visual method:Scatterplot, visual method:Bar charts,analysis method:Scagnostics"
},
{
"code":39,
"title":"Measuring data abstraction quality in multiresolution visualizations",
"abstract":"Data abstraction techniques are widely used in multiresolution visualization systems to reduce visual clutter and facilitate analysis from overview to detail. However, analysts are usually unaware of how well the abstracted data represent the original dataset, which can impact the reliability of results gleaned from the abstractions. In this paper, we define two data abstraction quality measures for computing the degree to which the abstraction conveys the original dataset: the histogram difference measure and the nearest neighbor measure. They have been integrated within XmdvTool, a public-domain multiresolution visualization system for multivariate data analysis that supports sampling as well as clustering to simplify data. Several interactive operations are provided, including adjusting the data abstraction level, changing selected regions, and setting the acceptable data abstraction quality level. Conducting these operations, analysts can select an optimal data abstraction level. Also, analysts can compare different abstraction methods using the measures to see how well relative data density and outliers are maintained, and then select an abstraction method that meets the requirement of their analytic tasks",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, Clutter Reduction, analysis method:Data Abstraction, visual method:Scatterplot, visual method:Bar Charts, visual method:Parallel Coordinates, data type:High-Dimensional Points, paper type:Technical"
},
{
"code":38,
"title":"Visualizing Social Photos on a Hasse Diagram for Eliciting Relations and Indexing New Photos",
"abstract":"Social photos, which are taken during family events or parties, represent individuals or groups of people. We show in this paper how a Hasse diagram is an efficient visualization strategy for eliciting different groups and navigating through them. However, we do not limit this strategy to these traditional uses. Instead we show how it can also be used for assisting in indexing new photos. Indexing consists of identifying the event and people in photos. It is an integral phase that takes place before searching and sharing. In our method we use existing indexed photos to index new photos. This is performed through a manual drag and drop procedure followed by a content fusion process that we call 'propagation'. At the core of this process is the necessity to organize and visualize the photos that will be used for indexing in a manner that is easily recognizable and accessible by the user. In this respect we make use of an object Galois sub-hierarchy and display it using a Hasse diagram. The need for an incremental display that maintains the user's mental map also leads us to propose a novel way of building the Hasse diagram. To validate the approach, we present some tests conducted with a sample of users that confirm the interest of this organization, visualization and indexation approach. Finally, we conclude by considering scalability, the possibility to extract social networks and automatically create personalised albums.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Node-link, data type:Nominal Data, paper type:System, analysis method:Segmentation"
},
{
"code":37,
"title":"Towards Robust Topology of Sparsely Sampled Data",
"abstract":"none",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, analysis method:Topological Analysis, data type:High-dimensional Points, paper type:Technical, visual method:Surfaces"
},
{
"code":36,
"title":"A framework for uncertainty-aware visual analytics",
"abstract":"Visual analytics has become an important tool for gaining insight on large and complex collections of data. Numerous statistical tools and data transformations, such as projections, binning and clustering, have been coupled with visualization to help analysts understand data better and faster. However, data is inherently uncertain, due to error, noise or unreliable sources. When making decisions based on uncertain data, it is important to quantify and present to the analyst both the aggregated uncertainty of the results and the impact of the sources of that uncertainty. In this paper, we present a new framework to support uncertainty in the visual analytics process, through statistic methods such as uncertainty modeling, propagation and aggregation. We show that data transformations, such as regression, principal component analysis and k-means clustering, can be adapted to account for uncertainty. This framework leads to better visualizations that improve the decision-making process and help analysts gain insight on the analytic process itself.",
"keywords":"pipeline stage:Data Transformation, pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Scatterplot, Uncertainty, visual method:Novel Visual Encoding, paper type:System, analysis method:Projection, analysis method:Regression, data type:High-dimensional Points"
},
{
"code":35,
"title":"Flexible Linked Axes for Multivariate Data Visualization",
"abstract":"Multivariate data visualization is a classic topic, for which many solutions have been proposed, each with its own strengths and weaknesses. In standard solutions the structure of the visualization is fixed, we explore how to give the user more freedom to define visualizations. Our new approach is based on the usage of Flexible Linked Axes: The user is enabled to define a visualization by drawing and linking axes on a canvas. Each axis has an associated attribute and range, which can be adapted. Links between pairs of axes are used to show data in either scatter plot- or Parallel Coordinates Plot-style. Flexible Linked Axes enable users to define a wide variety of different visualizations. These include standard methods, such as scatter plot matrices, radar charts, and PCPs [11]; less well known approaches, such as Hyperboxes [1], TimeWheels [17], and many-to-many relational parallel coordinate displays [14]; and also custom visualizations, consisting of combinations of scatter plots and PCPs. Furthermore, our method allows users to define composite visualizations that automatically support brushing and linking. We have discussed our approach with ten prospective users, who found the concept easy to understand and highly promising.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:not applicable, paper type:Technical, visual method:Parallel Coordinates, User Study"
},
{
"code":34,
"title":"iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction",
"abstract":"We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA). Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster structure. Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coordinates and a scatter plot. Furthermore, it significantly improves the interactivity and interpretability of LDA. LDA enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain. By using heat maps, iVisClassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space. Equipped with these functionalities, iVisClassifier supports users' classification tasks in an efficient way. Using several facial image data, we show how the above analysis is performed.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":33,
"title":"Two-stage framework for visualization of clustered high dimensional data",
"abstract":"In this paper, we discuss dimension reduction methods for 2D visualization of high dimensional clustered data. We propose a two-stage framework for visualizing such data based on dimension reduction methods. In the first stage, we obtain the reduced dimensional data by applying a supervised dimension reduction method such as linear discriminant analysis which preserves the original cluster structure in terms of its criteria. The resulting optimal reduced dimension depends on the optimization criteria and is often larger than 2. In the second stage, the dimension is further reduced to 2 for visualization purposes by another dimension reduction method such as principal component analysis. The role of the second-stage is to minimize the loss of information due to reducing the dimension all the way to 2. Using this framework, we propose several two-stage methods, and present their theoretical characteristics as well as experimental comparisons on both artificial and real-world text data sets.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Clustering, paper type:Technical, visual method:Scatterplot"
},
{
"code":32,
"title":"Multivariate glyphs for multi-object clusters",
"abstract":"Aggregating items can simplify the display of huge quantities of data values at the cost of losing information about the attribute values of the individual items. We propose a distribution glyph, in both two- and three-dimensional forms, which specifically addresses the concept of how the aggregated data is distributed over the possible range of values. It is capable of displaying distribution, variability and extent information for up to four attributes at a time of multivariate, clustered data. User studies validate the concept, showing that both glyphs are just as good as raw data and the 3D glyph is better for answering some questions.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Glyphs, visual method:Novel Visual Encoding, data type:High-dimensional Points, analysis method:statistic, paper type:Technical"
},
{
"code":31,
"title":"A taxonomy of visualization techniques using the data state reference model",
"abstract":"In previous work, researchers have attempted to construct taxonomies of information visualization techniques by examining the data domains that are compatible with these techniques. This is useful because implementers can quickly identify various techniques that can be applied to their domain of interest. However, these taxonomies do not help the implementers understand how to apply and implement these techniques. The author extends and proposes a new way to taxonomize information visualization techniques by using the Data State Model (E.H. Chi and J.T. Reidl, 1998). In fact, as the taxonomic analysis in the paper will show, many of the techniques share similar operating steps that can easily be reused. The paper shows that the Data State Model not only helps researchers understand the space of design, but also helps implementers understand how information visualization techniques can be applied more broadly",
"keywords":"pipeline stage:Data Transformation, pipeline stage:Visual Mapping, pipeline stage:View Transformation, user involvement:Computation Centric, user involvement:Interactive Exploration, user involvement:Model Manipulation, paper type:Theory, paper type:Survey, data type:High-Dimensional Points, visual method:not applicable, analysis method:not applicable"
},
{
"code":30,
"title":"The Generalized Sensitivity Scatterplot",
"abstract":"visual method:Scatterplots remain a powerful tool to visualize multidimensional data. However, accurately understanding the shape of multidimensional points from 2D projections remains challenging due to overlap. Consequently, there are a lot of variations on the scatterplot as a visual metaphor for this limitation. An important aspect often overlooked in scatterplots is the issue of sensitivity or local trend, which may help in identifying the type of relationship between two variables. However, it is not well known how or what factors influence the perception of trends from 2D scatterplots. To shed light on this aspect, we conducted an experiment where we asked people to directly draw the perceived trends on a 2D scatterplot. We found that augmenting scatterplots with local sensitivity helps to fill the gaps in visual perception while retaining the simplicity and readability of a 2D scatterplot. We call this augmentation the generalized sensitivity scatterplot (GSS). In a GSS, sensitivity coefficients are visually depicted as flow lines, which give a sense of continuity and orientation of the data that provide cues about the way data points are scattered in a higher dimensional space. We introduce a series of glyphs and operations that facilitate the analysis of multidimensional data sets using GSS, and validate with a number of well-known data sets for both regression and classification tasks.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Scatterplot, Sensitivity Analysis, visual method:Novel Visual Encoding, paper type:Technical, data type:High-Dimensional Points, analysis method:projection"
},
{
"code":29,
"title":"Flow-based scatterplots for sensitivity analysis The Use of Faces to Represent Points in K-Dimensional Space Graphically",
"abstract":"A novel method of representing multivariate data is presented. Each point in k-dimensional space, k 鈮�18, is represented by a cartoon of a face whose features, such as length of nose and curvature of mouth, correspond to components of the point. Thus every multivariate observation is visualized as a computer-drawn face. This presentation makes it easy for the human mind to grasp many of the essential regularities and irregularities present in the data. Other graphical representations are described briefly.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, visual method:Scatterplot, Sensitivity Analysis, visual method:Novel Visual Encoding, paper type:Technical, data type:High-Dimensional Points, analysis method:Projection"
},
{
"code":28,
"title":"Joint Contour Nets",
"abstract":"Contour Trees and Reeb Graphs are firmly embedded in scientific visualization for analysing univariate (scalar) fields. We generalize this analysis to multivariate fields with a data structure called the Joint Contour Net that quantizes the variation of multiple variables simultaneously. We report the first algorithm for constructing the Joint Contour Net, and demonstrate some of the properties that make it practically useful for visualisation, including accelerating computation by exploiting a relationship with rasterisation in the range of the function.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, visual method:Graph, paper type:Technical, analysis method:Topological Analysis, data type:High-Dimensional Points"
},
{
"code":27,
"title":"DICON: Interactive Visual Analysis of Multidimensional Clusters",
"abstract":"analysis method:Clustering as a fundamental data analysis technique has been widely used in many analytic applications. However, it is often difficult for users to understand and evaluate multidimensional clustering results, especially the quality of clusters and their semantics. For large and complex data, high-level statistical information about the clusters is often needed for users to evaluate cluster quality while a detailed display of multidimensional attributes of the data is necessary to understand the meaning of clusters. In this paper, we introduce DICON, an icon-based cluster visualization that embeds statistical information into a multi-attribute display to facilitate cluster interpretation, evaluation, and comparison. We design a treemap-like icon to represent a multidimensional cluster, and the quality of the cluster can be conveniently evaluated with the embedded statistical information. We further develop a novel layout algorithm which can generate similar icons for similar clusters, making comparisons of clusters easier. User interaction and clutter reduction are integrated into the system to help users more effectively analyze and refine clustering results for large datasets. We demonstrate the power of DICON through a user study and a case study in the healthcare domain. Our evaluation shows the benefits of the technique, especially in support of complex multidimensional cluster analysis.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, analysis method:Clustering, visual method:Novel Visual Encoding, visual method:Glyphs, paper type:System, data type:High-dimensional Points"
},
{
"code":26,
"title":"Design and Evaluation of Tiled Parallel Coordinates Visualization of Multichannel EEG Data",
"abstract":"The field of visualization assists data interpretation in many areas, but does not manage all types of data equally well. This holds, in particular, for time-varying multichannel EEG data. No existing method can successfully visualize simultaneous information from all channels in use at all time steps. To address this problem, a new visualization method is presented based on the parallel coordinate method and making use of a tiled organization. This tiled organization employs a two-dimensional row-column representation, rather than a one-dimensional arrangement in columns as used for classical parallel coordinates. The usefulness of the new method, referred to as tiled parallel coordinates (TPC), is demonstrated by a particular type of EEG data. It can be applied to an arbitrary number of time steps, handling the maximum number of channels currently in use. An extensive user evaluation shows that, for a typical EEG assessment task, data evaluation by the TPC method is faster than by an existing clinical EEG visualization method, without loss of information. The generality of the TPC method makes it widely applicable to other time-varying multivariate data types",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:Time Series, analysis method:not applicable, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":25,
"title":"Dynamic Multi-View Exploration of Shape Spaces",
"abstract":"Statistical shape modeling is a widely used technique for the representation and analysis of the shapes and shape variations present in a population. A statistical shape model models the distribution in a high dimensional shape space, where each shape is represented by a single point.We present a design study on the intuitive exploration and visualization of shape spaces and shape models. Our approach focuses on the dual-space nature of these spaces. The high-dimensional shape space represents the population, whereas object space represents the shape of the 3D object associated with a point in shape space.A 3D object view provides local details for a single shape. The high dimensional points in shape space are visualized using a 2D scatter plot projection, the axes of which can be manipulated interactively. This results in a dynamic scatter plot, with the further extension that each point is visualized as a small version of the object shape that it represents. We further enhance the population-object duality with a new type of view aimed at shape comparison. This new 鈥渟hape evolution view鈥�visualizes shape variability along a single trajectory in shape space, and serves as a link between the two spaces described above.Our three-view exploration concept strongly emphasizes linked interaction between all spaces. Moving the cursor over the scatter plot or evolution views, shapes are dynamically interpolated and shown in the object view. Conversely, camera manipulation in the object view affects the object visualizations in the other views. We present a GPU-accelerated implementation, and show the effectiveness of the three-view approach using a number of real-world cases. In these, we demonstrate how this multi-view approach can be used to visually explore important aspects of a statistical shape model, including specificity, compactness and reconstruction error.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Surfaces"
},
{
"code":24,
"title":"Visualization of multi-variate scientific data",
"abstract":"In this state-of-the-art report we discuss relevant research works related to the visualization of complex, multi-variate data. We discuss how different techniques take effect at specific stages of the visualization pipeline and how they apply to multi-variate data sets being composed of scalars, vectors and tensors. We also provide a categorization of these techniques with the aim for a better overview of related approaches. Based on this classification we highlight combinable and hybrid approaches and focus on techniques that potentially lead towards new directions in visualization research. In the second part of this paper we take a look at recent techniques that are useful for the visualization of complex data sets either because they are general purpose or because they can be adapted to specific problems.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, visual method:Volume Visualization, paper type:Survey, data type:Spatial Data, analysis method:not applicable"
},
{
"code":23,
"title":"Interactive High-Dimensional Data Visualization",
"abstract":"We propose a rudimentary taxonomy of interactive data visualization based on a triad of data analytic tasks: finding Gestalt, posing queries, and making comparisons. These tasks are supported by three classes of interactive view manipulations: focusing, linking, and arranging views. This discussion extends earlier work on the principles of focusing and linking and sets them on a firmer base. Next, we give a high-level introduction to a particular system for multivariate data visualization--XGobi. This introduction is not comprehensive but emphasizes XGobi tools that are examples of focusing, linking, and arranging views; namely, high-dimensional projections, linked scatterplot brushing, and matrices of conditional plots. Finally, in a series of case studies in data visualization, we show the powers and limitations of particular focusing, linking, and arranging tools. The discussion is dominated by high-dimensional projections that form an extremely well-developed part of XGobi. Of particular interest are the illustration of asymptotic normality of high-dimensional projections (a theorem of Diaconis and Freedman), the use of high-dimensional cubes for visualizing factorial experiments, and a method for interactively generating matrices of conditional plots with high-dimensional projections. Although there is a unifying theme to this article, each section--in particular the case studies--can be read separately.",
"keywords":"pipeline stage:Data Transformation, pipeline stage:Visual Mapping, pipeline stage:View Transformation, user involvement:Interactive Exploration, paper type:Survey,visual method:Parallel Coordinates, visual method:Scatterplot, analysis method:not applicable, data type:High-Dimensional Points"
},
{
"code":22,
"title":"Theory of Dynamic Projections in High-Dimensional Data Visualization",
"abstract":"Projections are a common tool for dimension reduction and visualization of highdimensional data. The generic example is a projection from IR p down to IR 2 which can be used to generate a 2-D view of p-D data. Going beyond static projections, tools have been developed over the last two decades for interactive viewing of high-dimensional data with dynamic, that is, moving or animated projections. These can be metaphorically described as moving film cameras in high-dimensional Euclidean spaces that return movies consisting of dimensionally reduced views. Such tools are variously known as 鈥済rand tours鈥� 鈥済uided tours鈥� and 鈥渕anual tours鈥� A special case are 鈥渞otations 鈥�in 3-D spaces. The purpose of this article is to describe dynamic projections in mathematical terms, and to show how their differential geometry links up with very concrete issues of animated views of data. Differential geometry applies because dynamic projections are just curves of projections. We start by discussing graphical rendering methods for projections of data, including cases in which the projection dimension is larger than one, two or three.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot"
},
{
"code":21,
"title":"Dis-function: Learning distance functions interactively",
"abstract":"The world's corpora of data grow in size and complexity every day, making it increasingly difficult for experts to make sense out of their data. Although machine learning offers algorithms for finding patterns in data automatically, they often require algorithm-specific parameters, such as an appropriate distance function, which are outside the purview of a domain expert. We present a system that allows an expert to interact directly with a visual representation of the data to define an appropriate distance function, thus avoiding direct manipulation of obtuse model parameters. Adopting an iterative approach, our system first assumes a uniformly weighted Euclidean distance function and projects the data into a two-dimensional scatterplot view. The user can then move incorrectly-positioned data points to locations that reflect his or her understanding of the similarity of those data points relative to the other data points. Based on this input, the system performs an optimization to learn a new distance function and then re-projects the data to redraw the scatter-plot. We illustrate empirically that with only a few iterations of interaction and optimization, a user can achieve a scatterplot view and its corresponding distance function that reflect the user's knowledge of the data. In addition, we evaluate our system to assess scalability in data size and data dimension, and show that our system is computationally efficient and can provide an interactive or near-interactive user experience.",
"keywords":"pipeline stage:Data Transformation, user involvement:Model Manipulation, visual method:Scatterplot, View Optimization, data type:High-dimensional Points, analysis method:Distance Metric, paper type:System"
},
{
"code":20,
"title":"Assisted Descriptor Selection Based on Visual Comparative Data Analysis",
"abstract":"Exploration and selection of data descriptors representing objects using a set of features are important components in many data analysis tasks. Usually, for a given dataset, an optimal data description does not exist, as the suitable data representation is strongly use case dependent. Many solutions for selecting a suitable data description have been proposed. In most instances, they require data labels and often are black box approaches. Non-expert users have difficulties to comprehend the coherency of input, parameters, and output of these algorithms. Alternative approaches, interactive systems for visual feature selection, overburden the user with an overwhelming set of options and data views. Therefore, it is essential to offer the users a guidance in this analytical process. In this paper, we present a novel system for data description selection, which facilitates the user's access to the data analysis process. As finding of suitable data description consists of several steps, we support the user with guidance. Our system combines automatic data analysis with interactive visualizations. By this, the system provides a recommendation for suitable data descriptor selections. It supports the comparison of data descriptors with differing dimensionality for unlabeled data. We propose specialized scores and interactive views for descriptor comparison. The visualization techniques are scatterplot-based and grid-based. For the latter case, we apply Self-Organizing Maps as adaptive grids which are well suited for large multi-dimensional data sets. As an example, we demonstrate the usability of our system on a real-world biochemical application.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:Feature Extraction, paper type:Technical, visual method:Pixel-based"
},
{
"code":19,
"title":"An Information-Aware Framework for Exploring Multivariate Data Sets",
"abstract":"Information theory provides a theoretical framework for measuring information content for an observed variable, and has attracted much attention from visualization researchers for its ability to quantify saliency and similarity among variables. In this paper, we present a new approach towards building an exploration framework based on information theory to guide the users through the multivariate data exploration process. In our framework, we compute the total entropy of the multivariate data set and identify the contribution of individual variables to the total entropy. The variables are classified into groups based on a novel graph model where a node represents a variable and the links encode the mutual information shared between the variables. The variables inside the groups are analyzed for their representativeness and an information based importance is assigned. We exploit specific information metrics to analyze the relationship between the variables and use the metrics to choose isocontours of selected variables. For a chosen group of points, parallel coordinates plots (PCP) are used to show the states of the variables and provide an interface for the user to select values of interest. Experiments with different data sets reveal the effectiveness of our proposed framework in depicting the interesting regions of the data sets taking into account the interaction among the variables.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, paper type:Technical, analysis method:Dimension Relationship, data type:Spatial Data, visual method:Volume Visualization, visual method:Parallel Coordinates"
},
{
"code":18,
"title":"Isosurface construction in any dimension using convex hulls",
"abstract":"We present an algorithm for constructing isosurfaces in any dimension. The input to the algorithm is a set of scalar values in a d-dimensional regular grid of (topological) hypercubes. The output is a set of (d-1)-dimensional simplices forming a piecewise linear approximation to the isosurface. The algorithm constructs the isosurface piecewise within each hypercube in the grid using the convex hull of an appropriate set of points. We prove that our algorithm correctly produces a triangulation of a (d-1 )-manifold with boundary. In dimensions three and four, lookup tables with 28 and 216 entries, respectively, can be used to speed the algorithm's running time. In three dimensions, this gives the popular marching cubes algorithm. We discuss applications of four-dimensional isosurface construction to time varying isosurfaces, interval volumes, and morphing.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, paper type:Technical, visual method:Volume Visualization, visual method:Isosurface, data type:High-Dimensional Function, analysis method:Feature Extraction"
},
{
"code":17,
"title":"Isosurfacing in higher dimensions",
"abstract":"Visualization algorithms have seen substantial improvements in the past several years. However, very few algorithms have been developed for directly studying data in dimensions higher than three. Most algorithms require a sampling in three-dimensions before applying any visualization algorithms. This sampling typically ignores vital features that may be present when examined in oblique cross-sections, and places an undo burden on system resources when animation through additional dimensions is desired. For time-varying data of large data sets, smooth animation is desired at interactive rates. We provide a fast Marching Cubes like algorithm for hypercubes of any dimension. To support this, we have developed a new algorithm to automatically generate the isosurface and triangulation tables for any dimension. This allows the efficient calculation of 4D isosurfaces, which can be interactively sliced to provide smooth animation or slicing through oblique hyperplanes. The former allows for smooth animation in a very compressed format. The latter provide better tools to study time-evolving features as they move downstream. We also provide examples in using this technique to show interval volumes or the sensitivity of a particular isovalue threshold.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, paper type:Technical, data type:high-dimensional function, analysis method:feature extraction, visual method:Isosurface"
},
{
"code":16,
"title":"Quality metrics in high-dimensional data visualization: an overview and systematization",
"abstract":"In this paper, we present a systematization of techniques that use quality metrics to help in the visual exploration of meaningful patterns in high-dimensional data. In a number of recent papers, different quality metrics are proposed to automate the demanding search through large spaces of alternative visualizations (e.g., alternative projections or ordering), allowing the user to concentrate on the most promising visualizations suggested by the quality metrics. Over the last decade, this approach has witnessed a remarkable development but few reflections exist on how these methods are related to each other and how the approach can be developed further. For this purpose, we provide an overview of approaches that use quality metrics in high-dimensional data visualization and propose a systematization based on a thorough literature review. We carefully analyze the papers and derive a set of factors for discriminating the quality metrics, visualization techniques, and the process itself. The process is described through a reworked version of the well-known information visualization pipeline. We demonstrate the usefulness of our model by applying it to several existing approaches that use quality metrics, and we provide reflections on implications of our model for future research.",
"keywords":"pipeline stage:Data Transformation, pipeline stage:Visual Mapping, pipeline stage:View Transformation, user involvement:Computation Centric, paper type:Survey,visual method:Parallel Coordinates, visual method:Scatterplot, data type:High-Dimensional Points, analysis method:Quality Measure"
},
{
"code":15,
"title":"Uncertainty-Aware Exploration of Continuous Parameter Spaces Using Multivariate Prediction",
"abstract":"Systems projecting a continuous n-dimensional parameter space to a continuous m-dimensional target space play an important role in science and engineering. If evaluating the system is expensive, however, an analysis is often limited to a small number of sample points. The main contribution of this paper is an interactive approach to enable a continuous analysis of a sampled parameter space with respect to multiple target values. We employ methods from statistical learning to predict results in real-time at any user-defined point and its neighborhood. In particular, we describe techniques to guide the user to potentially interesting parameter regions, and we visualize the inherent uncertainty of predictions in 2D scatterplots and parallel coordinates. An evaluation describes a real-world scenario in the application context of car engine design and reports feedback of domain experts. The results indicate that our approach is suitable to accelerate a local sensitivity analysis of multiple target dimensions, and to determine a sufficient local sampling density for interesting parameter regions.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, paper type:Technical, analysis method:Regression,visual method:Parallel Coordinates, visual method:Scatterplot, Uncertainty, data type:High-Dimensional Points"
},
{
"code":14,
"title":"Parallel sets: visual analysis of categorical data",
"abstract":"The discrete nature of categorical data makes it a particular challenge for visualization. Methods that work very well for continuous data are often hardly usable with categorical dimensions. Only few methods deal properly with such data, mostly because of the discrete nature of categorical data, which does not translate well into the continuous domains of space and color. Parallel sets is a new visualization method that adopts the layout of parallel coordinates, but substitutes the individual data points by a frequency based representation. This abstracted view, combined with a set of carefully designed interactions, supports visual data analysis of large and complex data sets. The technique allows efficient work with meta data, which is particularly important when dealing with categorical datasets. By creating new dimensions from existing ones, for example, the user can filter the data according to his or her current needs. We also present the results from an interactive analysis of CRM data using parallel sets. We demonstrate how the flexible layout eases the process of knowledge crystallization, especially when combined with a sophisticated interaction scheme.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, data type:Nominal Data, analysis method:Clustering, paper type:Technical, visual method:Parallel Coordinates, User Study"
},
{
"code":13,
"title":"Multivariate visual explanation for high dimensional datasets",
"abstract":"Understanding multivariate relationships is an important task in multivariate data analysis. Unfortunately, existing multivariate visualization systems lose effectiveness when analyzing relationships among variables that span more than a few dimensions. We present a novel multivariate visual explanation approach that helps users interactively discover multivariate relationships among a large number of dimensions by integrating automatic numerical differentiation techniques and multidimensional visualization techniques. The result is an efficient workflow for multivariate analysis model construction, interactive dimension reduction, and multivariate knowledge discovery leveraging both automatic multivariate analysis and interactive multivariate data visual exploration. Case studies and a formal user study with a real dataset illustrate the effectiveness of this approach.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, data type:High-dimensional Points, analysis method:dimension relationship, paper type:Technical, visual method:Parallel Coordinates"
},
{
"code":12,
"title":"A visual analytics approach to exploring protein flexibility subspaces",
"abstract":"Understanding what causes proteins to change shape and how the resulting shape influences function will expedite the design of more narrowly focused drugs and therapies. Shape alterations are often the result of flexibility changes in a set of localized neighborhoods that may or may not act in concert. Computational models have been developed to predict flexibility changes under varying empirical parameters. In this paper, we tackle a significant challenge facing scientists when analyzing outputs of a computational model, namely how to identify, examine, compare, and group interesting neighborhoods of proteins under different parameter sets. This is a difficult task since comparisons over protein subunits that comprise diverse neighborhoods are often too complex to characterize with a simple metric and too numerous to analyze manually. Here, we present a series of novel visual analytics approaches toward addressing this task. User scenarios illustrate the utility of these approaches and feedback from domain experts confirms their effectiveness.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Interactive Exploration, paper type:Technical, analysis method:Subspace, Histogram, data type:High-Dimensional Points, visual method:Scatterplot"
},
{
"code":11,
"title":"Surface-Based Structure Analysis and Visualization for Multifield Time-Varying Datasets",
"abstract":"This paper introduces a new feature analysis and visualization method for multifield datasets. Our approach applies a surface-centric model to characterize salient features and form an effective, schematic representation of the data. We propose a simple, geometrically motivated, multifield feature definition. This definition relies on an iterative algorithm that applies existing theory of skeleton derivation to fuse the structures from the constitutive fields into a coherent data description, while addressing noise and spurious details. This paper also presents a new method for non-rigid surface registration between the surfaces of consecutive time steps. This matching is used in conjunction with clustering to discover the interaction patterns between the different fields and their evolution over time. We document the unified visual analysis achieved by our method in the context of several multifield problems from large-scale time-varying simulations.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, paper type:Technical, visual method:Rendering Enhancement, visual method:Volume Visualization, data type:Time Series, data type:Spatial Data, analysis method:Data Abstraction"
},
{
"code":10,
"title":"Hypervolume visualization: a challenge in simplicity",
"abstract":"Hypervolume visualization is designed to provide simple and fully explanatory images that give comprehensive in-sights into the global structure of scalar fields of any dimension. The basic idea is to have a dimension independent viewing system that scales nicely with the geometric dimension of the dataset and that can be combined with classical approaches like isocontouring and animation of slices of nD data. One completely abandons (for core simplicity) rendering techniques, such as hidden surface removal or lighting or radiosity, that enhance three dimensional realism and concentrate on the real-time display of images that highlight structural (topological) features of the no dataset (holes, tunnels, cavities, depressions, extrema, etc.). Hypervolume visualization on the one hand is a generalization of direct parallel projection methods in volume rendering. To achieve efficiency (and real-time performance on a graphics workstation) the authors combine the advantages of (i) a hierarchical representations of the hypervolume data for multiresolution display and (ii) generalized object space splatting combined with texture-mapped graphics hardware acceleration. The main results of the paper are thus both a multiresolution direct rendering algorithm and scalable graphical user interface that provides global views of scalar fields in any dimension, while maintaining the fundamental characteristics of ease of use, and quick exploratory user interaction.",
"keywords":"pipeline stage:View Transformation, user involvement:Interactive Exploration, paper type:Technical, visual method:Rendering Enhancement, analysis method:Projection, data type:High-dimensional Points"
},
{
"code":9,
"title":"Efficient and Adaptive Rendering of 2-D Continuous Scatterplots",
"abstract":"We extend the rendering technique for continuous scatterplots to allow for a broad class of interpolation methods within the spatial grid instead of only linear interpolation. To do this, we propose an approach that projects the image of a cell from the spatial domain to the scatterplot domain. We approximate this image using either the convex hull or an axis-aligned rectangle that forms a tight fit of the projected points. In both cases, the approach relies on subdivision in the spatial domain to control the approximation error introduced in the scatterplot domain. Acceleration of this algorithm in homogeneous regions of the spatial domain is achieved using an octree hierarchy. The algorithm is scalable and adaptive since it allows us to balance computation time and scatterplot quality. We evaluate and discuss the results with respect to accuracy and computational speed. Our methods are applied to examples of 2-D transfer function design.",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Data Abstraction, paper type:Technical, visual method:Scatterplot"
},
{
"code":8,
"title":"Continuous Scatterplots",
"abstract":"The one-to-one strategy of mapping each single data item into a graphical marker adopted in many visualization techniques has limited usefulness when the number of records and/or the dimensionality of the data set are very high. In this situation, the strong overlapping of graphical markers severely hampers the user媒s ability to identify patterns in the data from its visual representation. We tackle this problem here with a strategy that computes frequency or density information from the data set, and uses such information in visual method:Parallel Coordinates visualizations to filter out the information to be presented to the user, thus reducing visual clutter and allowing the analyst to observe relevant patterns in the data. The algorithms to construct such visualizations, and the interaction mechanisms supported, inspired by traditional image processing techniques such as grayscale manipulation and thresholding are also presented. We also illustrate how such algorithms can assist users to effectively identify clusters in very noisy large data sets.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, paper type:Technical, visual method:Scatterplot, data type:High-dimensional Function, analysis method:Histogram, analysis method:Data Abstraction"
},
{
"code":7,
"title":"The Grand Tour: A Tool for Viewing Multidimensional Data",
"abstract":"The grand tour is a method for viewing multivariate statistical data via orthogonal projections onto a sequence of two-dimensional subspaces. The sequence of subspaces is chosen so that it is dense in the set of all two-dimensional subspaces. Desirable properties of such sequences of subspaces are considered, and several specific types of sequences are tested for rapidity of becoming dense. Tabulations are provided of the minimum length of a grand tour sequence necessary to achieve various degrees of denseness in dimensions up to 20.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot"
},
{
"code":6,
"title":"Uncovering Clusters in Crowded Parallel Coordinates Visualizations",
"abstract":"The one-to-one strategy of mapping each single data item into a graphical marker adopted in many visualization techniques has limited usefulness when the number of records and/or the dimensionality of the data set are very high. In this situation, the strong overlapping of graphical markers severely hampers the user's ability to identify patterns in the data from its visual representation. We tackle this problem here with a strategy that computes frequency or density information from the data set, and uses such information in parallel coordinates visualizations to filter out the information to be presented to the user, thus reducing visual clutter and allowing the analyst to observe relevant patterns in the data. The algorithms to construct such visualizations, and the interaction mechanisms supported, inspired by traditional image processing techniques such as grayscale manipulation and thresholding are also presented. We also illustrate how such algorithms can assist users to effectively identify clusters in very noisy large data sets",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, paper type:Technical,visual method:Parallel Coordinates, View Optimization, data type:High-dimensional Points, analysis method:Clustering"
},
{
"code":5,
"title":"Circle Segments: A Technique for Visually Exploring Large Multidimensional Data Sets",
"abstract":"In this paper, we describe a novel technique for visualizing large amounts of high-dimensional data, called ‘circle segments’. The technique uses one colored pixel per data value and can therefore be classified as a pixel-per-value technique [Kei 96]. The basic idea of the ‘circle segments ’ visualization technique is to display the data dimensions as segments of a circle. If the data consists of k dimensions, the circle is partitioned into k segments, each representing one data dimension. Inside the segments, the data values belonging to one dimension are arranged from the center of the circle to the outside in a back and forth manner orthogonal to the line that halves the segment. Our first results show that the ‘circle segment’ technique is very powerful for visualizing large amounts of data, providing more expressive visualizations than other wellknown techniques such as the ‘recursive pattern ’ technique and traditional",
"keywords":"pipeline stage:Visual Mapping, paper type:Technical, visual method:Pixel-based, analysis method:not applicable, user involvement:Interactive Exploration, data type:High-Dimensional Points"
},
{
"code":4,
"title":"Similarity clustering of dimensions for an enhanced visualization of multidimensional data",
"abstract":"The order and arrangement of dimensions (variates) is crucial for the effectiveness of a large number of visualization techniques such as parallel coordinates, scatterplots, recursive pattern, and many others. We describe a systematic approach to arrange the dimensions according to their similarity. The basic idea is to rearrange the data dimensions such that dimensions showing a similar behavior are positioned next to each other. For the similarity clustering of dimensions, we need to define similarity measures which determine the partial or global similarity of dimensions. We then consider the problem of finding an optimal one- or two-dimensional arrangement of the dimensions based on their similarity. Theoretical considerations show that both, the one- and the two-dimensional arrangement problem are surprisingly hard problems, i.e. they are NP complete. Our solution of the problem is therefore based on heuristic algorithms. An empirical evaluation using a number of different visualization techniques shows the high impact of our similarity clustering of dimensions on the visualization results",
"keywords":"pipeline stage:Visual Mapping, user involvement:Computation Centric, paper type:Technical, visual method:Parallel Coordinates,View Optimization, analysis method:Dimension Similarity, data type:High-Dimensional Points"
},
{
"code":3,
"title":"Visual pattern discovery using random projections",
"abstract":"An essential element of exploratory data analysis is the use of revealing low-dimensional projections of high-dimensional data. analysis method:Projection Pursuit has been an effective method for finding interesting low-dimensional projections of multidimensional spaces by optimizing a score function called a projection pursuit index. However, the technique is not scalable to high-dimensional spaces. Here, we introduce a novel method for discovering noteworthy views of high-dimensional data spaces by using binning and random projections. We define score functions, akin to projection pursuit indices, that characterize visual patterns of the low-dimensional projections that constitute feature subspaces. We also describe an analytic, multivariate visualization platform based on this algorithm that is scalable to extremely large problems.",
"keywords":"pipeline stage:Data Transformation, user involvement:Interactive Exploration, paper type:Technical, analysis method:Subspace, analysis method:Projection, visual method:Parallel Coordinates, visual method:Scatterplot, data type:High-Dimensional Points"
},
{
"code":2,
"title":"Synthetic Generation of High-Dimensional Datasets",
"abstract":"Generation of synthetic datasets is a common practice in many research areas. Such data is often generated to meet specific needs or certain conditions that may not be easily found in the original, real data. The nature of the data varies according to the application area and includes text, graphs, social or weather data, among many others. The common process to create such synthetic datasets is to implement small scripts or programs, restricted to small problems or to a specific application. In this paper we propose a framework designed to generate high dimensional datasets. Users can interactively create and navigate through multi dimensional datasets using a suitable graphical user-interface. The data creation is driven by statistical distributions based on a few user-defined parameters. First, a grounding dataset is created according to given inputs, and then structures and trends are included in selected dimensions and orthogonal projection planes. Furthermore, our framework supports the creation of complex non-orthogonal trends and classified datasets. It can successfully be used to create synthetic datasets simulating important trends as multidimensional clusters, correlations and outliers.",
"keywords":"pipeline stage:Data Transformation, user involvement:Computation Centric, data type:High-dimensional Points, analysis method:Projection, paper type:Technical, visual method:Scatterplot"
},
{
"code":1,
"title":"Improving the visual analysis of high-dimensional datasets using quality measures",
"abstract":"Modern visualization methods are needed to cope with very high-dimensional data. Efficient visual analytical techniques are required to extract the information content in these data. The large number of possible projections for each method, which usually grow quadratically or even exponentially with the number of dimensions, urges the necessity to employ automatic reduction techniques, automatic sorting or selecting the projections, based on their information-bearing content. Different quality measures have been successfully applied for several specified user tasks and established visualization techniques, like visual method:Scatterplots, visual method:Scatterplot Matrices or visual method:Parallel Coordinates. Many other popular visualization techniques exist, but due to the structural differences, the measures are not directly applicable to them and new approaches are needed. In this paper we propose new quality measures for three popular visualization methods: Radviz, Pixel-Oriented Displays and Table Lenses. Our experiments show that these measures efficiently guide the visual analysis task.",
"keywords":"pipeline stage:View Transformation, user involvement:Computation Centric, paper type:Technical, View Optimization, data type:High-dimensional Points, analysis method:Quality Measure, visual method:RadViz, visual method:Pixel-based"
}
]
